{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Data_loader import Dataset_Custom\n",
    "import argparse\n",
    "import warnings\n",
    "from tools import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_data\n",
    "from Model import ANN\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Server import  Server\n",
    "from Clients import Client\n",
    "from Train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=6)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--ensemble_flag', type=bool, default=True)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model12/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "args_train = parser_train.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "clients=[]\n",
    "for path in tqdm(args_train.dataset_paths):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths=path\n",
    "    clients.append(Client(args_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(args_train,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "scale=6\n",
    "# Load the server object\n",
    "with open('../result/'+str(scale)+'/server_benchmark.pkl', 'rb') as f:\n",
    "    server_benchmark = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients_benchmark.pkl', 'rb') as f:\n",
    "    clients_benchmark = pickle.load(f)\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    clients[i].copy_client(clients_benchmark[i])\n",
    "\n",
    "server = Server(args_train,clients)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: [0.16784182537908424, 0.16830104068942267, 0.19582986071297567, 0.17998810240054783, 0.17237979622736369, 0.18448530691229317, 0.18157152966787554]\n",
      "Epoch: 0 | Loss: 0.0861\n",
      "Epoch: 0 | Loss: 0.0699\n",
      "Epoch: 0 | Loss: 0.1111\n",
      "Epoch: 0 | Loss: 0.0865\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0815\n",
      "Epoch: 0 | Loss: 0.0901\n",
      "Federated training Epoch [1/200] Val Loss: 0.0734\n",
      "test performance: [0.07707778765016222, 0.08284751055379437, 0.09575462652599975, 0.08664964440546624, 0.08930565388745641, 0.08593025650471857, 0.09055326151827427]\n",
      "Epoch: 0 | Loss: 0.0637\n",
      "Epoch: 0 | Loss: 0.0701\n",
      "Epoch: 0 | Loss: 0.0900\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0869\n",
      "Federated training Epoch [2/200] Val Loss: 0.0676\n",
      "test performance: [0.07154573047252959, 0.07818217344633112, 0.08699091715253379, 0.07997376558511224, 0.08321518137132468, 0.07858349449218135, 0.08346996527828582]\n",
      "Epoch: 0 | Loss: 0.0562\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0609\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0742\n",
      "Epoch: 0 | Loss: 0.0780\n",
      "Federated training Epoch [3/200] Val Loss: 0.0614\n",
      "test performance: [0.06468631963768642, 0.07321006113229549, 0.0783146973061439, 0.0722960937028266, 0.07504238717717258, 0.07024918991613062, 0.0758311767386247]\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0661\n",
      "Epoch: 0 | Loss: 0.0691\n",
      "Epoch: 0 | Loss: 0.0603\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Epoch: 0 | Loss: 0.0710\n",
      "Federated training Epoch [4/200] Val Loss: 0.0592\n",
      "test performance: [0.061775405559535715, 0.07103061228225084, 0.07515003362491932, 0.06914603077385524, 0.07146307662741778, 0.06713519218594652, 0.07307040243575426]\n",
      "Epoch: 0 | Loss: 0.0553\n",
      "Epoch: 0 | Loss: 0.0739\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.0729\n",
      "Epoch: 0 | Loss: 0.0636\n",
      "Epoch: 0 | Loss: 0.0592\n",
      "Epoch: 0 | Loss: 0.0627\n",
      "Federated training Epoch [5/200] Val Loss: 0.0582\n",
      "test performance: [0.06048407483437698, 0.06951498358880412, 0.07335977528040132, 0.06771015360543173, 0.06933030052340194, 0.06567193630266271, 0.07135479400980554]\n",
      "Epoch: 0 | Loss: 0.0510\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0682\n",
      "Epoch: 0 | Loss: 0.0690\n",
      "Epoch: 0 | Loss: 0.0660\n",
      "Epoch: 0 | Loss: 0.0776\n",
      "Epoch: 0 | Loss: 0.0600\n",
      "Federated training Epoch [6/200] Val Loss: 0.0571\n",
      "test performance: [0.059205148930418984, 0.06881668418645859, 0.07257861258742744, 0.06635805651902744, 0.06862313106451949, 0.0645253329780208, 0.07064107236490674]\n",
      "Epoch: 0 | Loss: 0.0602\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0679\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Epoch: 0 | Loss: 0.0787\n",
      "Epoch: 0 | Loss: 0.0547\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Federated training Epoch [7/200] Val Loss: 0.0569\n",
      "test performance: [0.0589351047544855, 0.06786491307238601, 0.07173693180084229, 0.06602643029636716, 0.06742365706763039, 0.06396653476388078, 0.06952699356750675]\n",
      "Epoch: 0 | Loss: 0.0509\n",
      "Epoch: 0 | Loss: 0.0590\n",
      "Epoch: 0 | Loss: 0.0618\n",
      "Epoch: 0 | Loss: 0.0512\n",
      "Epoch: 0 | Loss: 0.0534\n",
      "Epoch: 0 | Loss: 0.0527\n",
      "Epoch: 0 | Loss: 0.0592\n",
      "Federated training Epoch [8/200] Val Loss: 0.0560\n",
      "test performance: [0.058169372494600406, 0.06762528288088841, 0.07147149274712555, 0.0651363521839862, 0.0672895905166252, 0.06328379907581495, 0.06920730499933435]\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0620\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0580\n",
      "Epoch: 0 | Loss: 0.0785\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0552\n",
      "Federated training Epoch [9/200] Val Loss: 0.0556\n",
      "test performance: [0.0578427815886393, 0.06726406681772372, 0.07116210100891655, 0.06468221959849335, 0.06696994194429215, 0.06286759392600762, 0.0687223588282319]\n",
      "Epoch: 0 | Loss: 0.0574\n",
      "Epoch: 0 | Loss: 0.0638\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0617\n",
      "Epoch: 0 | Loss: 0.0620\n",
      "Epoch: 0 | Loss: 0.0657\n",
      "Epoch: 0 | Loss: 0.0665\n",
      "Federated training Epoch [10/200] Val Loss: 0.0557\n",
      "test performance: [0.05793123347812319, 0.06635315209780246, 0.07044922390774097, 0.06473517519332571, 0.06593015991559584, 0.06275104009865286, 0.06782831337098798]\n",
      "Epoch: 0 | Loss: 0.0523\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0710\n",
      "Epoch: 0 | Loss: 0.0955\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0600\n",
      "Federated training Epoch [11/200] Val Loss: 0.0552\n",
      "test performance: [0.057458968833088875, 0.06633978058928497, 0.07033161419660669, 0.06421588140552582, 0.06587072560716778, 0.062260414027187926, 0.06765842761793366]\n",
      "Epoch: 0 | Loss: 0.0558\n",
      "Epoch: 0 | Loss: 0.0627\n",
      "Epoch: 0 | Loss: 0.0796\n",
      "Epoch: 0 | Loss: 0.0622\n",
      "Epoch: 0 | Loss: 0.0613\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Federated training Epoch [12/200] Val Loss: 0.0555\n",
      "test performance: [0.05781196078209028, 0.06589931958954628, 0.07010073791423889, 0.06457810757732758, 0.06523611471184516, 0.06250438328883419, 0.06721153568910206]\n",
      "Epoch: 0 | Loss: 0.0543\n",
      "Epoch: 0 | Loss: 0.0582\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0578\n",
      "Epoch: 0 | Loss: 0.0784\n",
      "Epoch: 0 | Loss: 0.0646\n",
      "Epoch: 0 | Loss: 0.0590\n",
      "Federated training Epoch [13/200] Val Loss: 0.0547\n",
      "test performance: [0.05706446744144371, 0.06594533375017855, 0.06996357731827318, 0.06371347413538661, 0.06536518289286593, 0.06181556682982673, 0.06710549769285198]\n",
      "Epoch: 0 | Loss: 0.0471\n",
      "Epoch: 0 | Loss: 0.0628\n",
      "Epoch: 0 | Loss: 0.0603\n",
      "Epoch: 0 | Loss: 0.0526\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0577\n",
      "Epoch: 0 | Loss: 0.0642\n",
      "Federated training Epoch [14/200] Val Loss: 0.0547\n",
      "test performance: [0.05717830675692387, 0.06560719255613137, 0.06972523208401382, 0.06383531472377785, 0.06496304432440499, 0.061844451193480865, 0.06676538663673891]\n",
      "Epoch: 0 | Loss: 0.0610\n",
      "Epoch: 0 | Loss: 0.0544\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Epoch: 0 | Loss: 0.0767\n",
      "Epoch: 0 | Loss: 0.0726\n",
      "Epoch: 0 | Loss: 0.0672\n",
      "Epoch: 0 | Loss: 0.0548\n",
      "Federated training Epoch [15/200] Val Loss: 0.0542\n",
      "test performance: [0.05668415381790024, 0.06572566624118449, 0.06973031463024959, 0.06324079704203017, 0.06506265678812992, 0.06140519410040077, 0.0667294892035935]\n",
      "Epoch: 0 | Loss: 0.0512\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0851\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.0617\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Epoch: 0 | Loss: 0.0604\n",
      "Federated training Epoch [16/200] Val Loss: 0.0538\n",
      "test performance: [0.05650414514980496, 0.0662207859887244, 0.07001074376732927, 0.06292637631501237, 0.06555883927679021, 0.06122882599138642, 0.06695155282696225]\n",
      "Epoch: 0 | Loss: 0.0566\n",
      "Epoch: 0 | Loss: 0.0535\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0611\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Federated training Epoch [17/200] Val Loss: 0.0539\n",
      "test performance: [0.05644516229680548, 0.06547979583121734, 0.06946460585020585, 0.06298577189700652, 0.06473628688629156, 0.06110945104761687, 0.0663694234513869]\n",
      "Epoch: 0 | Loss: 0.0622\n",
      "Epoch: 0 | Loss: 0.0493\n",
      "Epoch: 0 | Loss: 0.0595\n",
      "Epoch: 0 | Loss: 0.0537\n",
      "Epoch: 0 | Loss: 0.0715\n",
      "Epoch: 0 | Loss: 0.0535\n",
      "Epoch: 0 | Loss: 0.0711\n",
      "Federated training Epoch [18/200] Val Loss: 0.0538\n",
      "test performance: [0.05646645226707197, 0.06553669914297044, 0.06945085576544069, 0.06296129139420921, 0.06466717772466475, 0.06101645034265845, 0.06621000881957477]\n",
      "Epoch: 0 | Loss: 0.0548\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Epoch: 0 | Loss: 0.0476\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0634\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Federated training Epoch [19/200] Val Loss: 0.0536\n",
      "test performance: [0.05630218722436526, 0.06556653802933758, 0.06941976907267554, 0.06277376867524564, 0.06463294121602627, 0.06088724491913637, 0.06612689222196398]\n",
      "Epoch: 0 | Loss: 0.0608\n",
      "Epoch: 0 | Loss: 0.0648\n",
      "Epoch: 0 | Loss: 0.0592\n",
      "Epoch: 0 | Loss: 0.0530\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0433\n",
      "Epoch: 0 | Loss: 0.0815\n",
      "Federated training Epoch [20/200] Val Loss: 0.0539\n",
      "test performance: [0.056688768591425596, 0.06510903714352274, 0.06920315628896838, 0.06329143608039985, 0.0641279573590584, 0.06120974246463547, 0.06588534132437143]\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0580\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0478\n",
      "Epoch: 0 | Loss: 0.0578\n",
      "Epoch: 0 | Loss: 0.0479\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Federated training Epoch [21/200] Val Loss: 0.0534\n",
      "test performance: [0.05622141537805126, 0.06536498824006891, 0.06922298654505651, 0.06268958443750257, 0.06442205317650143, 0.06078873471395202, 0.06598656249474989]\n",
      "Epoch: 0 | Loss: 0.0570\n",
      "Epoch: 0 | Loss: 0.0570\n",
      "Epoch: 0 | Loss: 0.0595\n",
      "Epoch: 0 | Loss: 0.0667\n",
      "Epoch: 0 | Loss: 0.0802\n",
      "Epoch: 0 | Loss: 0.0582\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Federated training Epoch [22/200] Val Loss: 0.0534\n",
      "test performance: [0.05627572840105181, 0.06508911559230661, 0.06903348264781987, 0.06275910035107438, 0.06414592634146549, 0.060811937958869626, 0.06571798801600728]\n",
      "Epoch: 0 | Loss: 0.0532\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0562\n",
      "Epoch: 0 | Loss: 0.0653\n",
      "Epoch: 0 | Loss: 0.0491\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Federated training Epoch [23/200] Val Loss: 0.0534\n",
      "test performance: [0.05630996559861384, 0.06508704701004779, 0.06900711463483637, 0.06281051881075518, 0.06402404857671833, 0.0607270056841104, 0.06560036376093144]\n",
      "Epoch: 0 | Loss: 0.0429\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Epoch: 0 | Loss: 0.0514\n",
      "Epoch: 0 | Loss: 0.0636\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Epoch: 0 | Loss: 0.0574\n",
      "Federated training Epoch [24/200] Val Loss: 0.0532\n",
      "test performance: [0.05614316901064491, 0.06510091343358772, 0.06902426536105676, 0.06259268431681884, 0.06395725970322343, 0.060657688678076416, 0.06558104065463763]\n",
      "Epoch: 0 | Loss: 0.0570\n",
      "Epoch: 0 | Loss: 0.0600\n",
      "Epoch: 0 | Loss: 0.0543\n",
      "Epoch: 0 | Loss: 0.0658\n",
      "Epoch: 0 | Loss: 0.0599\n",
      "Epoch: 0 | Loss: 0.0668\n",
      "Epoch: 0 | Loss: 0.0732\n",
      "Federated training Epoch [25/200] Val Loss: 0.0528\n",
      "test performance: [0.055883160715744104, 0.06533445074695023, 0.06908587812867066, 0.06228243938506874, 0.06435200942629209, 0.06043706971463071, 0.0658041913802289]\n",
      "Epoch: 0 | Loss: 0.0621\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Epoch: 0 | Loss: 0.0500\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0465\n",
      "Epoch: 0 | Loss: 0.0564\n",
      "Federated training Epoch [26/200] Val Loss: 0.0533\n",
      "test performance: [0.05634689771200288, 0.06486613401658323, 0.06887964064842217, 0.06288468095231546, 0.06373560790304247, 0.060812134385006884, 0.06540434713130944]\n",
      "Epoch: 0 | Loss: 0.0491\n",
      "Epoch: 0 | Loss: 0.0605\n",
      "Epoch: 0 | Loss: 0.0634\n",
      "Epoch: 0 | Loss: 0.0492\n",
      "Epoch: 0 | Loss: 0.0571\n",
      "Epoch: 0 | Loss: 0.0571\n",
      "Epoch: 0 | Loss: 0.0489\n",
      "Federated training Epoch [27/200] Val Loss: 0.0532\n",
      "test performance: [0.05631614297832528, 0.06467633491560612, 0.06880665834584873, 0.06291630138222078, 0.06358275656294946, 0.06080278955808241, 0.06535984259353925]\n",
      "Epoch: 0 | Loss: 0.0501\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0553\n",
      "Epoch: 0 | Loss: 0.0548\n",
      "Epoch: 0 | Loss: 0.0727\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0573\n",
      "Federated training Epoch [28/200] Val Loss: 0.0529\n",
      "test performance: [0.055946559345426215, 0.06476165643854909, 0.06867931538248716, 0.06239413684361601, 0.0636237955345989, 0.060403683898996, 0.06520677485490499]\n",
      "Epoch: 0 | Loss: 0.0531\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0545\n",
      "Epoch: 0 | Loss: 0.0835\n",
      "Epoch: 0 | Loss: 0.0670\n",
      "Epoch: 0 | Loss: 0.0559\n",
      "Federated training Epoch [29/200] Val Loss: 0.0525\n",
      "test performance: [0.05565849902133827, 0.0650872495790867, 0.0687981431997598, 0.06199102203859246, 0.06413702524509536, 0.0601808275542643, 0.06550950824270306]\n",
      "Epoch: 0 | Loss: 0.0449\n",
      "Epoch: 0 | Loss: 0.0635\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0688\n",
      "Epoch: 0 | Loss: 0.0499\n",
      "Epoch: 0 | Loss: 0.0494\n",
      "Epoch: 0 | Loss: 0.0760\n",
      "Federated training Epoch [30/200] Val Loss: 0.0525\n",
      "test performance: [0.05576542002620966, 0.06487755761928346, 0.06867509858351048, 0.0621714172629665, 0.06376754748316048, 0.060259566993184695, 0.06520281815646242]\n",
      "Epoch: 0 | Loss: 0.0526\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0737\n",
      "Epoch: 0 | Loss: 0.0574\n",
      "Epoch: 0 | Loss: 0.0651\n",
      "Epoch: 0 | Loss: 0.0563\n",
      "Epoch: 0 | Loss: 0.0510\n",
      "Federated training Epoch [31/200] Val Loss: 0.0522\n",
      "test performance: [0.055573948340056696, 0.0652624231520785, 0.06889369096351813, 0.0618969216713146, 0.0642891872980415, 0.06010490550646839, 0.06560992385732801]\n",
      "Epoch: 0 | Loss: 0.0528\n",
      "Epoch: 0 | Loss: 0.0615\n",
      "Epoch: 0 | Loss: 0.0692\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0808\n",
      "Epoch: 0 | Loss: 0.0536\n",
      "Epoch: 0 | Loss: 0.0597\n",
      "Federated training Epoch [32/200] Val Loss: 0.0529\n",
      "test performance: [0.05611993859789959, 0.06453799863333164, 0.06847309831478825, 0.0626219704504801, 0.06334786240827955, 0.06051256710484828, 0.06491896443783421]\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Epoch: 0 | Loss: 0.0604\n",
      "Epoch: 0 | Loss: 0.0622\n",
      "Epoch: 0 | Loss: 0.0514\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0425\n",
      "Epoch: 0 | Loss: 0.0753\n",
      "Federated training Epoch [33/200] Val Loss: 0.0526\n",
      "test performance: [0.05591537112574259, 0.06458720039219072, 0.068459826802248, 0.0624415009882148, 0.06338224674205054, 0.06030913556555975, 0.06500736529594414]\n",
      "Epoch: 0 | Loss: 0.0448\n",
      "Epoch: 0 | Loss: 0.0805\n",
      "Epoch: 0 | Loss: 0.0568\n",
      "Epoch: 0 | Loss: 0.0513\n",
      "Epoch: 0 | Loss: 0.0577\n",
      "Epoch: 0 | Loss: 0.0571\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Federated training Epoch [34/200] Val Loss: 0.0525\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.fed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.0925 Val Loss: 0.0727\n",
      "Epoch [2/200] Train Loss: 0.0715 Val Loss: 0.0656\n",
      "Epoch [3/200] Train Loss: 0.0645 Val Loss: 0.0607\n",
      "Epoch [4/200] Train Loss: 0.0609 Val Loss: 0.0593\n",
      "Epoch [5/200] Train Loss: 0.0592 Val Loss: 0.0574\n",
      "Epoch [6/200] Train Loss: 0.0581 Val Loss: 0.0566\n",
      "Epoch [7/200] Train Loss: 0.0573 Val Loss: 0.0563\n",
      "Epoch [8/200] Train Loss: 0.0568 Val Loss: 0.0551\n",
      "Epoch [9/200] Train Loss: 0.0562 Val Loss: 0.0548\n",
      "Epoch [10/200] Train Loss: 0.0557 Val Loss: 0.0544\n",
      "Epoch [11/200] Train Loss: 0.0553 Val Loss: 0.0541\n",
      "Epoch [12/200] Train Loss: 0.0550 Val Loss: 0.0538\n",
      "Epoch [13/200] Train Loss: 0.0546 Val Loss: 0.0541\n",
      "Epoch [14/200] Train Loss: 0.0542 Val Loss: 0.0530\n",
      "Epoch [15/200] Train Loss: 0.0541 Val Loss: 0.0527\n",
      "Epoch [16/200] Train Loss: 0.0537 Val Loss: 0.0526\n",
      "Epoch [17/200] Train Loss: 0.0534 Val Loss: 0.0525\n",
      "Epoch [18/200] Train Loss: 0.0533 Val Loss: 0.0525\n",
      "Epoch [19/200] Train Loss: 0.0530 Val Loss: 0.0519\n",
      "Epoch [20/200] Train Loss: 0.0525 Val Loss: 0.0517\n",
      "Epoch [21/200] Train Loss: 0.0523 Val Loss: 0.0518\n",
      "Epoch [22/200] Train Loss: 0.0523 Val Loss: 0.0512\n",
      "Epoch [23/200] Train Loss: 0.0518 Val Loss: 0.0510\n",
      "Epoch [24/200] Train Loss: 0.0515 Val Loss: 0.0515\n",
      "Epoch [25/200] Train Loss: 0.0513 Val Loss: 0.0508\n",
      "Epoch [26/200] Train Loss: 0.0511 Val Loss: 0.0505\n",
      "Epoch [27/200] Train Loss: 0.0510 Val Loss: 0.0511\n",
      "Epoch [28/200] Train Loss: 0.0508 Val Loss: 0.0502\n",
      "Epoch [29/200] Train Loss: 0.0504 Val Loss: 0.0516\n",
      "Epoch [30/200] Train Loss: 0.0504 Val Loss: 0.0501\n",
      "Epoch [31/200] Train Loss: 0.0504 Val Loss: 0.0506\n",
      "Epoch [32/200] Train Loss: 0.0502 Val Loss: 0.0500\n",
      "Epoch [33/200] Train Loss: 0.0498 Val Loss: 0.0499\n",
      "Epoch [34/200] Train Loss: 0.0499 Val Loss: 0.0509\n",
      "Epoch [35/200] Train Loss: 0.0497 Val Loss: 0.0497\n",
      "Epoch [36/200] Train Loss: 0.0495 Val Loss: 0.0499\n",
      "Epoch [37/200] Train Loss: 0.0493 Val Loss: 0.0496\n",
      "Epoch [38/200] Train Loss: 0.0493 Val Loss: 0.0493\n",
      "Epoch [39/200] Train Loss: 0.0492 Val Loss: 0.0495\n",
      "Epoch [40/200] Train Loss: 0.0488 Val Loss: 0.0492\n",
      "Epoch [41/200] Train Loss: 0.0487 Val Loss: 0.0493\n",
      "Epoch [42/200] Train Loss: 0.0485 Val Loss: 0.0496\n",
      "Epoch [43/200] Train Loss: 0.0485 Val Loss: 0.0488\n",
      "Epoch [44/200] Train Loss: 0.0482 Val Loss: 0.0490\n",
      "Epoch [45/200] Train Loss: 0.0481 Val Loss: 0.0496\n",
      "Epoch [46/200] Train Loss: 0.0482 Val Loss: 0.0488\n",
      "Epoch [47/200] Train Loss: 0.0481 Val Loss: 0.0489\n",
      "Epoch [48/200] Train Loss: 0.0479 Val Loss: 0.0494\n",
      "Epoch [49/200] Train Loss: 0.0476 Val Loss: 0.0496\n",
      "Epoch [50/200] Train Loss: 0.0474 Val Loss: 0.0487\n",
      "Epoch [51/200] Train Loss: 0.0474 Val Loss: 0.0486\n",
      "Epoch [52/200] Train Loss: 0.0472 Val Loss: 0.0486\n",
      "Epoch [53/200] Train Loss: 0.0471 Val Loss: 0.0493\n",
      "Epoch [54/200] Train Loss: 0.0472 Val Loss: 0.0484\n",
      "Epoch [55/200] Train Loss: 0.0468 Val Loss: 0.0482\n",
      "Epoch [56/200] Train Loss: 0.0468 Val Loss: 0.0480\n",
      "Epoch [57/200] Train Loss: 0.0468 Val Loss: 0.0480\n",
      "Epoch [58/200] Train Loss: 0.0463 Val Loss: 0.0480\n",
      "Epoch [59/200] Train Loss: 0.0466 Val Loss: 0.0480\n",
      "Epoch [60/200] Train Loss: 0.0464 Val Loss: 0.0478\n",
      "Epoch [61/200] Train Loss: 0.0463 Val Loss: 0.0482\n",
      "Epoch [62/200] Train Loss: 0.0459 Val Loss: 0.0477\n",
      "Epoch [63/200] Train Loss: 0.0458 Val Loss: 0.0489\n",
      "Epoch [64/200] Train Loss: 0.0456 Val Loss: 0.0475\n",
      "Epoch [65/200] Train Loss: 0.0455 Val Loss: 0.0479\n",
      "Epoch [66/200] Train Loss: 0.0455 Val Loss: 0.0474\n",
      "Epoch [67/200] Train Loss: 0.0456 Val Loss: 0.0473\n",
      "Epoch [68/200] Train Loss: 0.0450 Val Loss: 0.0472\n",
      "Epoch [69/200] Train Loss: 0.0450 Val Loss: 0.0475\n",
      "Epoch [70/200] Train Loss: 0.0447 Val Loss: 0.0475\n",
      "Epoch [71/200] Train Loss: 0.0448 Val Loss: 0.0471\n",
      "Epoch [72/200] Train Loss: 0.0445 Val Loss: 0.0475\n",
      "Epoch [73/200] Train Loss: 0.0445 Val Loss: 0.0473\n",
      "Epoch [74/200] Train Loss: 0.0442 Val Loss: 0.0471\n",
      "Epoch [75/200] Train Loss: 0.0445 Val Loss: 0.0468\n",
      "Epoch [76/200] Train Loss: 0.0438 Val Loss: 0.0472\n",
      "Epoch [77/200] Train Loss: 0.0439 Val Loss: 0.0472\n",
      "Epoch [78/200] Train Loss: 0.0438 Val Loss: 0.0478\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.central_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch Local Training!\n",
      "Epoch [1/200] Train Loss: 0.0865 Val Loss: 0.0720\n",
      "Epoch [2/200] Train Loss: 0.0700 Val Loss: 0.0640\n",
      "Epoch [3/200] Train Loss: 0.0631 Val Loss: 0.0601\n",
      "Epoch [4/200] Train Loss: 0.0602 Val Loss: 0.0580\n",
      "Epoch [5/200] Train Loss: 0.0587 Val Loss: 0.0569\n",
      "Epoch [6/200] Train Loss: 0.0576 Val Loss: 0.0564\n",
      "Epoch [7/200] Train Loss: 0.0569 Val Loss: 0.0551\n",
      "Epoch [8/200] Train Loss: 0.0562 Val Loss: 0.0561\n",
      "Epoch [9/200] Train Loss: 0.0559 Val Loss: 0.0544\n",
      "Epoch [10/200] Train Loss: 0.0555 Val Loss: 0.0539\n",
      "Epoch [11/200] Train Loss: 0.0549 Val Loss: 0.0545\n",
      "Epoch [12/200] Train Loss: 0.0550 Val Loss: 0.0533\n",
      "Epoch [13/200] Train Loss: 0.0546 Val Loss: 0.0531\n",
      "Epoch [14/200] Train Loss: 0.0541 Val Loss: 0.0531\n",
      "Epoch [15/200] Train Loss: 0.0538 Val Loss: 0.0532\n",
      "Epoch [16/200] Train Loss: 0.0535 Val Loss: 0.0530\n",
      "Epoch [17/200] Train Loss: 0.0533 Val Loss: 0.0521\n",
      "Epoch [18/200] Train Loss: 0.0530 Val Loss: 0.0530\n",
      "Epoch [19/200] Train Loss: 0.0531 Val Loss: 0.0527\n",
      "Epoch [20/200] Train Loss: 0.0526 Val Loss: 0.0515\n",
      "Epoch [21/200] Train Loss: 0.0527 Val Loss: 0.0517\n",
      "Epoch [22/200] Train Loss: 0.0527 Val Loss: 0.0513\n",
      "Epoch [23/200] Train Loss: 0.0520 Val Loss: 0.0514\n",
      "Epoch [24/200] Train Loss: 0.0521 Val Loss: 0.0529\n",
      "Epoch [25/200] Train Loss: 0.0520 Val Loss: 0.0510\n",
      "Epoch [26/200] Train Loss: 0.0515 Val Loss: 0.0507\n",
      "Epoch [27/200] Train Loss: 0.0514 Val Loss: 0.0507\n",
      "Epoch [28/200] Train Loss: 0.0511 Val Loss: 0.0507\n",
      "Epoch [29/200] Train Loss: 0.0512 Val Loss: 0.0506\n",
      "Epoch [30/200] Train Loss: 0.0510 Val Loss: 0.0504\n",
      "Epoch [31/200] Train Loss: 0.0507 Val Loss: 0.0503\n",
      "Epoch [32/200] Train Loss: 0.0505 Val Loss: 0.0514\n",
      "Epoch [33/200] Train Loss: 0.0504 Val Loss: 0.0502\n",
      "Epoch [34/200] Train Loss: 0.0504 Val Loss: 0.0504\n",
      "Epoch [35/200] Train Loss: 0.0503 Val Loss: 0.0500\n",
      "Epoch [36/200] Train Loss: 0.0501 Val Loss: 0.0506\n",
      "Epoch [37/200] Train Loss: 0.0502 Val Loss: 0.0498\n",
      "Epoch [38/200] Train Loss: 0.0499 Val Loss: 0.0497\n",
      "Epoch [39/200] Train Loss: 0.0496 Val Loss: 0.0500\n",
      "Epoch [40/200] Train Loss: 0.0496 Val Loss: 0.0508\n",
      "Epoch [41/200] Train Loss: 0.0500 Val Loss: 0.0496\n",
      "Epoch [42/200] Train Loss: 0.0493 Val Loss: 0.0507\n",
      "Epoch [43/200] Train Loss: 0.0492 Val Loss: 0.0504\n",
      "Epoch [44/200] Train Loss: 0.0489 Val Loss: 0.0496\n",
      "Epoch [45/200] Train Loss: 0.0489 Val Loss: 0.0493\n",
      "Epoch [46/200] Train Loss: 0.0488 Val Loss: 0.0497\n",
      "Epoch [47/200] Train Loss: 0.0492 Val Loss: 0.0493\n",
      "Epoch [48/200] Train Loss: 0.0486 Val Loss: 0.0495\n",
      "Epoch [49/200] Train Loss: 0.0486 Val Loss: 0.0494\n",
      "Epoch [50/200] Train Loss: 0.0483 Val Loss: 0.0495\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0910 Val Loss: 0.0774\n",
      "Epoch [2/200] Train Loss: 0.0742 Val Loss: 0.0689\n",
      "Epoch [3/200] Train Loss: 0.0678 Val Loss: 0.0650\n",
      "Epoch [4/200] Train Loss: 0.0653 Val Loss: 0.0635\n",
      "Epoch [5/200] Train Loss: 0.0638 Val Loss: 0.0619\n",
      "Epoch [6/200] Train Loss: 0.0628 Val Loss: 0.0610\n",
      "Epoch [7/200] Train Loss: 0.0619 Val Loss: 0.0606\n",
      "Epoch [8/200] Train Loss: 0.0614 Val Loss: 0.0606\n",
      "Epoch [9/200] Train Loss: 0.0608 Val Loss: 0.0591\n",
      "Epoch [10/200] Train Loss: 0.0603 Val Loss: 0.0587\n",
      "Epoch [11/200] Train Loss: 0.0600 Val Loss: 0.0584\n",
      "Epoch [12/200] Train Loss: 0.0595 Val Loss: 0.0580\n",
      "Epoch [13/200] Train Loss: 0.0594 Val Loss: 0.0578\n",
      "Epoch [14/200] Train Loss: 0.0588 Val Loss: 0.0578\n",
      "Epoch [15/200] Train Loss: 0.0585 Val Loss: 0.0573\n",
      "Epoch [16/200] Train Loss: 0.0582 Val Loss: 0.0571\n",
      "Epoch [17/200] Train Loss: 0.0580 Val Loss: 0.0569\n",
      "Epoch [18/200] Train Loss: 0.0578 Val Loss: 0.0571\n",
      "Epoch [19/200] Train Loss: 0.0573 Val Loss: 0.0574\n",
      "Epoch [20/200] Train Loss: 0.0573 Val Loss: 0.0567\n",
      "Epoch [21/200] Train Loss: 0.0570 Val Loss: 0.0579\n",
      "Epoch [22/200] Train Loss: 0.0566 Val Loss: 0.0561\n",
      "Epoch [23/200] Train Loss: 0.0570 Val Loss: 0.0565\n",
      "Epoch [24/200] Train Loss: 0.0562 Val Loss: 0.0559\n",
      "Epoch [25/200] Train Loss: 0.0560 Val Loss: 0.0557\n",
      "Epoch [26/200] Train Loss: 0.0559 Val Loss: 0.0559\n",
      "Epoch [27/200] Train Loss: 0.0557 Val Loss: 0.0557\n",
      "Epoch [28/200] Train Loss: 0.0557 Val Loss: 0.0555\n",
      "Epoch [29/200] Train Loss: 0.0552 Val Loss: 0.0561\n",
      "Epoch [30/200] Train Loss: 0.0552 Val Loss: 0.0553\n",
      "Epoch [31/200] Train Loss: 0.0551 Val Loss: 0.0553\n",
      "Epoch [32/200] Train Loss: 0.0547 Val Loss: 0.0555\n",
      "Epoch [33/200] Train Loss: 0.0547 Val Loss: 0.0551\n",
      "Epoch [34/200] Train Loss: 0.0546 Val Loss: 0.0549\n",
      "Epoch [35/200] Train Loss: 0.0542 Val Loss: 0.0547\n",
      "Epoch [36/200] Train Loss: 0.0544 Val Loss: 0.0555\n",
      "Epoch [37/200] Train Loss: 0.0540 Val Loss: 0.0554\n",
      "Epoch [38/200] Train Loss: 0.0539 Val Loss: 0.0545\n",
      "Epoch [39/200] Train Loss: 0.0540 Val Loss: 0.0550\n",
      "Epoch [40/200] Train Loss: 0.0536 Val Loss: 0.0543\n",
      "Epoch [41/200] Train Loss: 0.0536 Val Loss: 0.0552\n",
      "Epoch [42/200] Train Loss: 0.0534 Val Loss: 0.0557\n",
      "Epoch [43/200] Train Loss: 0.0532 Val Loss: 0.0550\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1158 Val Loss: 0.0937\n",
      "Epoch [2/200] Train Loss: 0.0892 Val Loss: 0.0811\n",
      "Epoch [3/200] Train Loss: 0.0777 Val Loss: 0.0741\n",
      "Epoch [4/200] Train Loss: 0.0736 Val Loss: 0.0714\n",
      "Epoch [5/200] Train Loss: 0.0718 Val Loss: 0.0701\n",
      "Epoch [6/200] Train Loss: 0.0708 Val Loss: 0.0691\n",
      "Epoch [7/200] Train Loss: 0.0699 Val Loss: 0.0684\n",
      "Epoch [8/200] Train Loss: 0.0694 Val Loss: 0.0677\n",
      "Epoch [9/200] Train Loss: 0.0689 Val Loss: 0.0673\n",
      "Epoch [10/200] Train Loss: 0.0684 Val Loss: 0.0671\n",
      "Epoch [11/200] Train Loss: 0.0679 Val Loss: 0.0672\n",
      "Epoch [12/200] Train Loss: 0.0677 Val Loss: 0.0665\n",
      "Epoch [13/200] Train Loss: 0.0672 Val Loss: 0.0660\n",
      "Epoch [14/200] Train Loss: 0.0670 Val Loss: 0.0658\n",
      "Epoch [15/200] Train Loss: 0.0666 Val Loss: 0.0659\n",
      "Epoch [16/200] Train Loss: 0.0664 Val Loss: 0.0657\n",
      "Epoch [17/200] Train Loss: 0.0661 Val Loss: 0.0654\n",
      "Epoch [18/200] Train Loss: 0.0659 Val Loss: 0.0650\n",
      "Epoch [19/200] Train Loss: 0.0660 Val Loss: 0.0649\n",
      "Epoch [20/200] Train Loss: 0.0656 Val Loss: 0.0650\n",
      "Epoch [21/200] Train Loss: 0.0653 Val Loss: 0.0654\n",
      "Epoch [22/200] Train Loss: 0.0651 Val Loss: 0.0647\n",
      "Epoch [23/200] Train Loss: 0.0653 Val Loss: 0.0658\n",
      "Epoch [24/200] Train Loss: 0.0648 Val Loss: 0.0644\n",
      "Epoch [25/200] Train Loss: 0.0647 Val Loss: 0.0644\n",
      "Epoch [26/200] Train Loss: 0.0648 Val Loss: 0.0653\n",
      "Epoch [27/200] Train Loss: 0.0646 Val Loss: 0.0641\n",
      "Epoch [28/200] Train Loss: 0.0643 Val Loss: 0.0639\n",
      "Epoch [29/200] Train Loss: 0.0643 Val Loss: 0.0639\n",
      "Epoch [30/200] Train Loss: 0.0641 Val Loss: 0.0648\n",
      "Epoch [31/200] Train Loss: 0.0640 Val Loss: 0.0646\n",
      "Epoch [32/200] Train Loss: 0.0637 Val Loss: 0.0642\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0977 Val Loss: 0.0814\n",
      "Epoch [2/200] Train Loss: 0.0780 Val Loss: 0.0710\n",
      "Epoch [3/200] Train Loss: 0.0709 Val Loss: 0.0676\n",
      "Epoch [4/200] Train Loss: 0.0681 Val Loss: 0.0659\n",
      "Epoch [5/200] Train Loss: 0.0664 Val Loss: 0.0647\n",
      "Epoch [6/200] Train Loss: 0.0654 Val Loss: 0.0643\n",
      "Epoch [7/200] Train Loss: 0.0645 Val Loss: 0.0635\n",
      "Epoch [8/200] Train Loss: 0.0640 Val Loss: 0.0633\n",
      "Epoch [9/200] Train Loss: 0.0633 Val Loss: 0.0628\n",
      "Epoch [10/200] Train Loss: 0.0628 Val Loss: 0.0627\n",
      "Epoch [11/200] Train Loss: 0.0625 Val Loss: 0.0622\n",
      "Epoch [12/200] Train Loss: 0.0621 Val Loss: 0.0619\n",
      "Epoch [13/200] Train Loss: 0.0618 Val Loss: 0.0617\n",
      "Epoch [14/200] Train Loss: 0.0616 Val Loss: 0.0620\n",
      "Epoch [15/200] Train Loss: 0.0613 Val Loss: 0.0617\n",
      "Epoch [16/200] Train Loss: 0.0612 Val Loss: 0.0618\n",
      "Epoch [17/200] Train Loss: 0.0610 Val Loss: 0.0613\n",
      "Epoch [18/200] Train Loss: 0.0608 Val Loss: 0.0615\n",
      "Epoch [19/200] Train Loss: 0.0605 Val Loss: 0.0614\n",
      "Epoch [20/200] Train Loss: 0.0604 Val Loss: 0.0615\n",
      "Epoch [21/200] Train Loss: 0.0605 Val Loss: 0.0631\n",
      "Epoch [22/200] Train Loss: 0.0606 Val Loss: 0.0608\n",
      "Epoch [23/200] Train Loss: 0.0600 Val Loss: 0.0605\n",
      "Epoch [24/200] Train Loss: 0.0601 Val Loss: 0.0615\n",
      "Epoch [25/200] Train Loss: 0.0599 Val Loss: 0.0604\n",
      "Epoch [26/200] Train Loss: 0.0595 Val Loss: 0.0610\n",
      "Epoch [27/200] Train Loss: 0.0596 Val Loss: 0.0614\n",
      "Epoch [28/200] Train Loss: 0.0594 Val Loss: 0.0600\n",
      "Epoch [29/200] Train Loss: 0.0593 Val Loss: 0.0610\n",
      "Epoch [30/200] Train Loss: 0.0592 Val Loss: 0.0598\n",
      "Epoch [31/200] Train Loss: 0.0591 Val Loss: 0.0610\n",
      "Epoch [32/200] Train Loss: 0.0590 Val Loss: 0.0601\n",
      "Epoch [33/200] Train Loss: 0.0587 Val Loss: 0.0605\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1014 Val Loss: 0.0837\n",
      "Epoch [2/200] Train Loss: 0.0824 Val Loss: 0.0747\n",
      "Epoch [3/200] Train Loss: 0.0737 Val Loss: 0.0688\n",
      "Epoch [4/200] Train Loss: 0.0701 Val Loss: 0.0671\n",
      "Epoch [5/200] Train Loss: 0.0685 Val Loss: 0.0661\n",
      "Epoch [6/200] Train Loss: 0.0672 Val Loss: 0.0643\n",
      "Epoch [7/200] Train Loss: 0.0666 Val Loss: 0.0635\n",
      "Epoch [8/200] Train Loss: 0.0659 Val Loss: 0.0633\n",
      "Epoch [9/200] Train Loss: 0.0654 Val Loss: 0.0626\n",
      "Epoch [10/200] Train Loss: 0.0648 Val Loss: 0.0620\n",
      "Epoch [11/200] Train Loss: 0.0647 Val Loss: 0.0617\n",
      "Epoch [12/200] Train Loss: 0.0640 Val Loss: 0.0615\n",
      "Epoch [13/200] Train Loss: 0.0638 Val Loss: 0.0619\n",
      "Epoch [14/200] Train Loss: 0.0636 Val Loss: 0.0622\n",
      "Epoch [15/200] Train Loss: 0.0632 Val Loss: 0.0609\n",
      "Epoch [16/200] Train Loss: 0.0629 Val Loss: 0.0606\n",
      "Epoch [17/200] Train Loss: 0.0628 Val Loss: 0.0608\n",
      "Epoch [18/200] Train Loss: 0.0623 Val Loss: 0.0602\n",
      "Epoch [19/200] Train Loss: 0.0623 Val Loss: 0.0607\n",
      "Epoch [20/200] Train Loss: 0.0620 Val Loss: 0.0600\n",
      "Epoch [21/200] Train Loss: 0.0617 Val Loss: 0.0600\n",
      "Epoch [22/200] Train Loss: 0.0616 Val Loss: 0.0606\n",
      "Epoch [23/200] Train Loss: 0.0615 Val Loss: 0.0595\n",
      "Epoch [24/200] Train Loss: 0.0610 Val Loss: 0.0596\n",
      "Epoch [25/200] Train Loss: 0.0607 Val Loss: 0.0594\n",
      "Epoch [26/200] Train Loss: 0.0606 Val Loss: 0.0593\n",
      "Epoch [27/200] Train Loss: 0.0605 Val Loss: 0.0591\n",
      "Epoch [28/200] Train Loss: 0.0603 Val Loss: 0.0591\n",
      "Epoch [29/200] Train Loss: 0.0602 Val Loss: 0.0588\n",
      "Epoch [30/200] Train Loss: 0.0599 Val Loss: 0.0588\n",
      "Epoch [31/200] Train Loss: 0.0598 Val Loss: 0.0587\n",
      "Epoch [32/200] Train Loss: 0.0595 Val Loss: 0.0590\n",
      "Epoch [33/200] Train Loss: 0.0592 Val Loss: 0.0584\n",
      "Epoch [34/200] Train Loss: 0.0592 Val Loss: 0.0593\n",
      "Epoch [35/200] Train Loss: 0.0589 Val Loss: 0.0581\n",
      "Epoch [36/200] Train Loss: 0.0588 Val Loss: 0.0593\n",
      "Epoch [37/200] Train Loss: 0.0585 Val Loss: 0.0587\n",
      "Epoch [38/200] Train Loss: 0.0583 Val Loss: 0.0579\n",
      "Epoch [39/200] Train Loss: 0.0583 Val Loss: 0.0580\n",
      "Epoch [40/200] Train Loss: 0.0579 Val Loss: 0.0583\n",
      "Epoch [41/200] Train Loss: 0.0577 Val Loss: 0.0579\n",
      "Epoch [42/200] Train Loss: 0.0578 Val Loss: 0.0575\n",
      "Epoch [43/200] Train Loss: 0.0575 Val Loss: 0.0574\n",
      "Epoch [44/200] Train Loss: 0.0573 Val Loss: 0.0573\n",
      "Epoch [45/200] Train Loss: 0.0570 Val Loss: 0.0574\n",
      "Epoch [46/200] Train Loss: 0.0569 Val Loss: 0.0571\n",
      "Epoch [47/200] Train Loss: 0.0566 Val Loss: 0.0571\n",
      "Epoch [48/200] Train Loss: 0.0566 Val Loss: 0.0571\n",
      "Epoch [49/200] Train Loss: 0.0564 Val Loss: 0.0568\n",
      "Epoch [50/200] Train Loss: 0.0563 Val Loss: 0.0571\n",
      "Epoch [51/200] Train Loss: 0.0559 Val Loss: 0.0576\n",
      "Epoch [52/200] Train Loss: 0.0559 Val Loss: 0.0568\n",
      "Epoch [53/200] Train Loss: 0.0556 Val Loss: 0.0567\n",
      "Epoch [54/200] Train Loss: 0.0556 Val Loss: 0.0562\n",
      "Epoch [55/200] Train Loss: 0.0554 Val Loss: 0.0562\n",
      "Epoch [56/200] Train Loss: 0.0550 Val Loss: 0.0560\n",
      "Epoch [57/200] Train Loss: 0.0551 Val Loss: 0.0564\n",
      "Epoch [58/200] Train Loss: 0.0548 Val Loss: 0.0561\n",
      "Epoch [59/200] Train Loss: 0.0545 Val Loss: 0.0558\n",
      "Epoch [60/200] Train Loss: 0.0544 Val Loss: 0.0558\n",
      "Epoch [61/200] Train Loss: 0.0542 Val Loss: 0.0558\n",
      "Epoch [62/200] Train Loss: 0.0540 Val Loss: 0.0559\n",
      "Epoch [63/200] Train Loss: 0.0539 Val Loss: 0.0554\n",
      "Epoch [64/200] Train Loss: 0.0537 Val Loss: 0.0550\n",
      "Epoch [65/200] Train Loss: 0.0534 Val Loss: 0.0554\n",
      "Epoch [66/200] Train Loss: 0.0536 Val Loss: 0.0549\n",
      "Epoch [67/200] Train Loss: 0.0531 Val Loss: 0.0547\n",
      "Epoch [68/200] Train Loss: 0.0529 Val Loss: 0.0546\n",
      "Epoch [69/200] Train Loss: 0.0528 Val Loss: 0.0546\n",
      "Epoch [70/200] Train Loss: 0.0526 Val Loss: 0.0557\n",
      "Epoch [71/200] Train Loss: 0.0524 Val Loss: 0.0551\n",
      "Epoch [72/200] Train Loss: 0.0523 Val Loss: 0.0544\n",
      "Epoch [73/200] Train Loss: 0.0521 Val Loss: 0.0542\n",
      "Epoch [74/200] Train Loss: 0.0520 Val Loss: 0.0540\n",
      "Epoch [75/200] Train Loss: 0.0520 Val Loss: 0.0543\n",
      "Epoch [76/200] Train Loss: 0.0517 Val Loss: 0.0541\n",
      "Epoch [77/200] Train Loss: 0.0515 Val Loss: 0.0541\n",
      "Epoch [78/200] Train Loss: 0.0512 Val Loss: 0.0536\n",
      "Epoch [79/200] Train Loss: 0.0511 Val Loss: 0.0539\n",
      "Epoch [80/200] Train Loss: 0.0512 Val Loss: 0.0536\n",
      "Epoch [81/200] Train Loss: 0.0507 Val Loss: 0.0533\n",
      "Epoch [82/200] Train Loss: 0.0505 Val Loss: 0.0533\n",
      "Epoch [83/200] Train Loss: 0.0504 Val Loss: 0.0533\n",
      "Epoch [84/200] Train Loss: 0.0503 Val Loss: 0.0531\n",
      "Epoch [85/200] Train Loss: 0.0500 Val Loss: 0.0528\n",
      "Epoch [86/200] Train Loss: 0.0499 Val Loss: 0.0524\n",
      "Epoch [87/200] Train Loss: 0.0499 Val Loss: 0.0523\n",
      "Epoch [88/200] Train Loss: 0.0499 Val Loss: 0.0524\n",
      "Epoch [89/200] Train Loss: 0.0495 Val Loss: 0.0525\n",
      "Epoch [90/200] Train Loss: 0.0493 Val Loss: 0.0532\n",
      "Epoch [91/200] Train Loss: 0.0491 Val Loss: 0.0520\n",
      "Epoch [92/200] Train Loss: 0.0492 Val Loss: 0.0515\n",
      "Epoch [93/200] Train Loss: 0.0488 Val Loss: 0.0520\n",
      "Epoch [94/200] Train Loss: 0.0484 Val Loss: 0.0524\n",
      "Epoch [95/200] Train Loss: 0.0484 Val Loss: 0.0513\n",
      "Epoch [96/200] Train Loss: 0.0479 Val Loss: 0.0514\n",
      "Epoch [97/200] Train Loss: 0.0481 Val Loss: 0.0527\n",
      "Epoch [98/200] Train Loss: 0.0476 Val Loss: 0.0512\n",
      "Epoch [99/200] Train Loss: 0.0480 Val Loss: 0.0508\n",
      "Epoch [100/200] Train Loss: 0.0471 Val Loss: 0.0507\n",
      "Epoch [101/200] Train Loss: 0.0470 Val Loss: 0.0513\n",
      "Epoch [102/200] Train Loss: 0.0472 Val Loss: 0.0511\n",
      "Epoch [103/200] Train Loss: 0.0468 Val Loss: 0.0502\n",
      "Epoch [104/200] Train Loss: 0.0465 Val Loss: 0.0505\n",
      "Epoch [105/200] Train Loss: 0.0464 Val Loss: 0.0499\n",
      "Epoch [106/200] Train Loss: 0.0463 Val Loss: 0.0508\n",
      "Epoch [107/200] Train Loss: 0.0460 Val Loss: 0.0503\n",
      "Epoch [108/200] Train Loss: 0.0461 Val Loss: 0.0500\n",
      "Epoch [109/200] Train Loss: 0.0457 Val Loss: 0.0494\n",
      "Epoch [110/200] Train Loss: 0.0454 Val Loss: 0.0491\n",
      "Epoch [111/200] Train Loss: 0.0453 Val Loss: 0.0488\n",
      "Epoch [112/200] Train Loss: 0.0450 Val Loss: 0.0487\n",
      "Epoch [113/200] Train Loss: 0.0448 Val Loss: 0.0489\n",
      "Epoch [114/200] Train Loss: 0.0449 Val Loss: 0.0486\n",
      "Epoch [115/200] Train Loss: 0.0447 Val Loss: 0.0495\n",
      "Epoch [116/200] Train Loss: 0.0447 Val Loss: 0.0482\n",
      "Epoch [117/200] Train Loss: 0.0448 Val Loss: 0.0488\n",
      "Epoch [118/200] Train Loss: 0.0444 Val Loss: 0.0484\n",
      "Epoch [119/200] Train Loss: 0.0439 Val Loss: 0.0485\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0964 Val Loss: 0.0809\n",
      "Epoch [2/200] Train Loss: 0.0776 Val Loss: 0.0709\n",
      "Epoch [3/200] Train Loss: 0.0680 Val Loss: 0.0660\n",
      "Epoch [4/200] Train Loss: 0.0643 Val Loss: 0.0626\n",
      "Epoch [5/200] Train Loss: 0.0625 Val Loss: 0.0611\n",
      "Epoch [6/200] Train Loss: 0.0614 Val Loss: 0.0604\n",
      "Epoch [7/200] Train Loss: 0.0606 Val Loss: 0.0598\n",
      "Epoch [8/200] Train Loss: 0.0600 Val Loss: 0.0593\n",
      "Epoch [9/200] Train Loss: 0.0597 Val Loss: 0.0604\n",
      "Epoch [10/200] Train Loss: 0.0591 Val Loss: 0.0588\n",
      "Epoch [11/200] Train Loss: 0.0586 Val Loss: 0.0592\n",
      "Epoch [12/200] Train Loss: 0.0583 Val Loss: 0.0583\n",
      "Epoch [13/200] Train Loss: 0.0579 Val Loss: 0.0584\n",
      "Epoch [14/200] Train Loss: 0.0577 Val Loss: 0.0593\n",
      "Epoch [15/200] Train Loss: 0.0575 Val Loss: 0.0581\n",
      "Epoch [16/200] Train Loss: 0.0574 Val Loss: 0.0574\n",
      "Epoch [17/200] Train Loss: 0.0570 Val Loss: 0.0572\n",
      "Epoch [18/200] Train Loss: 0.0566 Val Loss: 0.0570\n",
      "Epoch [19/200] Train Loss: 0.0565 Val Loss: 0.0569\n",
      "Epoch [20/200] Train Loss: 0.0563 Val Loss: 0.0574\n",
      "Epoch [21/200] Train Loss: 0.0559 Val Loss: 0.0568\n",
      "Epoch [22/200] Train Loss: 0.0558 Val Loss: 0.0568\n",
      "Epoch [23/200] Train Loss: 0.0557 Val Loss: 0.0570\n",
      "Epoch [24/200] Train Loss: 0.0557 Val Loss: 0.0568\n",
      "Epoch [25/200] Train Loss: 0.0553 Val Loss: 0.0564\n",
      "Epoch [26/200] Train Loss: 0.0551 Val Loss: 0.0559\n",
      "Epoch [27/200] Train Loss: 0.0548 Val Loss: 0.0559\n",
      "Epoch [28/200] Train Loss: 0.0550 Val Loss: 0.0562\n",
      "Epoch [29/200] Train Loss: 0.0547 Val Loss: 0.0568\n",
      "Epoch [30/200] Train Loss: 0.0544 Val Loss: 0.0555\n",
      "Epoch [31/200] Train Loss: 0.0542 Val Loss: 0.0556\n",
      "Epoch [32/200] Train Loss: 0.0540 Val Loss: 0.0556\n",
      "Epoch [33/200] Train Loss: 0.0539 Val Loss: 0.0565\n",
      "Epoch [34/200] Train Loss: 0.0538 Val Loss: 0.0554\n",
      "Epoch [35/200] Train Loss: 0.0537 Val Loss: 0.0550\n",
      "Epoch [36/200] Train Loss: 0.0534 Val Loss: 0.0558\n",
      "Epoch [37/200] Train Loss: 0.0533 Val Loss: 0.0551\n",
      "Epoch [38/200] Train Loss: 0.0533 Val Loss: 0.0548\n",
      "Epoch [39/200] Train Loss: 0.0534 Val Loss: 0.0557\n",
      "Epoch [40/200] Train Loss: 0.0530 Val Loss: 0.0565\n",
      "Epoch [41/200] Train Loss: 0.0527 Val Loss: 0.0571\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1051 Val Loss: 0.0865\n",
      "Epoch [2/200] Train Loss: 0.0838 Val Loss: 0.0765\n",
      "Epoch [3/200] Train Loss: 0.0750 Val Loss: 0.0713\n",
      "Epoch [4/200] Train Loss: 0.0710 Val Loss: 0.0694\n",
      "Epoch [5/200] Train Loss: 0.0689 Val Loss: 0.0675\n",
      "Epoch [6/200] Train Loss: 0.0677 Val Loss: 0.0667\n",
      "Epoch [7/200] Train Loss: 0.0667 Val Loss: 0.0660\n",
      "Epoch [8/200] Train Loss: 0.0661 Val Loss: 0.0655\n",
      "Epoch [9/200] Train Loss: 0.0655 Val Loss: 0.0654\n",
      "Epoch [10/200] Train Loss: 0.0649 Val Loss: 0.0652\n",
      "Epoch [11/200] Train Loss: 0.0644 Val Loss: 0.0645\n",
      "Epoch [12/200] Train Loss: 0.0641 Val Loss: 0.0642\n",
      "Epoch [13/200] Train Loss: 0.0638 Val Loss: 0.0640\n",
      "Epoch [14/200] Train Loss: 0.0634 Val Loss: 0.0640\n",
      "Epoch [15/200] Train Loss: 0.0630 Val Loss: 0.0641\n",
      "Epoch [16/200] Train Loss: 0.0628 Val Loss: 0.0646\n",
      "Epoch [17/200] Train Loss: 0.0626 Val Loss: 0.0635\n",
      "Epoch [18/200] Train Loss: 0.0622 Val Loss: 0.0638\n",
      "Epoch [19/200] Train Loss: 0.0621 Val Loss: 0.0629\n",
      "Epoch [20/200] Train Loss: 0.0618 Val Loss: 0.0637\n",
      "Epoch [21/200] Train Loss: 0.0617 Val Loss: 0.0629\n",
      "Epoch [22/200] Train Loss: 0.0614 Val Loss: 0.0626\n",
      "Epoch [23/200] Train Loss: 0.0610 Val Loss: 0.0624\n",
      "Epoch [24/200] Train Loss: 0.0610 Val Loss: 0.0625\n",
      "Epoch [25/200] Train Loss: 0.0608 Val Loss: 0.0630\n",
      "Epoch [26/200] Train Loss: 0.0606 Val Loss: 0.0642\n",
      "Epoch [27/200] Train Loss: 0.0603 Val Loss: 0.0617\n",
      "Epoch [28/200] Train Loss: 0.0603 Val Loss: 0.0629\n",
      "Epoch [29/200] Train Loss: 0.0601 Val Loss: 0.0620\n",
      "Epoch [30/200] Train Loss: 0.0598 Val Loss: 0.0623\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "server.local_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [2/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [3/20] Train Loss: 0.0528 Val Loss: 0.0516\n",
      "Epoch [4/20] Train Loss: 0.0526 Val Loss: 0.0515\n",
      "Epoch [5/20] Train Loss: 0.0526 Val Loss: 0.0515\n",
      "Epoch [6/20] Train Loss: 0.0525 Val Loss: 0.0514\n",
      "Epoch [7/20] Train Loss: 0.0525 Val Loss: 0.0514\n",
      "Epoch [8/20] Train Loss: 0.0524 Val Loss: 0.0514\n",
      "Epoch [9/20] Train Loss: 0.0524 Val Loss: 0.0514\n",
      "Epoch [10/20] Train Loss: 0.0524 Val Loss: 0.0516\n",
      "Epoch [11/20] Train Loss: 0.0523 Val Loss: 0.0513\n",
      "Epoch [12/20] Train Loss: 0.0522 Val Loss: 0.0513\n",
      "Epoch [13/20] Train Loss: 0.0522 Val Loss: 0.0512\n",
      "Epoch [14/20] Train Loss: 0.0522 Val Loss: 0.0512\n",
      "Epoch [15/20] Train Loss: 0.0521 Val Loss: 0.0512\n",
      "Epoch [16/20] Train Loss: 0.0521 Val Loss: 0.0512\n",
      "Epoch [17/20] Train Loss: 0.0520 Val Loss: 0.0512\n",
      "Epoch [18/20] Train Loss: 0.0520 Val Loss: 0.0511\n",
      "Epoch [19/20] Train Loss: 0.0520 Val Loss: 0.0511\n",
      "Epoch [20/20] Train Loss: 0.0520 Val Loss: 0.0511\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0585 Val Loss: 0.0574\n",
      "Epoch [2/20] Train Loss: 0.0585 Val Loss: 0.0573\n",
      "Epoch [3/20] Train Loss: 0.0584 Val Loss: 0.0572\n",
      "Epoch [4/20] Train Loss: 0.0583 Val Loss: 0.0573\n",
      "Epoch [5/20] Train Loss: 0.0582 Val Loss: 0.0571\n",
      "Epoch [6/20] Train Loss: 0.0581 Val Loss: 0.0570\n",
      "Epoch [7/20] Train Loss: 0.0580 Val Loss: 0.0569\n",
      "Epoch [8/20] Train Loss: 0.0580 Val Loss: 0.0569\n",
      "Epoch [9/20] Train Loss: 0.0579 Val Loss: 0.0568\n",
      "Epoch [10/20] Train Loss: 0.0578 Val Loss: 0.0568\n",
      "Epoch [11/20] Train Loss: 0.0578 Val Loss: 0.0568\n",
      "Epoch [12/20] Train Loss: 0.0577 Val Loss: 0.0568\n",
      "Epoch [13/20] Train Loss: 0.0577 Val Loss: 0.0567\n",
      "Epoch [14/20] Train Loss: 0.0576 Val Loss: 0.0567\n",
      "Epoch [15/20] Train Loss: 0.0576 Val Loss: 0.0567\n",
      "Epoch [16/20] Train Loss: 0.0575 Val Loss: 0.0567\n",
      "Epoch [17/20] Train Loss: 0.0575 Val Loss: 0.0566\n",
      "Epoch [18/20] Train Loss: 0.0574 Val Loss: 0.0565\n",
      "Epoch [19/20] Train Loss: 0.0574 Val Loss: 0.0565\n",
      "Epoch [20/20] Train Loss: 0.0573 Val Loss: 0.0564\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0648 Val Loss: 0.0643\n",
      "Epoch [2/20] Train Loss: 0.0650 Val Loss: 0.0642\n",
      "Epoch [3/20] Train Loss: 0.0649 Val Loss: 0.0641\n",
      "Epoch [4/20] Train Loss: 0.0648 Val Loss: 0.0642\n",
      "Epoch [5/20] Train Loss: 0.0647 Val Loss: 0.0642\n",
      "Epoch [6/20] Train Loss: 0.0647 Val Loss: 0.0641\n",
      "Epoch [7/20] Train Loss: 0.0646 Val Loss: 0.0641\n",
      "Epoch [8/20] Train Loss: 0.0646 Val Loss: 0.0641\n",
      "Epoch [9/20] Train Loss: 0.0645 Val Loss: 0.0641\n",
      "Epoch [10/20] Train Loss: 0.0645 Val Loss: 0.0641\n",
      "Epoch [11/20] Train Loss: 0.0644 Val Loss: 0.0640\n",
      "Epoch [12/20] Train Loss: 0.0644 Val Loss: 0.0639\n",
      "Epoch [13/20] Train Loss: 0.0644 Val Loss: 0.0639\n",
      "Epoch [14/20] Train Loss: 0.0643 Val Loss: 0.0640\n",
      "Epoch [15/20] Train Loss: 0.0643 Val Loss: 0.0639\n",
      "Epoch [16/20] Train Loss: 0.0643 Val Loss: 0.0639\n",
      "Epoch [17/20] Train Loss: 0.0642 Val Loss: 0.0638\n",
      "Epoch [18/20] Train Loss: 0.0642 Val Loss: 0.0638\n",
      "Epoch [19/20] Train Loss: 0.0642 Val Loss: 0.0638\n",
      "Epoch [20/20] Train Loss: 0.0641 Val Loss: 0.0638\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0583 Val Loss: 0.0590\n",
      "Epoch [2/20] Train Loss: 0.0583 Val Loss: 0.0590\n",
      "Epoch [3/20] Train Loss: 0.0582 Val Loss: 0.0590\n",
      "Epoch [4/20] Train Loss: 0.0582 Val Loss: 0.0589\n",
      "Epoch [5/20] Train Loss: 0.0582 Val Loss: 0.0589\n",
      "Epoch [6/20] Train Loss: 0.0581 Val Loss: 0.0589\n",
      "Epoch [7/20] Train Loss: 0.0581 Val Loss: 0.0588\n",
      "Epoch [8/20] Train Loss: 0.0581 Val Loss: 0.0588\n",
      "Epoch [9/20] Train Loss: 0.0580 Val Loss: 0.0589\n",
      "Epoch [10/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [11/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [12/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [13/20] Train Loss: 0.0578 Val Loss: 0.0587\n",
      "Epoch [14/20] Train Loss: 0.0578 Val Loss: 0.0587\n",
      "Epoch [15/20] Train Loss: 0.0578 Val Loss: 0.0587\n",
      "Epoch [16/20] Train Loss: 0.0578 Val Loss: 0.0588\n",
      "Epoch [17/20] Train Loss: 0.0577 Val Loss: 0.0588\n",
      "Epoch [18/20] Train Loss: 0.0577 Val Loss: 0.0588\n",
      "Epoch [19/20] Train Loss: 0.0577 Val Loss: 0.0586\n",
      "Epoch [20/20] Train Loss: 0.0576 Val Loss: 0.0586\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0620 Val Loss: 0.0605\n",
      "Epoch [2/20] Train Loss: 0.0621 Val Loss: 0.0604\n",
      "Epoch [3/20] Train Loss: 0.0620 Val Loss: 0.0603\n",
      "Epoch [4/20] Train Loss: 0.0618 Val Loss: 0.0603\n",
      "Epoch [5/20] Train Loss: 0.0618 Val Loss: 0.0602\n",
      "Epoch [6/20] Train Loss: 0.0617 Val Loss: 0.0601\n",
      "Epoch [7/20] Train Loss: 0.0617 Val Loss: 0.0601\n",
      "Epoch [8/20] Train Loss: 0.0616 Val Loss: 0.0601\n",
      "Epoch [9/20] Train Loss: 0.0616 Val Loss: 0.0600\n",
      "Epoch [10/20] Train Loss: 0.0615 Val Loss: 0.0600\n",
      "Epoch [11/20] Train Loss: 0.0615 Val Loss: 0.0599\n",
      "Epoch [12/20] Train Loss: 0.0614 Val Loss: 0.0599\n",
      "Epoch [13/20] Train Loss: 0.0613 Val Loss: 0.0599\n",
      "Epoch [14/20] Train Loss: 0.0613 Val Loss: 0.0598\n",
      "Epoch [15/20] Train Loss: 0.0613 Val Loss: 0.0598\n",
      "Epoch [16/20] Train Loss: 0.0613 Val Loss: 0.0599\n",
      "Epoch [17/20] Train Loss: 0.0612 Val Loss: 0.0600\n",
      "Epoch [18/20] Train Loss: 0.0612 Val Loss: 0.0597\n",
      "Epoch [19/20] Train Loss: 0.0611 Val Loss: 0.0598\n",
      "Epoch [20/20] Train Loss: 0.0611 Val Loss: 0.0597\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0551 Val Loss: 0.0559\n",
      "Epoch [2/20] Train Loss: 0.0552 Val Loss: 0.0558\n",
      "Epoch [3/20] Train Loss: 0.0551 Val Loss: 0.0558\n",
      "Epoch [4/20] Train Loss: 0.0551 Val Loss: 0.0558\n",
      "Epoch [5/20] Train Loss: 0.0550 Val Loss: 0.0558\n",
      "Epoch [6/20] Train Loss: 0.0550 Val Loss: 0.0558\n",
      "Epoch [7/20] Train Loss: 0.0550 Val Loss: 0.0557\n",
      "Epoch [8/20] Train Loss: 0.0550 Val Loss: 0.0557\n",
      "Epoch [9/20] Train Loss: 0.0549 Val Loss: 0.0557\n",
      "Epoch [10/20] Train Loss: 0.0549 Val Loss: 0.0557\n",
      "Epoch [11/20] Train Loss: 0.0549 Val Loss: 0.0557\n",
      "Epoch [12/20] Train Loss: 0.0548 Val Loss: 0.0556\n",
      "Epoch [13/20] Train Loss: 0.0548 Val Loss: 0.0556\n",
      "Epoch [14/20] Train Loss: 0.0548 Val Loss: 0.0556\n",
      "Epoch [15/20] Train Loss: 0.0547 Val Loss: 0.0555\n",
      "Epoch [16/20] Train Loss: 0.0547 Val Loss: 0.0556\n",
      "Epoch [17/20] Train Loss: 0.0547 Val Loss: 0.0555\n",
      "Epoch [18/20] Train Loss: 0.0546 Val Loss: 0.0555\n",
      "Epoch [19/20] Train Loss: 0.0546 Val Loss: 0.0557\n",
      "Epoch [20/20] Train Loss: 0.0546 Val Loss: 0.0554\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0609 Val Loss: 0.0625\n",
      "Epoch [2/20] Train Loss: 0.0610 Val Loss: 0.0620\n",
      "Epoch [3/20] Train Loss: 0.0609 Val Loss: 0.0619\n",
      "Epoch [4/20] Train Loss: 0.0609 Val Loss: 0.0619\n",
      "Epoch [5/20] Train Loss: 0.0608 Val Loss: 0.0619\n",
      "Epoch [6/20] Train Loss: 0.0607 Val Loss: 0.0619\n",
      "Epoch [7/20] Train Loss: 0.0607 Val Loss: 0.0617\n",
      "Epoch [8/20] Train Loss: 0.0607 Val Loss: 0.0617\n",
      "Epoch [9/20] Train Loss: 0.0606 Val Loss: 0.0618\n",
      "Epoch [10/20] Train Loss: 0.0606 Val Loss: 0.0617\n",
      "Epoch [11/20] Train Loss: 0.0605 Val Loss: 0.0616\n",
      "Epoch [12/20] Train Loss: 0.0605 Val Loss: 0.0616\n",
      "Epoch [13/20] Train Loss: 0.0605 Val Loss: 0.0616\n",
      "Epoch [14/20] Train Loss: 0.0604 Val Loss: 0.0615\n",
      "Epoch [15/20] Train Loss: 0.0604 Val Loss: 0.0617\n",
      "Epoch [16/20] Train Loss: 0.0603 Val Loss: 0.0615\n",
      "Epoch [17/20] Train Loss: 0.0603 Val Loss: 0.0615\n",
      "Epoch [18/20] Train Loss: 0.0603 Val Loss: 0.0615\n",
      "Epoch [19/20] Train Loss: 0.0602 Val Loss: 0.0615\n",
      "Epoch [20/20] Train Loss: 0.0602 Val Loss: 0.0614\n",
      "[0.05532612334554122, 0.0637155881228104, 0.0681344347635973, 0.06144858836770466, 0.06267848048257092, 0.06014074691354412, 0.06467975304168586]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_losses=[]\n",
    "local_fine_tune_preds=[]\n",
    "local_fine_tune_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(args_train.number_clients):\n",
    "    local_fine_tune_pred,local_fine_tune_loss,local_fine_tune_model=clients[i].local_fine_tune()\n",
    "    local_fine_tune_losses.append(local_fine_tune_loss)\n",
    "    local_fine_tune_preds.append(local_fine_tune_pred)\n",
    "    local_fine_tune_models.append(local_fine_tune_model)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import utils import plot_prob_result\\nargs_temp=copy.deepcopy(args_train)\\nargs_temp.dataset_paths='wf7'\\ntest_data, test_loader = get_data(args_train,flag='test')\\nactual_y=[]\\nfor idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\\n    actual_y.append(seq_y)\\nactual_y = torch.cat([torch.flatten(t) for t in actual_y])\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import utils import plot_prob_result\n",
    "args_temp=copy.deepcopy(args_train)\n",
    "args_temp.dataset_paths='wf7'\n",
    "test_data, test_loader = get_data(args_train,flag='test')\n",
    "actual_y=[]\n",
    "for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "    actual_y.append(seq_y)\n",
    "actual_y = torch.cat([torch.flatten(t) for t in actual_y])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05583555196501212, 0.06445825300243212, 0.06835072321740732, 0.06237655860206036, 0.06335907018016258, 0.060239760326349166, 0.06499778458925143]\n",
      "0.06280252884038215\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)\n",
    "print(np.mean(fed_local_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05624059526479407, 0.067097952398025, 0.06974936732129283, 0.06228208582057324, 0.06914129998688012, 0.06154689922520559, 0.06558624371146299]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    local_pred,local_loss,local_model=clients[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05901773436607359, 0.07539156261729459, 0.08007056001982052, 0.06832268555355195, 0.07501274052599113, 0.06960178451127794, 0.07918094079431197]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    central_pred,central_loss,central_model=server.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05624059526479407, 0.067097952398025, 0.06974936732129283, 0.06228208582057324, 0.06914129998688012, 0.06154689922520559, 0.06558624371146299]\n",
      "[0.05901773436607359, 0.07539156261729459, 0.08007056001982052, 0.06832268555355195, 0.07501274052599113, 0.06960178451127794, 0.07918094079431197]\n",
      "[0.05583555196501212, 0.06445825300243212, 0.06835072321740732, 0.06237655860206036, 0.06335907018016258, 0.060239760326349166, 0.06499778458925143]\n",
      "[0.05532612334554122, 0.0637155881228104, 0.0681344347635973, 0.06144858836770466, 0.06267848048257092, 0.06014074691354412, 0.06467975304168586]\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_losses': local_fine_tune_losses\n",
    "})\n",
    "df.T.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the server object\n",
    "with open('../result/6/server_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "# Save the clients object\n",
    "with open('../result/6/clients_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save server and clients\n",
    "with open('server.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "with open('clients.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)\n",
    "\n",
    "# Load server and clients\n",
    "with open('server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "with open('clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
