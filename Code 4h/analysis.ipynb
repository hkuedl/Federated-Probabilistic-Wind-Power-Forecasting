{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "from utils import get_data, plot_prob_result,seed_everything\n",
    "import copy\n",
    "import pandas as pd\n",
    "# Load the server object\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "from torch.nn import L1Loss\n",
    "scale=4\n",
    "\n",
    "with open('../result/'+str(scale)+'/server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04661360359431742, 0.05353561454541879, 0.0576981087279034, 0.05322195598472879, 0.052451892526284474, 0.05084744619511782, 0.05232491294776842]\n"
     ]
    }
   ],
   "source": [
    "fed_local_proposed_losses=[]\n",
    "fed_local_proposed_preds=[]\n",
    "fed_local_proposed_models=[]\n",
    "for i in range(7):\n",
    "    fed_local_proposed_pred,fed_local_proposed_loss,fed_local_proposed_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_proposed_losses.append(fed_local_proposed_loss[server.index_set[i]])\n",
    "    fed_local_proposed_preds.append(fed_local_proposed_pred[server.index_set[i]])\n",
    "    fed_local_proposed_models.append(fed_local_proposed_model[server.index_set[i]])\n",
    "print(fed_local_proposed_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0386 Val Loss: 0.0398\n",
      "Epoch [2/20] Train Loss: 0.0384 Val Loss: 0.0397\n",
      "Epoch [3/20] Train Loss: 0.0384 Val Loss: 0.0396\n",
      "Epoch [4/20] Train Loss: 0.0383 Val Loss: 0.0396\n",
      "Epoch [5/20] Train Loss: 0.0383 Val Loss: 0.0396\n",
      "Epoch [6/20] Train Loss: 0.0382 Val Loss: 0.0396\n",
      "Epoch [7/20] Train Loss: 0.0382 Val Loss: 0.0396\n",
      "Epoch [8/20] Train Loss: 0.0382 Val Loss: 0.0396\n",
      "Epoch [9/20] Train Loss: 0.0381 Val Loss: 0.0395\n",
      "Epoch [10/20] Train Loss: 0.0381 Val Loss: 0.0397\n",
      "Epoch [11/20] Train Loss: 0.0381 Val Loss: 0.0395\n",
      "Epoch [12/20] Train Loss: 0.0381 Val Loss: 0.0396\n",
      "Epoch [13/20] Train Loss: 0.0381 Val Loss: 0.0395\n",
      "Epoch [14/20] Train Loss: 0.0381 Val Loss: 0.0394\n",
      "Epoch [15/20] Train Loss: 0.0380 Val Loss: 0.0395\n",
      "Epoch [16/20] Train Loss: 0.0380 Val Loss: 0.0397\n",
      "Epoch [17/20] Train Loss: 0.0380 Val Loss: 0.0395\n",
      "Epoch [18/20] Train Loss: 0.0380 Val Loss: 0.0397\n",
      "Epoch [19/20] Train Loss: 0.0380 Val Loss: 0.0394\n",
      "Epoch [20/20] Train Loss: 0.0380 Val Loss: 0.0393\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0468 Val Loss: 0.0470\n",
      "Epoch [2/20] Train Loss: 0.0468 Val Loss: 0.0469\n",
      "Epoch [3/20] Train Loss: 0.0468 Val Loss: 0.0468\n",
      "Epoch [4/20] Train Loss: 0.0468 Val Loss: 0.0468\n",
      "Epoch [5/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "Epoch [6/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "Epoch [7/20] Train Loss: 0.0467 Val Loss: 0.0468\n",
      "Epoch [8/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [9/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [10/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [11/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [12/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [13/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [14/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [15/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [16/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [17/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [18/20] Train Loss: 0.0466 Val Loss: 0.0467\n",
      "Epoch [19/20] Train Loss: 0.0467 Val Loss: 0.0471\n",
      "Epoch [20/20] Train Loss: 0.0467 Val Loss: 0.0466\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0482 Val Loss: 0.0488\n",
      "Epoch [2/20] Train Loss: 0.0478 Val Loss: 0.0486\n",
      "Epoch [3/20] Train Loss: 0.0476 Val Loss: 0.0485\n",
      "Epoch [4/20] Train Loss: 0.0476 Val Loss: 0.0484\n",
      "Epoch [5/20] Train Loss: 0.0475 Val Loss: 0.0486\n",
      "Epoch [6/20] Train Loss: 0.0474 Val Loss: 0.0483\n",
      "Epoch [7/20] Train Loss: 0.0475 Val Loss: 0.0485\n",
      "Epoch [8/20] Train Loss: 0.0474 Val Loss: 0.0483\n",
      "Epoch [9/20] Train Loss: 0.0474 Val Loss: 0.0483\n",
      "Epoch [10/20] Train Loss: 0.0473 Val Loss: 0.0483\n",
      "Epoch [11/20] Train Loss: 0.0473 Val Loss: 0.0483\n",
      "Epoch [12/20] Train Loss: 0.0473 Val Loss: 0.0483\n",
      "Epoch [13/20] Train Loss: 0.0473 Val Loss: 0.0483\n",
      "Epoch [14/20] Train Loss: 0.0473 Val Loss: 0.0483\n",
      "Epoch [15/20] Train Loss: 0.0473 Val Loss: 0.0486\n",
      "Epoch [16/20] Train Loss: 0.0473 Val Loss: 0.0482\n",
      "Epoch [17/20] Train Loss: 0.0472 Val Loss: 0.0482\n",
      "Epoch [18/20] Train Loss: 0.0473 Val Loss: 0.0482\n",
      "Epoch [19/20] Train Loss: 0.0472 Val Loss: 0.0482\n",
      "Epoch [20/20] Train Loss: 0.0472 Val Loss: 0.0482\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0428 Val Loss: 0.0421\n",
      "Epoch [2/20] Train Loss: 0.0419 Val Loss: 0.0420\n",
      "Epoch [3/20] Train Loss: 0.0417 Val Loss: 0.0417\n",
      "Epoch [4/20] Train Loss: 0.0417 Val Loss: 0.0417\n",
      "Epoch [5/20] Train Loss: 0.0416 Val Loss: 0.0418\n",
      "Epoch [6/20] Train Loss: 0.0416 Val Loss: 0.0417\n",
      "Epoch [7/20] Train Loss: 0.0415 Val Loss: 0.0416\n",
      "Epoch [8/20] Train Loss: 0.0415 Val Loss: 0.0417\n",
      "Epoch [9/20] Train Loss: 0.0415 Val Loss: 0.0417\n",
      "Epoch [10/20] Train Loss: 0.0415 Val Loss: 0.0416\n",
      "Epoch [11/20] Train Loss: 0.0415 Val Loss: 0.0416\n",
      "Epoch [12/20] Train Loss: 0.0414 Val Loss: 0.0415\n",
      "Epoch [13/20] Train Loss: 0.0414 Val Loss: 0.0415\n",
      "Epoch [14/20] Train Loss: 0.0414 Val Loss: 0.0415\n",
      "Epoch [15/20] Train Loss: 0.0414 Val Loss: 0.0415\n",
      "Epoch [16/20] Train Loss: 0.0414 Val Loss: 0.0414\n",
      "Epoch [17/20] Train Loss: 0.0414 Val Loss: 0.0415\n",
      "Epoch [18/20] Train Loss: 0.0413 Val Loss: 0.0415\n",
      "Epoch [19/20] Train Loss: 0.0414 Val Loss: 0.0414\n",
      "Epoch [20/20] Train Loss: 0.0413 Val Loss: 0.0414\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0442 Val Loss: 0.0443\n",
      "Epoch [2/20] Train Loss: 0.0441 Val Loss: 0.0442\n",
      "Epoch [3/20] Train Loss: 0.0440 Val Loss: 0.0441\n",
      "Epoch [4/20] Train Loss: 0.0439 Val Loss: 0.0441\n",
      "Epoch [5/20] Train Loss: 0.0439 Val Loss: 0.0440\n",
      "Epoch [6/20] Train Loss: 0.0438 Val Loss: 0.0439\n",
      "Epoch [7/20] Train Loss: 0.0438 Val Loss: 0.0439\n",
      "Epoch [8/20] Train Loss: 0.0438 Val Loss: 0.0438\n",
      "Epoch [9/20] Train Loss: 0.0437 Val Loss: 0.0438\n",
      "Epoch [10/20] Train Loss: 0.0437 Val Loss: 0.0439\n",
      "Epoch [11/20] Train Loss: 0.0437 Val Loss: 0.0438\n",
      "Epoch [12/20] Train Loss: 0.0437 Val Loss: 0.0439\n",
      "Epoch [13/20] Train Loss: 0.0437 Val Loss: 0.0440\n",
      "Epoch [14/20] Train Loss: 0.0437 Val Loss: 0.0440\n",
      "Epoch [15/20] Train Loss: 0.0436 Val Loss: 0.0437\n",
      "Epoch [16/20] Train Loss: 0.0436 Val Loss: 0.0438\n",
      "Epoch [17/20] Train Loss: 0.0436 Val Loss: 0.0438\n",
      "Epoch [18/20] Train Loss: 0.0436 Val Loss: 0.0438\n",
      "Epoch [19/20] Train Loss: 0.0436 Val Loss: 0.0437\n",
      "Epoch [20/20] Train Loss: 0.0436 Val Loss: 0.0439\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0472 Val Loss: 0.0470\n",
      "Epoch [2/20] Train Loss: 0.0472 Val Loss: 0.0469\n",
      "Epoch [3/20] Train Loss: 0.0472 Val Loss: 0.0469\n",
      "Epoch [4/20] Train Loss: 0.0471 Val Loss: 0.0469\n",
      "Epoch [5/20] Train Loss: 0.0471 Val Loss: 0.0469\n",
      "Epoch [6/20] Train Loss: 0.0471 Val Loss: 0.0469\n",
      "Epoch [7/20] Train Loss: 0.0471 Val Loss: 0.0468\n",
      "Epoch [8/20] Train Loss: 0.0471 Val Loss: 0.0468\n",
      "Epoch [9/20] Train Loss: 0.0471 Val Loss: 0.0468\n",
      "Epoch [10/20] Train Loss: 0.0470 Val Loss: 0.0469\n",
      "Epoch [11/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [12/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [13/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [14/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [15/20] Train Loss: 0.0470 Val Loss: 0.0467\n",
      "Epoch [16/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [17/20] Train Loss: 0.0470 Val Loss: 0.0469\n",
      "Epoch [18/20] Train Loss: 0.0470 Val Loss: 0.0467\n",
      "Epoch [19/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [20/20] Train Loss: 0.0470 Val Loss: 0.0467\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0422 Val Loss: 0.0426\n",
      "Epoch [2/20] Train Loss: 0.0422 Val Loss: 0.0425\n",
      "Epoch [3/20] Train Loss: 0.0422 Val Loss: 0.0425\n",
      "Epoch [4/20] Train Loss: 0.0421 Val Loss: 0.0424\n",
      "Epoch [5/20] Train Loss: 0.0421 Val Loss: 0.0424\n",
      "Epoch [6/20] Train Loss: 0.0421 Val Loss: 0.0424\n",
      "Epoch [7/20] Train Loss: 0.0420 Val Loss: 0.0424\n",
      "Epoch [8/20] Train Loss: 0.0420 Val Loss: 0.0423\n",
      "Epoch [9/20] Train Loss: 0.0420 Val Loss: 0.0426\n",
      "Epoch [10/20] Train Loss: 0.0420 Val Loss: 0.0423\n",
      "Epoch [11/20] Train Loss: 0.0420 Val Loss: 0.0423\n",
      "Epoch [12/20] Train Loss: 0.0420 Val Loss: 0.0422\n",
      "Epoch [13/20] Train Loss: 0.0419 Val Loss: 0.0422\n",
      "Epoch [14/20] Train Loss: 0.0420 Val Loss: 0.0422\n",
      "Epoch [15/20] Train Loss: 0.0419 Val Loss: 0.0423\n",
      "Epoch [16/20] Train Loss: 0.0419 Val Loss: 0.0424\n",
      "Epoch [17/20] Train Loss: 0.0419 Val Loss: 0.0422\n",
      "Epoch [18/20] Train Loss: 0.0419 Val Loss: 0.0423\n",
      "Epoch [19/20] Train Loss: 0.0419 Val Loss: 0.0422\n",
      "Epoch [20/20] Train Loss: 0.0419 Val Loss: 0.0423\n",
      "[0.04543353971221471, 0.052364160064352704, 0.054230563923053136, 0.048755916125740706, 0.05008785599485448, 0.050637765098618316, 0.05053426531041424]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_ewc_losses=[]\n",
    "local_fine_tune_ewc_preds=[]\n",
    "local_fine_tune_ewc_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_ewc_pred,local_fine_tune_ewc_loss,local_fine_tune_ewc_model=clients[i].local_fine_tune(server.index_set[i],ewc_flag=True,importance=0.02,fine_tune_epochs=20)\n",
    "    local_fine_tune_ewc_losses.append(local_fine_tune_ewc_loss)\n",
    "    local_fine_tune_ewc_preds.append(local_fine_tune_ewc_pred)\n",
    "    local_fine_tune_ewc_models.append(local_fine_tune_ewc_model)\n",
    "print(local_fine_tune_ewc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0385 Val Loss: 0.0397\n",
      "Epoch [2/20] Train Loss: 0.0383 Val Loss: 0.0396\n",
      "Epoch [3/20] Train Loss: 0.0382 Val Loss: 0.0396\n",
      "Epoch [4/20] Train Loss: 0.0380 Val Loss: 0.0395\n",
      "Epoch [5/20] Train Loss: 0.0380 Val Loss: 0.0396\n",
      "Epoch [6/20] Train Loss: 0.0379 Val Loss: 0.0396\n",
      "Epoch [7/20] Train Loss: 0.0379 Val Loss: 0.0395\n",
      "Epoch [8/20] Train Loss: 0.0378 Val Loss: 0.0397\n",
      "Epoch [9/20] Train Loss: 0.0377 Val Loss: 0.0394\n",
      "Epoch [10/20] Train Loss: 0.0377 Val Loss: 0.0398\n",
      "Epoch [11/20] Train Loss: 0.0376 Val Loss: 0.0394\n",
      "Epoch [12/20] Train Loss: 0.0376 Val Loss: 0.0394\n",
      "Epoch [13/20] Train Loss: 0.0375 Val Loss: 0.0393\n",
      "Epoch [14/20] Train Loss: 0.0375 Val Loss: 0.0393\n",
      "Epoch [15/20] Train Loss: 0.0375 Val Loss: 0.0395\n",
      "Epoch [16/20] Train Loss: 0.0374 Val Loss: 0.0394\n",
      "Epoch [17/20] Train Loss: 0.0374 Val Loss: 0.0392\n",
      "Epoch [18/20] Train Loss: 0.0374 Val Loss: 0.0393\n",
      "Epoch [19/20] Train Loss: 0.0373 Val Loss: 0.0392\n",
      "Epoch [20/20] Train Loss: 0.0373 Val Loss: 0.0392\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0468 Val Loss: 0.0469\n",
      "Epoch [2/20] Train Loss: 0.0468 Val Loss: 0.0468\n",
      "Epoch [3/20] Train Loss: 0.0468 Val Loss: 0.0468\n",
      "Epoch [4/20] Train Loss: 0.0467 Val Loss: 0.0470\n",
      "Epoch [5/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [6/20] Train Loss: 0.0467 Val Loss: 0.0467\n",
      "Epoch [7/20] Train Loss: 0.0466 Val Loss: 0.0467\n",
      "Epoch [8/20] Train Loss: 0.0466 Val Loss: 0.0467\n",
      "Epoch [9/20] Train Loss: 0.0466 Val Loss: 0.0467\n",
      "Epoch [10/20] Train Loss: 0.0465 Val Loss: 0.0468\n",
      "Epoch [11/20] Train Loss: 0.0465 Val Loss: 0.0467\n",
      "Epoch [12/20] Train Loss: 0.0465 Val Loss: 0.0466\n",
      "Epoch [13/20] Train Loss: 0.0465 Val Loss: 0.0466\n",
      "Epoch [14/20] Train Loss: 0.0465 Val Loss: 0.0467\n",
      "Epoch [15/20] Train Loss: 0.0465 Val Loss: 0.0466\n",
      "Epoch [16/20] Train Loss: 0.0464 Val Loss: 0.0465\n",
      "Epoch [17/20] Train Loss: 0.0464 Val Loss: 0.0467\n",
      "Epoch [18/20] Train Loss: 0.0464 Val Loss: 0.0466\n",
      "Epoch [19/20] Train Loss: 0.0464 Val Loss: 0.0466\n",
      "Epoch [20/20] Train Loss: 0.0464 Val Loss: 0.0466\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0480 Val Loss: 0.0486\n",
      "Epoch [2/20] Train Loss: 0.0476 Val Loss: 0.0487\n",
      "Epoch [3/20] Train Loss: 0.0474 Val Loss: 0.0485\n",
      "Epoch [4/20] Train Loss: 0.0472 Val Loss: 0.0483\n",
      "Epoch [5/20] Train Loss: 0.0471 Val Loss: 0.0483\n",
      "Epoch [6/20] Train Loss: 0.0470 Val Loss: 0.0484\n",
      "Epoch [7/20] Train Loss: 0.0469 Val Loss: 0.0482\n",
      "Epoch [8/20] Train Loss: 0.0468 Val Loss: 0.0484\n",
      "Epoch [9/20] Train Loss: 0.0467 Val Loss: 0.0481\n",
      "Epoch [10/20] Train Loss: 0.0466 Val Loss: 0.0481\n",
      "Epoch [11/20] Train Loss: 0.0466 Val Loss: 0.0481\n",
      "Epoch [12/20] Train Loss: 0.0465 Val Loss: 0.0481\n",
      "Epoch [13/20] Train Loss: 0.0464 Val Loss: 0.0480\n",
      "Epoch [14/20] Train Loss: 0.0464 Val Loss: 0.0480\n",
      "Epoch [15/20] Train Loss: 0.0463 Val Loss: 0.0480\n",
      "Epoch [16/20] Train Loss: 0.0463 Val Loss: 0.0483\n",
      "Epoch [17/20] Train Loss: 0.0462 Val Loss: 0.0480\n",
      "Epoch [18/20] Train Loss: 0.0462 Val Loss: 0.0479\n",
      "Epoch [19/20] Train Loss: 0.0461 Val Loss: 0.0478\n",
      "Epoch [20/20] Train Loss: 0.0460 Val Loss: 0.0478\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0423 Val Loss: 0.0419\n",
      "Epoch [2/20] Train Loss: 0.0412 Val Loss: 0.0416\n",
      "Epoch [3/20] Train Loss: 0.0409 Val Loss: 0.0414\n",
      "Epoch [4/20] Train Loss: 0.0407 Val Loss: 0.0415\n",
      "Epoch [5/20] Train Loss: 0.0407 Val Loss: 0.0414\n",
      "Epoch [6/20] Train Loss: 0.0406 Val Loss: 0.0412\n",
      "Epoch [7/20] Train Loss: 0.0405 Val Loss: 0.0412\n",
      "Epoch [8/20] Train Loss: 0.0404 Val Loss: 0.0412\n",
      "Epoch [9/20] Train Loss: 0.0404 Val Loss: 0.0412\n",
      "Epoch [10/20] Train Loss: 0.0403 Val Loss: 0.0413\n",
      "Epoch [11/20] Train Loss: 0.0403 Val Loss: 0.0411\n",
      "Epoch [12/20] Train Loss: 0.0402 Val Loss: 0.0411\n",
      "Epoch [13/20] Train Loss: 0.0401 Val Loss: 0.0410\n",
      "Epoch [14/20] Train Loss: 0.0401 Val Loss: 0.0412\n",
      "Epoch [15/20] Train Loss: 0.0401 Val Loss: 0.0410\n",
      "Epoch [16/20] Train Loss: 0.0400 Val Loss: 0.0410\n",
      "Epoch [17/20] Train Loss: 0.0400 Val Loss: 0.0410\n",
      "Epoch [18/20] Train Loss: 0.0399 Val Loss: 0.0411\n",
      "Epoch [19/20] Train Loss: 0.0399 Val Loss: 0.0410\n",
      "Epoch [20/20] Train Loss: 0.0399 Val Loss: 0.0410\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0441 Val Loss: 0.0442\n",
      "Epoch [2/20] Train Loss: 0.0439 Val Loss: 0.0441\n",
      "Epoch [3/20] Train Loss: 0.0437 Val Loss: 0.0440\n",
      "Epoch [4/20] Train Loss: 0.0435 Val Loss: 0.0438\n",
      "Epoch [5/20] Train Loss: 0.0434 Val Loss: 0.0437\n",
      "Epoch [6/20] Train Loss: 0.0433 Val Loss: 0.0436\n",
      "Epoch [7/20] Train Loss: 0.0433 Val Loss: 0.0436\n",
      "Epoch [8/20] Train Loss: 0.0432 Val Loss: 0.0435\n",
      "Epoch [9/20] Train Loss: 0.0431 Val Loss: 0.0435\n",
      "Epoch [10/20] Train Loss: 0.0430 Val Loss: 0.0434\n",
      "Epoch [11/20] Train Loss: 0.0429 Val Loss: 0.0436\n",
      "Epoch [12/20] Train Loss: 0.0429 Val Loss: 0.0434\n",
      "Epoch [13/20] Train Loss: 0.0428 Val Loss: 0.0433\n",
      "Epoch [14/20] Train Loss: 0.0428 Val Loss: 0.0433\n",
      "Epoch [15/20] Train Loss: 0.0427 Val Loss: 0.0433\n",
      "Epoch [16/20] Train Loss: 0.0426 Val Loss: 0.0432\n",
      "Epoch [17/20] Train Loss: 0.0426 Val Loss: 0.0432\n",
      "Epoch [18/20] Train Loss: 0.0425 Val Loss: 0.0432\n",
      "Epoch [19/20] Train Loss: 0.0425 Val Loss: 0.0431\n",
      "Epoch [20/20] Train Loss: 0.0424 Val Loss: 0.0431\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0472 Val Loss: 0.0470\n",
      "Epoch [2/20] Train Loss: 0.0472 Val Loss: 0.0469\n",
      "Epoch [3/20] Train Loss: 0.0472 Val Loss: 0.0469\n",
      "Epoch [4/20] Train Loss: 0.0471 Val Loss: 0.0469\n",
      "Epoch [5/20] Train Loss: 0.0471 Val Loss: 0.0468\n",
      "Epoch [6/20] Train Loss: 0.0470 Val Loss: 0.0469\n",
      "Epoch [7/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [8/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [9/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [10/20] Train Loss: 0.0470 Val Loss: 0.0468\n",
      "Epoch [11/20] Train Loss: 0.0469 Val Loss: 0.0467\n",
      "Epoch [12/20] Train Loss: 0.0469 Val Loss: 0.0468\n",
      "Epoch [13/20] Train Loss: 0.0469 Val Loss: 0.0467\n",
      "Epoch [14/20] Train Loss: 0.0469 Val Loss: 0.0467\n",
      "Epoch [15/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "Epoch [16/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "Epoch [17/20] Train Loss: 0.0469 Val Loss: 0.0467\n",
      "Epoch [18/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "Epoch [19/20] Train Loss: 0.0468 Val Loss: 0.0469\n",
      "Epoch [20/20] Train Loss: 0.0468 Val Loss: 0.0467\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0422 Val Loss: 0.0426\n",
      "Epoch [2/20] Train Loss: 0.0422 Val Loss: 0.0425\n",
      "Epoch [3/20] Train Loss: 0.0420 Val Loss: 0.0425\n",
      "Epoch [4/20] Train Loss: 0.0420 Val Loss: 0.0424\n",
      "Epoch [5/20] Train Loss: 0.0419 Val Loss: 0.0424\n",
      "Epoch [6/20] Train Loss: 0.0418 Val Loss: 0.0423\n",
      "Epoch [7/20] Train Loss: 0.0417 Val Loss: 0.0422\n",
      "Epoch [8/20] Train Loss: 0.0417 Val Loss: 0.0422\n",
      "Epoch [9/20] Train Loss: 0.0416 Val Loss: 0.0421\n",
      "Epoch [10/20] Train Loss: 0.0415 Val Loss: 0.0422\n",
      "Epoch [11/20] Train Loss: 0.0415 Val Loss: 0.0421\n",
      "Epoch [12/20] Train Loss: 0.0415 Val Loss: 0.0420\n",
      "Epoch [13/20] Train Loss: 0.0414 Val Loss: 0.0420\n",
      "Epoch [14/20] Train Loss: 0.0413 Val Loss: 0.0420\n",
      "Epoch [15/20] Train Loss: 0.0413 Val Loss: 0.0419\n",
      "Epoch [16/20] Train Loss: 0.0412 Val Loss: 0.0419\n",
      "Epoch [17/20] Train Loss: 0.0412 Val Loss: 0.0419\n",
      "Epoch [18/20] Train Loss: 0.0411 Val Loss: 0.0418\n",
      "Epoch [19/20] Train Loss: 0.0411 Val Loss: 0.0418\n",
      "Epoch [20/20] Train Loss: 0.0411 Val Loss: 0.0418\n",
      "[0.04542406636277494, 0.05251354423083671, 0.054330342985077267, 0.0489938806723293, 0.04995013929724897, 0.05067031757829532, 0.05072127472392398]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_noewc_losses=[]\n",
    "local_fine_tune_noewc_preds=[]\n",
    "local_fine_tune_noewc_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_noewc_pred,local_fine_tune_noewc_loss,local_fine_tune_noewc_model=clients[i].local_fine_tune(server.index_set[i],ewc_flag=False,importance=0.1,fine_tune_epochs=20)\n",
    "    local_fine_tune_noewc_losses.append(local_fine_tune_noewc_loss)\n",
    "    local_fine_tune_noewc_preds.append(local_fine_tune_noewc_pred)\n",
    "    local_fine_tune_noewc_models.append(local_fine_tune_noewc_model)\n",
    "print(local_fine_tune_noewc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the server object\n",
    "with open('../result/'+str(scale)+'/server_benchmark.pkl', 'rb') as f:\n",
    "    server_benchmark = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients_benchmark.pkl', 'rb') as f:\n",
    "    clients_benchmark = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04738055886573171, 0.05263746971238966, 0.05798239472692143, 0.05267014441220728, 0.05327578918523576, 0.05104644261683299, 0.05414957301818753]\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(7):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients_benchmark[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04773888712085477, 0.055938527995899115, 0.05991943744457748, 0.053290099919811915, 0.055807694607758765, 0.05220189785635839, 0.056752502771528206]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(7):\n",
    "    central_pred,central_loss,central_model=server_benchmark.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047523875287032294, 0.053075658706055115, 0.0580338093337335, 0.05174410740576991, 0.05369003568712163, 0.05185019247846244, 0.05432962053391623]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(7):\n",
    "    local_pred,local_loss,local_model=clients_benchmark[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0451 Val Loss: 0.0443\n",
      "Epoch [2/20] Train Loss: 0.0452 Val Loss: 0.0443\n",
      "Epoch [3/20] Train Loss: 0.0451 Val Loss: 0.0442\n",
      "Epoch [4/20] Train Loss: 0.0450 Val Loss: 0.0443\n",
      "Epoch [5/20] Train Loss: 0.0450 Val Loss: 0.0443\n",
      "Epoch [6/20] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [7/20] Train Loss: 0.0450 Val Loss: 0.0441\n",
      "Epoch [8/20] Train Loss: 0.0450 Val Loss: 0.0443\n",
      "Epoch [9/20] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [10/20] Train Loss: 0.0449 Val Loss: 0.0442\n",
      "Epoch [11/20] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [12/20] Train Loss: 0.0449 Val Loss: 0.0440\n",
      "Epoch [13/20] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [14/20] Train Loss: 0.0449 Val Loss: 0.0440\n",
      "Epoch [15/20] Train Loss: 0.0449 Val Loss: 0.0440\n",
      "Epoch [16/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [17/20] Train Loss: 0.0449 Val Loss: 0.0440\n",
      "Epoch [18/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [19/20] Train Loss: 0.0448 Val Loss: 0.0440\n",
      "Epoch [20/20] Train Loss: 0.0448 Val Loss: 0.0440\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0486 Val Loss: 0.0482\n",
      "Epoch [2/20] Train Loss: 0.0486 Val Loss: 0.0480\n",
      "Epoch [3/20] Train Loss: 0.0485 Val Loss: 0.0480\n",
      "Epoch [4/20] Train Loss: 0.0484 Val Loss: 0.0478\n",
      "Epoch [5/20] Train Loss: 0.0484 Val Loss: 0.0478\n",
      "Epoch [6/20] Train Loss: 0.0483 Val Loss: 0.0478\n",
      "Epoch [7/20] Train Loss: 0.0483 Val Loss: 0.0478\n",
      "Epoch [8/20] Train Loss: 0.0483 Val Loss: 0.0477\n",
      "Epoch [9/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [10/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [11/20] Train Loss: 0.0482 Val Loss: 0.0476\n",
      "Epoch [12/20] Train Loss: 0.0482 Val Loss: 0.0478\n",
      "Epoch [13/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [14/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [15/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [16/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [17/20] Train Loss: 0.0481 Val Loss: 0.0475\n",
      "Epoch [18/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [19/20] Train Loss: 0.0481 Val Loss: 0.0479\n",
      "Epoch [20/20] Train Loss: 0.0481 Val Loss: 0.0475\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0559 Val Loss: 0.0564\n",
      "Epoch [2/20] Train Loss: 0.0560 Val Loss: 0.0563\n",
      "Epoch [3/20] Train Loss: 0.0559 Val Loss: 0.0562\n",
      "Epoch [4/20] Train Loss: 0.0559 Val Loss: 0.0563\n",
      "Epoch [5/20] Train Loss: 0.0559 Val Loss: 0.0563\n",
      "Epoch [6/20] Train Loss: 0.0558 Val Loss: 0.0562\n",
      "Epoch [7/20] Train Loss: 0.0559 Val Loss: 0.0564\n",
      "Epoch [8/20] Train Loss: 0.0558 Val Loss: 0.0561\n",
      "Epoch [9/20] Train Loss: 0.0558 Val Loss: 0.0561\n",
      "Epoch [10/20] Train Loss: 0.0558 Val Loss: 0.0561\n",
      "Epoch [11/20] Train Loss: 0.0558 Val Loss: 0.0561\n",
      "Epoch [12/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [13/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [14/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [15/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [16/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [17/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [18/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [19/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [20/20] Train Loss: 0.0557 Val Loss: 0.0560\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0497 Val Loss: 0.0490\n",
      "Epoch [2/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [3/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [4/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [5/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [6/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [7/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [8/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [9/20] Train Loss: 0.0497 Val Loss: 0.0490\n",
      "Epoch [10/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [11/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [12/20] Train Loss: 0.0497 Val Loss: 0.0488\n",
      "Epoch [13/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [14/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [15/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [16/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [17/20] Train Loss: 0.0496 Val Loss: 0.0490\n",
      "Epoch [18/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [19/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [20/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0532 Val Loss: 0.0523\n",
      "Epoch [2/20] Train Loss: 0.0533 Val Loss: 0.0521\n",
      "Epoch [3/20] Train Loss: 0.0532 Val Loss: 0.0520\n",
      "Epoch [4/20] Train Loss: 0.0532 Val Loss: 0.0520\n",
      "Epoch [5/20] Train Loss: 0.0532 Val Loss: 0.0519\n",
      "Epoch [6/20] Train Loss: 0.0531 Val Loss: 0.0520\n",
      "Epoch [7/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [8/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [9/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [10/20] Train Loss: 0.0531 Val Loss: 0.0518\n",
      "Epoch [11/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [12/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [13/20] Train Loss: 0.0530 Val Loss: 0.0517\n",
      "Epoch [14/20] Train Loss: 0.0530 Val Loss: 0.0517\n",
      "Epoch [15/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [16/20] Train Loss: 0.0529 Val Loss: 0.0519\n",
      "Epoch [17/20] Train Loss: 0.0529 Val Loss: 0.0521\n",
      "Epoch [18/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [19/20] Train Loss: 0.0529 Val Loss: 0.0516\n",
      "Epoch [20/20] Train Loss: 0.0529 Val Loss: 0.0519\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0477 Val Loss: 0.0471\n",
      "Epoch [2/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [3/20] Train Loss: 0.0478 Val Loss: 0.0470\n",
      "Epoch [4/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [5/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [6/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [7/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [8/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [9/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [10/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [11/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [12/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [13/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [14/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [15/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [16/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [17/20] Train Loss: 0.0476 Val Loss: 0.0472\n",
      "Epoch [18/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "Epoch [19/20] Train Loss: 0.0476 Val Loss: 0.0471\n",
      "Epoch [20/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0524 Val Loss: 0.0518\n",
      "Epoch [2/20] Train Loss: 0.0525 Val Loss: 0.0519\n",
      "Epoch [3/20] Train Loss: 0.0525 Val Loss: 0.0518\n",
      "Epoch [4/20] Train Loss: 0.0525 Val Loss: 0.0517\n",
      "Epoch [5/20] Train Loss: 0.0524 Val Loss: 0.0517\n",
      "Epoch [6/20] Train Loss: 0.0524 Val Loss: 0.0517\n",
      "Epoch [7/20] Train Loss: 0.0524 Val Loss: 0.0517\n",
      "Epoch [8/20] Train Loss: 0.0524 Val Loss: 0.0516\n",
      "Epoch [9/20] Train Loss: 0.0524 Val Loss: 0.0520\n",
      "Epoch [10/20] Train Loss: 0.0524 Val Loss: 0.0516\n",
      "Epoch [11/20] Train Loss: 0.0523 Val Loss: 0.0517\n",
      "Epoch [12/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [13/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [14/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [15/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [16/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [17/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [18/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [19/20] Train Loss: 0.0522 Val Loss: 0.0515\n",
      "Epoch [20/20] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "[0.0467919398749834, 0.052113543124231575, 0.05765840289986705, 0.05167011064413476, 0.053093973464294245, 0.050561777596706395, 0.053845099588116146]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_ewc_benchmark_losses=[]\n",
    "local_fine_tune_ewc_benchmark_preds=[]\n",
    "local_fine_tune_ewc_benchmark_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_ewc_benchmark_pred,local_fine_tune_ewc_benchmark_loss,local_fine_tune_ewc_benchmark_model=clients_benchmark[i].local_fine_tune(ewc_flag=True,importance=0.02,fine_tune_epochs=20)\n",
    "    local_fine_tune_ewc_benchmark_losses.append(local_fine_tune_ewc_benchmark_loss)\n",
    "    local_fine_tune_ewc_benchmark_preds.append(local_fine_tune_ewc_benchmark_pred)\n",
    "    local_fine_tune_ewc_benchmark_models.append(local_fine_tune_ewc_benchmark_model)\n",
    "print(local_fine_tune_ewc_benchmark_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0451 Val Loss: 0.0445\n",
      "Epoch [2/20] Train Loss: 0.0451 Val Loss: 0.0442\n",
      "Epoch [3/20] Train Loss: 0.0451 Val Loss: 0.0442\n",
      "Epoch [4/20] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [5/20] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [6/20] Train Loss: 0.0449 Val Loss: 0.0442\n",
      "Epoch [7/20] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [8/20] Train Loss: 0.0449 Val Loss: 0.0442\n",
      "Epoch [9/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [10/20] Train Loss: 0.0448 Val Loss: 0.0445\n",
      "Epoch [11/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [12/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [13/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [14/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [15/20] Train Loss: 0.0447 Val Loss: 0.0442\n",
      "Epoch [16/20] Train Loss: 0.0446 Val Loss: 0.0440\n",
      "Epoch [17/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [18/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [19/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [20/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0485 Val Loss: 0.0481\n",
      "Epoch [2/20] Train Loss: 0.0486 Val Loss: 0.0480\n",
      "Epoch [3/20] Train Loss: 0.0485 Val Loss: 0.0479\n",
      "Epoch [4/20] Train Loss: 0.0484 Val Loss: 0.0482\n",
      "Epoch [5/20] Train Loss: 0.0483 Val Loss: 0.0478\n",
      "Epoch [6/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [7/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [8/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [9/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [10/20] Train Loss: 0.0481 Val Loss: 0.0478\n",
      "Epoch [11/20] Train Loss: 0.0480 Val Loss: 0.0477\n",
      "Epoch [12/20] Train Loss: 0.0480 Val Loss: 0.0476\n",
      "Epoch [13/20] Train Loss: 0.0480 Val Loss: 0.0475\n",
      "Epoch [14/20] Train Loss: 0.0479 Val Loss: 0.0477\n",
      "Epoch [15/20] Train Loss: 0.0479 Val Loss: 0.0476\n",
      "Epoch [16/20] Train Loss: 0.0479 Val Loss: 0.0475\n",
      "Epoch [17/20] Train Loss: 0.0478 Val Loss: 0.0476\n",
      "Epoch [18/20] Train Loss: 0.0478 Val Loss: 0.0474\n",
      "Epoch [19/20] Train Loss: 0.0478 Val Loss: 0.0476\n",
      "Epoch [20/20] Train Loss: 0.0478 Val Loss: 0.0475\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0558 Val Loss: 0.0563\n",
      "Epoch [2/20] Train Loss: 0.0560 Val Loss: 0.0563\n",
      "Epoch [3/20] Train Loss: 0.0559 Val Loss: 0.0562\n",
      "Epoch [4/20] Train Loss: 0.0559 Val Loss: 0.0562\n",
      "Epoch [5/20] Train Loss: 0.0558 Val Loss: 0.0562\n",
      "Epoch [6/20] Train Loss: 0.0558 Val Loss: 0.0563\n",
      "Epoch [7/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [8/20] Train Loss: 0.0558 Val Loss: 0.0564\n",
      "Epoch [9/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [10/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [11/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [12/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [13/20] Train Loss: 0.0556 Val Loss: 0.0560\n",
      "Epoch [14/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [15/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [16/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [17/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [18/20] Train Loss: 0.0554 Val Loss: 0.0560\n",
      "Epoch [19/20] Train Loss: 0.0554 Val Loss: 0.0561\n",
      "Epoch [20/20] Train Loss: 0.0554 Val Loss: 0.0560\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [2/20] Train Loss: 0.0499 Val Loss: 0.0490\n",
      "Epoch [3/20] Train Loss: 0.0498 Val Loss: 0.0489\n",
      "Epoch [4/20] Train Loss: 0.0497 Val Loss: 0.0490\n",
      "Epoch [5/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [6/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [7/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [8/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [9/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [10/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [11/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [12/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [13/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [14/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [15/20] Train Loss: 0.0495 Val Loss: 0.0489\n",
      "Epoch [16/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [17/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [18/20] Train Loss: 0.0494 Val Loss: 0.0490\n",
      "Epoch [19/20] Train Loss: 0.0494 Val Loss: 0.0488\n",
      "Epoch [20/20] Train Loss: 0.0494 Val Loss: 0.0488\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0532 Val Loss: 0.0521\n",
      "Epoch [2/20] Train Loss: 0.0533 Val Loss: 0.0520\n",
      "Epoch [3/20] Train Loss: 0.0532 Val Loss: 0.0520\n",
      "Epoch [4/20] Train Loss: 0.0532 Val Loss: 0.0519\n",
      "Epoch [5/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [6/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [7/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [8/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [9/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [10/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [11/20] Train Loss: 0.0529 Val Loss: 0.0518\n",
      "Epoch [12/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [13/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [14/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [15/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [16/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [17/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [18/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [19/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [20/20] Train Loss: 0.0527 Val Loss: 0.0515\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0477 Val Loss: 0.0471\n",
      "Epoch [2/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [3/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [4/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [5/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [6/20] Train Loss: 0.0477 Val Loss: 0.0471\n",
      "Epoch [7/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [8/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [9/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [10/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "Epoch [11/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "Epoch [12/20] Train Loss: 0.0475 Val Loss: 0.0470\n",
      "Epoch [13/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [14/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [15/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [16/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [17/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [18/20] Train Loss: 0.0474 Val Loss: 0.0469\n",
      "Epoch [19/20] Train Loss: 0.0474 Val Loss: 0.0472\n",
      "Epoch [20/20] Train Loss: 0.0474 Val Loss: 0.0469\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0524 Val Loss: 0.0520\n",
      "Epoch [2/20] Train Loss: 0.0525 Val Loss: 0.0518\n",
      "Epoch [3/20] Train Loss: 0.0525 Val Loss: 0.0520\n",
      "Epoch [4/20] Train Loss: 0.0525 Val Loss: 0.0517\n",
      "Epoch [5/20] Train Loss: 0.0525 Val Loss: 0.0519\n",
      "Epoch [6/20] Train Loss: 0.0523 Val Loss: 0.0519\n",
      "Epoch [7/20] Train Loss: 0.0524 Val Loss: 0.0516\n",
      "Epoch [8/20] Train Loss: 0.0523 Val Loss: 0.0517\n",
      "Epoch [9/20] Train Loss: 0.0523 Val Loss: 0.0518\n",
      "Epoch [10/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [11/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [12/20] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "Epoch [13/20] Train Loss: 0.0522 Val Loss: 0.0515\n",
      "Epoch [14/20] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "Epoch [15/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [16/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [17/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [18/20] Train Loss: 0.0521 Val Loss: 0.0516\n",
      "Epoch [19/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [20/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "[0.04672484316748299, 0.05219567769959773, 0.05765293501572658, 0.05168611156971079, 0.05303531432562597, 0.05059497972491057, 0.05391620361080317]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_benchmark_losses=[]\n",
    "local_fine_tune_benchmark_preds=[]\n",
    "local_fine_tune_benchmark_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_benchmark_pred,local_fine_tune_benchmark_loss,local_fine_tune_benchmark_model=clients_benchmark[i].local_fine_tune()\n",
    "    local_fine_tune_benchmark_losses.append(local_fine_tune_benchmark_loss)\n",
    "    local_fine_tune_benchmark_preds.append(local_fine_tune_benchmark_pred)\n",
    "    local_fine_tune_benchmark_models.append(local_fine_tune_benchmark_model)\n",
    "print(local_fine_tune_benchmark_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047523875287032294, 0.053075658706055115, 0.0580338093337335, 0.05174410740576991, 0.05369003568712163, 0.05185019247846244, 0.05432962053391623]\n",
      "[0.04773888712085477, 0.055938527995899115, 0.05991943744457748, 0.053290099919811915, 0.055807694607758765, 0.05220189785635839, 0.056752502771528206]\n",
      "[0.04738055886573171, 0.05263746971238966, 0.05798239472692143, 0.05267014441220728, 0.05327578918523576, 0.05104644261683299, 0.05414957301818753]\n",
      "[0.04672484316748299, 0.05219567769959773, 0.05765293501572658, 0.05168611156971079, 0.05303531432562597, 0.05059497972491057, 0.05391620361080317]\n",
      "[0.0467919398749834, 0.052113543124231575, 0.05765840289986705, 0.05167011064413476, 0.053093973464294245, 0.050561777596706395, 0.053845099588116146]\n",
      "[0.04661360359431742, 0.05353561454541879, 0.0576981087279034, 0.05322195598472879, 0.052451892526284474, 0.05084744619511782, 0.05232491294776842]\n",
      "[0.04542406636277494, 0.05251354423083671, 0.054330342985077267, 0.0489938806723293, 0.04995013929724897, 0.05067031757829532, 0.05072127472392398]\n",
      "[0.04543353971221471, 0.052364160064352704, 0.054230563923053136, 0.048755916125740706, 0.05008785599485448, 0.050637765098618316, 0.05053426531041424]\n",
      "0.052247835313190515\n",
      "0.05225800930197969\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_benchmark_losses)\n",
    "print(local_fine_tune_ewc_benchmark_losses)\n",
    "print(fed_local_proposed_losses)\n",
    "print(local_fine_tune_noewc_losses)\n",
    "print(local_fine_tune_ewc_losses)\n",
    "\n",
    "print(np.mean(local_fine_tune_ewc_benchmark_losses))\n",
    "print(np.mean(local_fine_tune_benchmark_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=scale)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "parser_train.add_argument('--k', type=int, default=2)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "\n",
    "parser_train.add_argument('--warm_up_epochs', type=int, default=15)\n",
    "parser_train.add_argument('--selection_epochs', type=int, default=4)\n",
    "parser_train.add_argument('--importance', type=float, default=0)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model2/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "parser_train.add_argument('--decay', type=float, default=0.75)\n",
    "args_train = parser_train.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "class RMSELoss:\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        self.reduction = reduction\n",
    "        self.mse_loss = MSELoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        output=output.cpu()[:,4]#.reshape(-1,1)\n",
    "        mse = self.mse_loss(output, target)\n",
    "        rmse = torch.sqrt(mse)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            rmse = rmse.sum()\n",
    "        if self.reduction == 'mean':\n",
    "            rmse = rmse.mean()\n",
    "\n",
    "        return rmse\n",
    "    \n",
    "\n",
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "class MAELoss:\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        self.reduction = reduction\n",
    "        self.mae_loss = L1Loss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        output = output.cpu()[:, 4]\n",
    "        mae = self.mae_loss(output, target)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            mae = mae.sum()\n",
    "        if self.reduction == 'mean':\n",
    "            mae = mae.mean()\n",
    "\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y_lst=[]\n",
    "\n",
    "for i in range(7):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths='wf'+str(i+1)\n",
    "    test_data, test_loader = get_data(args_temp,flag='test')\n",
    "    actual_y=[]\n",
    "    for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "        actual_y.append(seq_y)\n",
    "    actual_y = torch.cat([torch.flatten(t) for t in actual_y])\n",
    "    actual_y_lst.append(actual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rmse = []\n",
    "central_rmse = []\n",
    "fed_local_rmse = []\n",
    "local_fine_tune_rmse = []\n",
    "fed_local_proposed_rmse = []\n",
    "local_fine_tune_noewc_rmse = []\n",
    "local_fine_tune_ewc_rmse = []\n",
    "local_fine_tune_benchmark_ewc_rmse = []\n",
    "rmse_loss = RMSELoss('mean')\n",
    "\n",
    "for i in range(7):\n",
    "    fed_local_rmse.append(rmse_loss(fed_local_preds[i], actual_y_lst[i]))\n",
    "    local_rmse.append(rmse_loss(local_preds[i], actual_y_lst[i]))\n",
    "    central_rmse.append(rmse_loss(central_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_rmse.append(rmse_loss(local_fine_tune_benchmark_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_benchmark_ewc_rmse.append(rmse_loss(local_fine_tune_ewc_benchmark_preds[i], actual_y_lst[i]))\n",
    "    fed_local_proposed_rmse.append(rmse_loss(fed_local_proposed_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_noewc_rmse.append(rmse_loss(local_fine_tune_noewc_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_ewc_rmse.append(rmse_loss(local_fine_tune_ewc_preds[i], actual_y_lst[i]))\n",
    "\n",
    "df_RMSE = pd.DataFrame({\n",
    "    'local_rmse': np.array(local_rmse),\n",
    "    'central_rmse': np.array(central_rmse),\n",
    "    'fed_local_rmse': np.array(fed_local_rmse),\n",
    "    'local_fine_tune_rmse': np.array(local_fine_tune_rmse),\n",
    "    'local_fine_tune_benchmark_ewc_rmse': np.array(local_fine_tune_benchmark_ewc_rmse),\n",
    "    'fed_local_proposed_rmse': np.array(fed_local_proposed_rmse),\n",
    "    'local_fine_tune_noewc_rmse': np.array(local_fine_tune_noewc_rmse),\n",
    "    'local_fine_tune_ewc_rmse': np.array(local_fine_tune_ewc_rmse),\n",
    "}).T\n",
    "df_RMSE['mean_rmse'] = df_RMSE.mean(axis=1)\n",
    "df_RMSE.to_csv('rmse.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mae = []\n",
    "central_mae = []\n",
    "fed_local_mae = []\n",
    "local_fine_tune_mae = []\n",
    "fed_local_proposed_mae = []\n",
    "local_fine_tune_noewc_mae = []\n",
    "local_fine_tune_ewc_mae = []\n",
    "local_fine_tune_benchmark_ewc_mae = []\n",
    "mae_loss = MAELoss('mean')\n",
    "\n",
    "for i in range(7):\n",
    "    fed_local_mae.append(mae_loss(fed_local_preds[i], actual_y_lst[i]))\n",
    "    local_mae.append(mae_loss(local_preds[i], actual_y_lst[i]))\n",
    "    central_mae.append(mae_loss(central_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_mae.append(mae_loss(local_fine_tune_benchmark_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_benchmark_ewc_mae.append(mae_loss(local_fine_tune_ewc_benchmark_preds[i], actual_y_lst[i]))\n",
    "    fed_local_proposed_mae.append(mae_loss(fed_local_proposed_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_noewc_mae.append(mae_loss(local_fine_tune_noewc_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_ewc_mae.append(mae_loss(local_fine_tune_ewc_preds[i], actual_y_lst[i]))\n",
    "\n",
    "df_MAE = pd.DataFrame({\n",
    "    'local_mae': np.array(local_mae),\n",
    "    'central_mae': np.array(central_mae),\n",
    "    'fed_local_mae': np.array(fed_local_mae),\n",
    "    'local_fine_tune_mae': np.array(local_fine_tune_mae),\n",
    "    'local_fine_tune_benchmark_ewc_mae': np.array(local_fine_tune_benchmark_ewc_mae),\n",
    "    'fed_local_proposed_mae': np.array(fed_local_proposed_mae),\n",
    "    'local_fine_tune_noewc_mae': np.array(local_fine_tune_noewc_mae),\n",
    "    'local_fine_tune_ewc_mae': np.array(local_fine_tune_ewc_mae),\n",
    "}).T\n",
    "df_MAE['mean_mae'] = df_MAE.mean(axis=1)\n",
    "df_MAE.to_csv('mae.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>mean_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>local_mae</th>\n",
       "      <td>0.122145</td>\n",
       "      <td>0.136123</td>\n",
       "      <td>0.149602</td>\n",
       "      <td>0.133117</td>\n",
       "      <td>0.136393</td>\n",
       "      <td>0.132337</td>\n",
       "      <td>0.139272</td>\n",
       "      <td>0.135570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central_mae</th>\n",
       "      <td>0.122274</td>\n",
       "      <td>0.143926</td>\n",
       "      <td>0.151888</td>\n",
       "      <td>0.136231</td>\n",
       "      <td>0.142540</td>\n",
       "      <td>0.133549</td>\n",
       "      <td>0.145579</td>\n",
       "      <td>0.139427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fed_local_mae</th>\n",
       "      <td>0.121948</td>\n",
       "      <td>0.136047</td>\n",
       "      <td>0.147786</td>\n",
       "      <td>0.136046</td>\n",
       "      <td>0.135507</td>\n",
       "      <td>0.131167</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.135354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_mae</th>\n",
       "      <td>0.119969</td>\n",
       "      <td>0.134683</td>\n",
       "      <td>0.147717</td>\n",
       "      <td>0.132811</td>\n",
       "      <td>0.135038</td>\n",
       "      <td>0.128996</td>\n",
       "      <td>0.138121</td>\n",
       "      <td>0.133905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_benchmark_ewc_mae</th>\n",
       "      <td>0.120102</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.147695</td>\n",
       "      <td>0.132730</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.128970</td>\n",
       "      <td>0.138339</td>\n",
       "      <td>0.133925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fed_local_proposed_mae</th>\n",
       "      <td>0.120353</td>\n",
       "      <td>0.136813</td>\n",
       "      <td>0.148757</td>\n",
       "      <td>0.136157</td>\n",
       "      <td>0.134694</td>\n",
       "      <td>0.129513</td>\n",
       "      <td>0.134197</td>\n",
       "      <td>0.134355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_noewc_mae</th>\n",
       "      <td>0.117046</td>\n",
       "      <td>0.135011</td>\n",
       "      <td>0.139706</td>\n",
       "      <td>0.125178</td>\n",
       "      <td>0.128088</td>\n",
       "      <td>0.128890</td>\n",
       "      <td>0.130428</td>\n",
       "      <td>0.129192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_ewc_mae</th>\n",
       "      <td>0.117143</td>\n",
       "      <td>0.134488</td>\n",
       "      <td>0.139598</td>\n",
       "      <td>0.124503</td>\n",
       "      <td>0.128275</td>\n",
       "      <td>0.128840</td>\n",
       "      <td>0.129856</td>\n",
       "      <td>0.128958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0         1         2         3  \\\n",
       "local_mae                          0.122145  0.136123  0.149602  0.133117   \n",
       "central_mae                        0.122274  0.143926  0.151888  0.136231   \n",
       "fed_local_mae                      0.121948  0.136047  0.147786  0.136046   \n",
       "local_fine_tune_mae                0.119969  0.134683  0.147717  0.132811   \n",
       "local_fine_tune_benchmark_ewc_mae  0.120102  0.134500  0.147695  0.132730   \n",
       "fed_local_proposed_mae             0.120353  0.136813  0.148757  0.136157   \n",
       "local_fine_tune_noewc_mae          0.117046  0.135011  0.139706  0.125178   \n",
       "local_fine_tune_ewc_mae            0.117143  0.134488  0.139598  0.124503   \n",
       "\n",
       "                                          4         5         6  mean_mae  \n",
       "local_mae                          0.136393  0.132337  0.139272  0.135570  \n",
       "central_mae                        0.142540  0.133549  0.145579  0.139427  \n",
       "fed_local_mae                      0.135507  0.131167  0.138975  0.135354  \n",
       "local_fine_tune_mae                0.135038  0.128996  0.138121  0.133905  \n",
       "local_fine_tune_benchmark_ewc_mae  0.135139  0.128970  0.138339  0.133925  \n",
       "fed_local_proposed_mae             0.134694  0.129513  0.134197  0.134355  \n",
       "local_fine_tune_noewc_mae          0.128088  0.128890  0.130428  0.129192  \n",
       "local_fine_tune_ewc_mae            0.128275  0.128840  0.129856  0.128958  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>mean_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>local_rmse</th>\n",
       "      <td>0.168491</td>\n",
       "      <td>0.196137</td>\n",
       "      <td>0.213595</td>\n",
       "      <td>0.188012</td>\n",
       "      <td>0.200688</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.203647</td>\n",
       "      <td>0.194378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central_rmse</th>\n",
       "      <td>0.168512</td>\n",
       "      <td>0.197552</td>\n",
       "      <td>0.213132</td>\n",
       "      <td>0.188328</td>\n",
       "      <td>0.203443</td>\n",
       "      <td>0.186499</td>\n",
       "      <td>0.201457</td>\n",
       "      <td>0.194132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fed_local_rmse</th>\n",
       "      <td>0.173638</td>\n",
       "      <td>0.195965</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.194083</td>\n",
       "      <td>0.203288</td>\n",
       "      <td>0.189814</td>\n",
       "      <td>0.200816</td>\n",
       "      <td>0.196187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_rmse</th>\n",
       "      <td>0.168188</td>\n",
       "      <td>0.192687</td>\n",
       "      <td>0.214521</td>\n",
       "      <td>0.189909</td>\n",
       "      <td>0.201606</td>\n",
       "      <td>0.187021</td>\n",
       "      <td>0.201710</td>\n",
       "      <td>0.193663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_benchmark_ewc_rmse</th>\n",
       "      <td>0.167315</td>\n",
       "      <td>0.193784</td>\n",
       "      <td>0.214428</td>\n",
       "      <td>0.190103</td>\n",
       "      <td>0.201558</td>\n",
       "      <td>0.187145</td>\n",
       "      <td>0.201916</td>\n",
       "      <td>0.193750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fed_local_proposed_rmse</th>\n",
       "      <td>0.168419</td>\n",
       "      <td>0.194372</td>\n",
       "      <td>0.208031</td>\n",
       "      <td>0.187653</td>\n",
       "      <td>0.189501</td>\n",
       "      <td>0.186364</td>\n",
       "      <td>0.189509</td>\n",
       "      <td>0.189121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_noewc_rmse</th>\n",
       "      <td>0.162667</td>\n",
       "      <td>0.193099</td>\n",
       "      <td>0.199383</td>\n",
       "      <td>0.179286</td>\n",
       "      <td>0.186069</td>\n",
       "      <td>0.187247</td>\n",
       "      <td>0.186019</td>\n",
       "      <td>0.184824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_fine_tune_ewc_rmse</th>\n",
       "      <td>0.162712</td>\n",
       "      <td>0.193578</td>\n",
       "      <td>0.199099</td>\n",
       "      <td>0.178751</td>\n",
       "      <td>0.187208</td>\n",
       "      <td>0.187316</td>\n",
       "      <td>0.186385</td>\n",
       "      <td>0.185007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0         1         2         3  \\\n",
       "local_rmse                          0.168491  0.196137  0.213595  0.188012   \n",
       "central_rmse                        0.168512  0.197552  0.213132  0.188328   \n",
       "fed_local_rmse                      0.173638  0.195965  0.215706  0.194083   \n",
       "local_fine_tune_rmse                0.168188  0.192687  0.214521  0.189909   \n",
       "local_fine_tune_benchmark_ewc_rmse  0.167315  0.193784  0.214428  0.190103   \n",
       "fed_local_proposed_rmse             0.168419  0.194372  0.208031  0.187653   \n",
       "local_fine_tune_noewc_rmse          0.162667  0.193099  0.199383  0.179286   \n",
       "local_fine_tune_ewc_rmse            0.162712  0.193578  0.199099  0.178751   \n",
       "\n",
       "                                           4         5         6  mean_rmse  \n",
       "local_rmse                          0.200688  0.190078  0.203647   0.194378  \n",
       "central_rmse                        0.203443  0.186499  0.201457   0.194132  \n",
       "fed_local_rmse                      0.203288  0.189814  0.200816   0.196187  \n",
       "local_fine_tune_rmse                0.201606  0.187021  0.201710   0.193663  \n",
       "local_fine_tune_benchmark_ewc_rmse  0.201558  0.187145  0.201916   0.193750  \n",
       "fed_local_proposed_rmse             0.189501  0.186364  0.189509   0.189121  \n",
       "local_fine_tune_noewc_rmse          0.186069  0.187247  0.186019   0.184824  \n",
       "local_fine_tune_ewc_rmse            0.187208  0.187316  0.186385   0.185007  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m Kernel \n",
      "\u001b[1;31m\n",
      "\u001b[1;31m<a href='https://aka.ms/vscodeJupyterKernelCrash'></a>\n",
      "\u001b[1;31m Jupyter <a href='command:jupyter.viewOutput'>log</a>"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_benchmark_losses': local_fine_tune_benchmark_losses,\n",
    "    'local_fine_tune_ewc_benchmark_losses': local_fine_tune_ewc_benchmark_losses,\n",
    "    'fed_local_proposed_losses': fed_local_proposed_losses,\n",
    "    'local_fine_tune_noewc_losses': local_fine_tune_noewc_losses,\n",
    "    'local_fine_tune_ewc_losses': local_fine_tune_ewc_losses\n",
    "}).T\n",
    "\n",
    "df.to_csv('losses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
