{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Data_loader import Dataset_Custom\n",
    "import argparse\n",
    "import warnings\n",
    "from tools import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_data\n",
    "from Model import ANN\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Server import  Server\n",
    "from Clients import Client\n",
    "from Train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=4)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--ensemble_flag', type=bool, default=True)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model12/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "args_train = parser_train.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "clients=[]\n",
    "for path in tqdm(args_train.dataset_paths):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths=path\n",
    "    clients.append(Client(args_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(args_train,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: [0.16786858957413942, 0.1676235998891396, 0.19591147434731868, 0.18001745878527425, 0.172340816142012, 0.18452763549779375, 0.18163522973350466]\n",
      "Epoch: 0 | Loss: 0.0851\n",
      "Epoch: 0 | Loss: 0.0677\n",
      "Epoch: 0 | Loss: 0.1082\n",
      "Epoch: 0 | Loss: 0.0842\n",
      "Epoch: 0 | Loss: 0.0824\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.0874\n",
      "Federated training Epoch [1/200] Val Loss: 0.0725\n",
      "test performance: [0.07624770001801726, 0.08147190712800581, 0.09445986327753492, 0.08585161174813362, 0.08840716811381791, 0.08492420858716311, 0.08964639950594673]\n",
      "Epoch: 0 | Loss: 0.0632\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0851\n",
      "Epoch: 0 | Loss: 0.0685\n",
      "Epoch: 0 | Loss: 0.0681\n",
      "Epoch: 0 | Loss: 0.0672\n",
      "Epoch: 0 | Loss: 0.0832\n",
      "Federated training Epoch [2/200] Val Loss: 0.0634\n",
      "test performance: [0.06760701211807255, 0.07407047186199933, 0.08220775467534996, 0.07557713283761723, 0.0785346180838469, 0.07411682624880174, 0.07904363443998441]\n",
      "Epoch: 0 | Loss: 0.0529\n",
      "Epoch: 0 | Loss: 0.0669\n",
      "Epoch: 0 | Loss: 0.0566\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Epoch: 0 | Loss: 0.0582\n",
      "Epoch: 0 | Loss: 0.0578\n",
      "Epoch: 0 | Loss: 0.0755\n",
      "Federated training Epoch [3/200] Val Loss: 0.0553\n",
      "test performance: [0.057857706224265164, 0.06612764379290277, 0.07084834172505222, 0.0649336322708285, 0.06707476688013093, 0.06297265329997834, 0.06856115628033876]\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Epoch: 0 | Loss: 0.0615\n",
      "Epoch: 0 | Loss: 0.0667\n",
      "Epoch: 0 | Loss: 0.0568\n",
      "Epoch: 0 | Loss: 0.0611\n",
      "Epoch: 0 | Loss: 0.0433\n",
      "Epoch: 0 | Loss: 0.0580\n",
      "Federated training Epoch [4/200] Val Loss: 0.0527\n",
      "test performance: [0.05438645037962762, 0.06219597643062676, 0.0666347769510052, 0.06101279430831336, 0.062400675691307, 0.0591643930489376, 0.0643813353899407]\n",
      "Epoch: 0 | Loss: 0.0515\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Epoch: 0 | Loss: 0.0666\n",
      "Epoch: 0 | Loss: 0.0639\n",
      "Epoch: 0 | Loss: 0.0493\n",
      "Epoch: 0 | Loss: 0.0546\n",
      "Epoch: 0 | Loss: 0.0613\n",
      "Federated training Epoch [5/200] Val Loss: 0.0511\n",
      "test performance: [0.0526019654313281, 0.059897433723999215, 0.0645389999709513, 0.058854220052288006, 0.060046442069929755, 0.057083809812081186, 0.06188124381260921]\n",
      "Epoch: 0 | Loss: 0.0420\n",
      "Epoch: 0 | Loss: 0.0525\n",
      "Epoch: 0 | Loss: 0.0551\n",
      "Epoch: 0 | Loss: 0.0590\n",
      "Epoch: 0 | Loss: 0.0573\n",
      "Epoch: 0 | Loss: 0.0608\n",
      "Epoch: 0 | Loss: 0.0558\n",
      "Federated training Epoch [6/200] Val Loss: 0.0502\n",
      "test performance: [0.05160561023433119, 0.05828002456269444, 0.06325563995091066, 0.05764235736011234, 0.05850209317437998, 0.055924358815975385, 0.060317404472511514]\n",
      "Epoch: 0 | Loss: 0.0475\n",
      "Epoch: 0 | Loss: 0.0527\n",
      "Epoch: 0 | Loss: 0.0497\n",
      "Epoch: 0 | Loss: 0.0646\n",
      "Epoch: 0 | Loss: 0.0615\n",
      "Epoch: 0 | Loss: 0.0504\n",
      "Epoch: 0 | Loss: 0.0584\n",
      "Federated training Epoch [7/200] Val Loss: 0.0491\n",
      "test performance: [0.050575242384196556, 0.05741659213776049, 0.06245648067393531, 0.056422350322189806, 0.05778908406779783, 0.05481482304198897, 0.05926280702808744]\n",
      "Epoch: 0 | Loss: 0.0451\n",
      "Epoch: 0 | Loss: 0.0485\n",
      "Epoch: 0 | Loss: 0.0524\n",
      "Epoch: 0 | Loss: 0.0440\n",
      "Epoch: 0 | Loss: 0.0510\n",
      "Epoch: 0 | Loss: 0.0509\n",
      "Epoch: 0 | Loss: 0.0472\n",
      "Federated training Epoch [8/200] Val Loss: 0.0482\n",
      "test performance: [0.04981572611563622, 0.05669984106637844, 0.061858567740921286, 0.055515589588002796, 0.05718407271531959, 0.05400230512278129, 0.0585060733281178]\n",
      "Epoch: 0 | Loss: 0.0464\n",
      "Epoch: 0 | Loss: 0.0472\n",
      "Epoch: 0 | Loss: 0.0551\n",
      "Epoch: 0 | Loss: 0.0537\n",
      "Epoch: 0 | Loss: 0.0708\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0538\n",
      "Federated training Epoch [9/200] Val Loss: 0.0478\n",
      "test performance: [0.04945113867113035, 0.056066960900103394, 0.061263586254152534, 0.05505238049217079, 0.0565964098356358, 0.05349805615587185, 0.05782652079854926]\n",
      "Epoch: 0 | Loss: 0.0436\n",
      "Epoch: 0 | Loss: 0.0573\n",
      "Epoch: 0 | Loss: 0.0559\n",
      "Epoch: 0 | Loss: 0.0529\n",
      "Epoch: 0 | Loss: 0.0536\n",
      "Epoch: 0 | Loss: 0.0566\n",
      "Epoch: 0 | Loss: 0.0567\n",
      "Federated training Epoch [10/200] Val Loss: 0.0475\n",
      "test performance: [0.04925091199143088, 0.055448402727201376, 0.060806931723674684, 0.05477154064821462, 0.055957892329164156, 0.053203582801945405, 0.05722463384194121]\n",
      "Epoch: 0 | Loss: 0.0477\n",
      "Epoch: 0 | Loss: 0.0505\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0565\n",
      "Epoch: 0 | Loss: 0.0797\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Epoch: 0 | Loss: 0.0586\n",
      "Federated training Epoch [11/200] Val Loss: 0.0474\n",
      "test performance: [0.049110787065878306, 0.05507912970313879, 0.06049169240238732, 0.05454944268073121, 0.055533260910784546, 0.05296883052087402, 0.0567948848698033]\n",
      "Epoch: 0 | Loss: 0.0452\n",
      "Epoch: 0 | Loss: 0.0488\n",
      "Epoch: 0 | Loss: 0.0640\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0585\n",
      "Epoch: 0 | Loss: 0.0501\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Federated training Epoch [12/200] Val Loss: 0.0476\n",
      "test performance: [0.049372887442985625, 0.05481718013649934, 0.06043123076223347, 0.05471628978338144, 0.055147849807949505, 0.05319531685481333, 0.056487006090632445]\n",
      "Epoch: 0 | Loss: 0.0557\n",
      "Epoch: 0 | Loss: 0.0451\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0477\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0495\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Federated training Epoch [13/200] Val Loss: 0.0465\n",
      "test performance: [0.04834907091133399, 0.05477601127724533, 0.060075275076884926, 0.05372496606935173, 0.05544977906249361, 0.05224142004799557, 0.05641053763077888]\n",
      "Epoch: 0 | Loss: 0.0392\n",
      "Epoch: 0 | Loss: 0.0453\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0494\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Epoch: 0 | Loss: 0.0554\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Federated training Epoch [14/200] Val Loss: 0.0472\n",
      "test performance: [0.048974530152619294, 0.054280135640236614, 0.05982174058381009, 0.05434458305395833, 0.054735392389487325, 0.05275522044872585, 0.05599333545550296]\n",
      "Epoch: 0 | Loss: 0.0511\n",
      "Epoch: 0 | Loss: 0.0456\n",
      "Epoch: 0 | Loss: 0.0576\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0527\n",
      "Epoch: 0 | Loss: 0.0405\n",
      "Federated training Epoch [15/200] Val Loss: 0.0464\n",
      "test performance: [0.04826355448043714, 0.05405912589128703, 0.059532179178236284, 0.05361441619158404, 0.05463184202600219, 0.05207920815693597, 0.05577740112115463]\n",
      "Epoch: 0 | Loss: 0.0389\n",
      "Epoch: 0 | Loss: 0.0635\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0557\n",
      "Epoch: 0 | Loss: 0.0494\n",
      "Epoch: 0 | Loss: 0.0470\n",
      "Epoch: 0 | Loss: 0.0508\n",
      "Federated training Epoch [16/200] Val Loss: 0.0463\n",
      "test performance: [0.04824043680237581, 0.05397037499622531, 0.0593239457588898, 0.05357182309133549, 0.054490080517908074, 0.0519997821036369, 0.055579132757672706]\n",
      "Epoch: 0 | Loss: 0.0478\n",
      "Epoch: 0 | Loss: 0.0446\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0544\n",
      "Epoch: 0 | Loss: 0.0607\n",
      "Epoch: 0 | Loss: 0.0507\n",
      "Epoch: 0 | Loss: 0.0445\n",
      "Federated training Epoch [17/200] Val Loss: 0.0457\n",
      "test performance: [0.047731304515714515, 0.05418185543983358, 0.059365626067331395, 0.05317451503153329, 0.055048660541029824, 0.05165248260233704, 0.05591398571962363]\n",
      "Epoch: 0 | Loss: 0.0446\n",
      "Epoch: 0 | Loss: 0.0406\n",
      "Epoch: 0 | Loss: 0.0481\n",
      "Epoch: 0 | Loss: 0.0506\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0433\n",
      "Epoch: 0 | Loss: 0.0585\n",
      "Federated training Epoch [18/200] Val Loss: 0.0457\n",
      "test performance: [0.04768546953899403, 0.054120287694649334, 0.059254360385239124, 0.053033228395889476, 0.05484843848604862, 0.05151991068372783, 0.05568690789378669]\n",
      "Epoch: 0 | Loss: 0.0572\n",
      "Epoch: 0 | Loss: 0.0426\n",
      "Epoch: 0 | Loss: 0.0457\n",
      "Epoch: 0 | Loss: 0.0359\n",
      "Epoch: 0 | Loss: 0.0637\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Federated training Epoch [19/200] Val Loss: 0.0458\n",
      "test performance: [0.0477485576874181, 0.053653913294922, 0.05891258946989905, 0.05307851800669546, 0.05420040896111359, 0.05149325626991587, 0.05516470177099109]\n",
      "Epoch: 0 | Loss: 0.0491\n",
      "Epoch: 0 | Loss: 0.0508\n",
      "Epoch: 0 | Loss: 0.0510\n",
      "Epoch: 0 | Loss: 0.0560\n",
      "Epoch: 0 | Loss: 0.0556\n",
      "Epoch: 0 | Loss: 0.0378\n",
      "Epoch: 0 | Loss: 0.0631\n",
      "Federated training Epoch [20/200] Val Loss: 0.0456\n",
      "test performance: [0.04764675038343627, 0.05346125638597224, 0.05873340456299994, 0.052967240344988155, 0.05408848543995864, 0.051415206290373244, 0.055040104722935856]\n",
      "Epoch: 0 | Loss: 0.0497\n",
      "Epoch: 0 | Loss: 0.0458\n",
      "Epoch: 0 | Loss: 0.0635\n",
      "Epoch: 0 | Loss: 0.0454\n",
      "Epoch: 0 | Loss: 0.0579\n",
      "Epoch: 0 | Loss: 0.0476\n",
      "Epoch: 0 | Loss: 0.0576\n",
      "Federated training Epoch [21/200] Val Loss: 0.0453\n",
      "test performance: [0.04745755062043054, 0.054023558529068344, 0.05910100750880291, 0.052828297261403846, 0.05487035579137402, 0.051340575400127535, 0.05561673384134288]\n",
      "Epoch: 0 | Loss: 0.0490\n",
      "Epoch: 0 | Loss: 0.0477\n",
      "Epoch: 0 | Loss: 0.0457\n",
      "Epoch: 0 | Loss: 0.0449\n",
      "Epoch: 0 | Loss: 0.0683\n",
      "Epoch: 0 | Loss: 0.0546\n",
      "Epoch: 0 | Loss: 0.0478\n",
      "Federated training Epoch [22/200] Val Loss: 0.0458\n",
      "test performance: [0.04777025880470668, 0.053158860168840784, 0.05854540362903108, 0.05312987813751583, 0.053714397943846574, 0.05151438820877508, 0.05480753770735983]\n",
      "Epoch: 0 | Loss: 0.0459\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Epoch: 0 | Loss: 0.0517\n",
      "Epoch: 0 | Loss: 0.0441\n",
      "Epoch: 0 | Loss: 0.0590\n",
      "Epoch: 0 | Loss: 0.0369\n",
      "Epoch: 0 | Loss: 0.0552\n",
      "Federated training Epoch [23/200] Val Loss: 0.0453\n",
      "test performance: [0.04737842178023229, 0.053300514319681955, 0.058566770261179096, 0.05268700085963084, 0.05390584842008475, 0.0511049487602527, 0.05479521732394622]\n",
      "Epoch: 0 | Loss: 0.0369\n",
      "Epoch: 0 | Loss: 0.0516\n",
      "Epoch: 0 | Loss: 0.0525\n",
      "Epoch: 0 | Loss: 0.0424\n",
      "Epoch: 0 | Loss: 0.0540\n",
      "Epoch: 0 | Loss: 0.0566\n",
      "Epoch: 0 | Loss: 0.0460\n",
      "Federated training Epoch [24/200] Val Loss: 0.0451\n",
      "test performance: [0.047239649638992874, 0.05340012983253149, 0.058558580312520675, 0.05254670277824753, 0.054097949010511376, 0.0510112252660504, 0.05489526410931594]\n",
      "Epoch: 0 | Loss: 0.0482\n",
      "Epoch: 0 | Loss: 0.0439\n",
      "Epoch: 0 | Loss: 0.0395\n",
      "Epoch: 0 | Loss: 0.0548\n",
      "Epoch: 0 | Loss: 0.0479\n",
      "Epoch: 0 | Loss: 0.0467\n",
      "Epoch: 0 | Loss: 0.0653\n",
      "Federated training Epoch [25/200] Val Loss: 0.0450\n",
      "test performance: [0.047180018840638334, 0.05325655501386891, 0.05843393052991939, 0.052464885273565576, 0.05399790094935731, 0.05092182715283069, 0.054771153594379965]\n",
      "Epoch: 0 | Loss: 0.0528\n",
      "Epoch: 0 | Loss: 0.0560\n",
      "Epoch: 0 | Loss: 0.0576\n",
      "Epoch: 0 | Loss: 0.0396\n",
      "Epoch: 0 | Loss: 0.0564\n",
      "Epoch: 0 | Loss: 0.0452\n",
      "Epoch: 0 | Loss: 0.0471\n",
      "Federated training Epoch [26/200] Val Loss: 0.0451\n",
      "test performance: [0.047238148863695255, 0.053153897673912244, 0.05835325157346383, 0.052496311758734183, 0.053835060192297585, 0.05096901930894141, 0.05463744719091752]\n",
      "Epoch: 0 | Loss: 0.0404\n",
      "Epoch: 0 | Loss: 0.0475\n",
      "Epoch: 0 | Loss: 0.0544\n",
      "Epoch: 0 | Loss: 0.0416\n",
      "Epoch: 0 | Loss: 0.0502\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Epoch: 0 | Loss: 0.0446\n",
      "Federated training Epoch [27/200] Val Loss: 0.0453\n",
      "test performance: [0.04741722913076208, 0.05284804845713589, 0.058259192139726794, 0.052768843427096326, 0.05348572135609511, 0.05116015499177044, 0.054473458929625275]\n",
      "Epoch: 0 | Loss: 0.0448\n",
      "Epoch: 0 | Loss: 0.0492\n",
      "Epoch: 0 | Loss: 0.0463\n",
      "Epoch: 0 | Loss: 0.0446\n",
      "Epoch: 0 | Loss: 0.0667\n",
      "Epoch: 0 | Loss: 0.0506\n",
      "Epoch: 0 | Loss: 0.0429\n",
      "Federated training Epoch [28/200] Val Loss: 0.0449\n",
      "test performance: [0.047054514703532196, 0.05311622626262985, 0.05827742919632017, 0.05231836409473868, 0.0538481782981488, 0.05075697133266558, 0.05455158669373965]\n",
      "Epoch: 0 | Loss: 0.0417\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0606\n",
      "Epoch: 0 | Loss: 0.0382\n",
      "Epoch: 0 | Loss: 0.0623\n",
      "Epoch: 0 | Loss: 0.0604\n",
      "Epoch: 0 | Loss: 0.0492\n",
      "Federated training Epoch [29/200] Val Loss: 0.0448\n",
      "test performance: [0.047018831595778465, 0.05300367226798649, 0.05818119073567325, 0.052294765203578834, 0.05381343920064503, 0.05071368980004567, 0.05451015923937706]\n",
      "Epoch: 0 | Loss: 0.0380\n",
      "Epoch: 0 | Loss: 0.0537\n",
      "Epoch: 0 | Loss: 0.0529\n",
      "Epoch: 0 | Loss: 0.0540\n",
      "Epoch: 0 | Loss: 0.0361\n",
      "Epoch: 0 | Loss: 0.0493\n",
      "Epoch: 0 | Loss: 0.0699\n",
      "Federated training Epoch [30/200] Val Loss: 0.0449\n",
      "test performance: [0.047102640695512704, 0.052840843886034945, 0.058105449536687705, 0.05232555035833422, 0.05357362197916189, 0.050827958570053317, 0.054304388581379634]\n",
      "Epoch: 0 | Loss: 0.0465\n",
      "Epoch: 0 | Loss: 0.0458\n",
      "Epoch: 0 | Loss: 0.0609\n",
      "Epoch: 0 | Loss: 0.0586\n",
      "Epoch: 0 | Loss: 0.0454\n",
      "Epoch: 0 | Loss: 0.0447\n",
      "Epoch: 0 | Loss: 0.0463\n",
      "Federated training Epoch [31/200] Val Loss: 0.0449\n",
      "test performance: [0.04708787693629322, 0.05277017852265949, 0.05806501267146166, 0.05241292642352924, 0.05348894237987187, 0.05077711090904801, 0.05430679336470896]\n",
      "Epoch: 0 | Loss: 0.0442\n",
      "Epoch: 0 | Loss: 0.0455\n",
      "Epoch: 0 | Loss: 0.0539\n",
      "Epoch: 0 | Loss: 0.0532\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0377\n",
      "Epoch: 0 | Loss: 0.0529\n",
      "Federated training Epoch [32/200] Val Loss: 0.0452\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.fed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.0944 Val Loss: 0.0712\n",
      "Epoch [2/200] Train Loss: 0.0673 Val Loss: 0.0589\n",
      "Epoch [3/200] Train Loss: 0.0578 Val Loss: 0.0543\n",
      "Epoch [4/200] Train Loss: 0.0538 Val Loss: 0.0515\n",
      "Epoch [5/200] Train Loss: 0.0515 Val Loss: 0.0496\n",
      "Epoch [6/200] Train Loss: 0.0504 Val Loss: 0.0498\n",
      "Epoch [7/200] Train Loss: 0.0494 Val Loss: 0.0478\n",
      "Epoch [8/200] Train Loss: 0.0488 Val Loss: 0.0477\n",
      "Epoch [9/200] Train Loss: 0.0486 Val Loss: 0.0479\n",
      "Epoch [10/200] Train Loss: 0.0479 Val Loss: 0.0468\n",
      "Epoch [11/200] Train Loss: 0.0479 Val Loss: 0.0465\n",
      "Epoch [12/200] Train Loss: 0.0473 Val Loss: 0.0462\n",
      "Epoch [13/200] Train Loss: 0.0472 Val Loss: 0.0461\n",
      "Epoch [14/200] Train Loss: 0.0468 Val Loss: 0.0463\n",
      "Epoch [15/200] Train Loss: 0.0465 Val Loss: 0.0455\n",
      "Epoch [16/200] Train Loss: 0.0464 Val Loss: 0.0455\n",
      "Epoch [17/200] Train Loss: 0.0464 Val Loss: 0.0452\n",
      "Epoch [18/200] Train Loss: 0.0462 Val Loss: 0.0451\n",
      "Epoch [19/200] Train Loss: 0.0460 Val Loss: 0.0450\n",
      "Epoch [20/200] Train Loss: 0.0459 Val Loss: 0.0452\n",
      "Epoch [21/200] Train Loss: 0.0457 Val Loss: 0.0447\n",
      "Epoch [22/200] Train Loss: 0.0455 Val Loss: 0.0448\n",
      "Epoch [23/200] Train Loss: 0.0454 Val Loss: 0.0445\n",
      "Epoch [24/200] Train Loss: 0.0453 Val Loss: 0.0444\n",
      "Epoch [25/200] Train Loss: 0.0453 Val Loss: 0.0452\n",
      "Epoch [26/200] Train Loss: 0.0451 Val Loss: 0.0449\n",
      "Epoch [27/200] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [28/200] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [29/200] Train Loss: 0.0447 Val Loss: 0.0447\n",
      "Epoch [30/200] Train Loss: 0.0448 Val Loss: 0.0442\n",
      "Epoch [31/200] Train Loss: 0.0447 Val Loss: 0.0439\n",
      "Epoch [32/200] Train Loss: 0.0446 Val Loss: 0.0440\n",
      "Epoch [33/200] Train Loss: 0.0445 Val Loss: 0.0440\n",
      "Epoch [34/200] Train Loss: 0.0443 Val Loss: 0.0438\n",
      "Epoch [35/200] Train Loss: 0.0443 Val Loss: 0.0438\n",
      "Epoch [36/200] Train Loss: 0.0442 Val Loss: 0.0446\n",
      "Epoch [37/200] Train Loss: 0.0444 Val Loss: 0.0440\n",
      "Epoch [38/200] Train Loss: 0.0441 Val Loss: 0.0439\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.central_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch Local Training!\n",
      "Epoch [1/200] Train Loss: 0.0863 Val Loss: 0.0703\n",
      "Epoch [2/200] Train Loss: 0.0664 Val Loss: 0.0589\n",
      "Epoch [3/200] Train Loss: 0.0571 Val Loss: 0.0538\n",
      "Epoch [4/200] Train Loss: 0.0534 Val Loss: 0.0509\n",
      "Epoch [5/200] Train Loss: 0.0513 Val Loss: 0.0494\n",
      "Epoch [6/200] Train Loss: 0.0501 Val Loss: 0.0484\n",
      "Epoch [7/200] Train Loss: 0.0494 Val Loss: 0.0479\n",
      "Epoch [8/200] Train Loss: 0.0488 Val Loss: 0.0474\n",
      "Epoch [9/200] Train Loss: 0.0482 Val Loss: 0.0469\n",
      "Epoch [10/200] Train Loss: 0.0479 Val Loss: 0.0467\n",
      "Epoch [11/200] Train Loss: 0.0477 Val Loss: 0.0464\n",
      "Epoch [12/200] Train Loss: 0.0473 Val Loss: 0.0462\n",
      "Epoch [13/200] Train Loss: 0.0470 Val Loss: 0.0467\n",
      "Epoch [14/200] Train Loss: 0.0467 Val Loss: 0.0458\n",
      "Epoch [15/200] Train Loss: 0.0465 Val Loss: 0.0459\n",
      "Epoch [16/200] Train Loss: 0.0463 Val Loss: 0.0454\n",
      "Epoch [17/200] Train Loss: 0.0462 Val Loss: 0.0453\n",
      "Epoch [18/200] Train Loss: 0.0460 Val Loss: 0.0451\n",
      "Epoch [19/200] Train Loss: 0.0458 Val Loss: 0.0451\n",
      "Epoch [20/200] Train Loss: 0.0457 Val Loss: 0.0448\n",
      "Epoch [21/200] Train Loss: 0.0455 Val Loss: 0.0458\n",
      "Epoch [22/200] Train Loss: 0.0455 Val Loss: 0.0447\n",
      "Epoch [23/200] Train Loss: 0.0452 Val Loss: 0.0461\n",
      "Epoch [24/200] Train Loss: 0.0452 Val Loss: 0.0445\n",
      "Epoch [25/200] Train Loss: 0.0451 Val Loss: 0.0494\n",
      "Epoch [26/200] Train Loss: 0.0451 Val Loss: 0.0444\n",
      "Epoch [27/200] Train Loss: 0.0450 Val Loss: 0.0464\n",
      "Epoch [28/200] Train Loss: 0.0451 Val Loss: 0.0461\n",
      "Epoch [29/200] Train Loss: 0.0448 Val Loss: 0.0442\n",
      "Epoch [30/200] Train Loss: 0.0446 Val Loss: 0.0444\n",
      "Epoch [31/200] Train Loss: 0.0447 Val Loss: 0.0442\n",
      "Epoch [32/200] Train Loss: 0.0445 Val Loss: 0.0444\n",
      "Epoch [33/200] Train Loss: 0.0444 Val Loss: 0.0441\n",
      "Epoch [34/200] Train Loss: 0.0443 Val Loss: 0.0439\n",
      "Epoch [35/200] Train Loss: 0.0443 Val Loss: 0.0450\n",
      "Epoch [36/200] Train Loss: 0.0442 Val Loss: 0.0456\n",
      "Epoch [37/200] Train Loss: 0.0440 Val Loss: 0.0464\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0900 Val Loss: 0.0766\n",
      "Epoch [2/200] Train Loss: 0.0708 Val Loss: 0.0643\n",
      "Epoch [3/200] Train Loss: 0.0618 Val Loss: 0.0587\n",
      "Epoch [4/200] Train Loss: 0.0577 Val Loss: 0.0558\n",
      "Epoch [5/200] Train Loss: 0.0557 Val Loss: 0.0536\n",
      "Epoch [6/200] Train Loss: 0.0541 Val Loss: 0.0526\n",
      "Epoch [7/200] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [8/200] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "Epoch [9/200] Train Loss: 0.0516 Val Loss: 0.0505\n",
      "Epoch [10/200] Train Loss: 0.0510 Val Loss: 0.0517\n",
      "Epoch [11/200] Train Loss: 0.0505 Val Loss: 0.0496\n",
      "Epoch [12/200] Train Loss: 0.0502 Val Loss: 0.0496\n",
      "Epoch [13/200] Train Loss: 0.0499 Val Loss: 0.0503\n",
      "Epoch [14/200] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [15/200] Train Loss: 0.0495 Val Loss: 0.0502\n",
      "Epoch [16/200] Train Loss: 0.0490 Val Loss: 0.0484\n",
      "Epoch [17/200] Train Loss: 0.0487 Val Loss: 0.0484\n",
      "Epoch [18/200] Train Loss: 0.0489 Val Loss: 0.0492\n",
      "Epoch [19/200] Train Loss: 0.0483 Val Loss: 0.0479\n",
      "Epoch [20/200] Train Loss: 0.0482 Val Loss: 0.0479\n",
      "Epoch [21/200] Train Loss: 0.0482 Val Loss: 0.0482\n",
      "Epoch [22/200] Train Loss: 0.0477 Val Loss: 0.0475\n",
      "Epoch [23/200] Train Loss: 0.0479 Val Loss: 0.0479\n",
      "Epoch [24/200] Train Loss: 0.0474 Val Loss: 0.0476\n",
      "Epoch [25/200] Train Loss: 0.0473 Val Loss: 0.0474\n",
      "Epoch [26/200] Train Loss: 0.0471 Val Loss: 0.0474\n",
      "Epoch [27/200] Train Loss: 0.0470 Val Loss: 0.0471\n",
      "Epoch [28/200] Train Loss: 0.0469 Val Loss: 0.0469\n",
      "Epoch [29/200] Train Loss: 0.0468 Val Loss: 0.0469\n",
      "Epoch [30/200] Train Loss: 0.0465 Val Loss: 0.0481\n",
      "Epoch [31/200] Train Loss: 0.0468 Val Loss: 0.0468\n",
      "Epoch [32/200] Train Loss: 0.0463 Val Loss: 0.0473\n",
      "Epoch [33/200] Train Loss: 0.0462 Val Loss: 0.0485\n",
      "Epoch [34/200] Train Loss: 0.0460 Val Loss: 0.0475\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1162 Val Loss: 0.0923\n",
      "Epoch [2/200] Train Loss: 0.0868 Val Loss: 0.0765\n",
      "Epoch [3/200] Train Loss: 0.0712 Val Loss: 0.0671\n",
      "Epoch [4/200] Train Loss: 0.0656 Val Loss: 0.0639\n",
      "Epoch [5/200] Train Loss: 0.0634 Val Loss: 0.0624\n",
      "Epoch [6/200] Train Loss: 0.0621 Val Loss: 0.0611\n",
      "Epoch [7/200] Train Loss: 0.0613 Val Loss: 0.0603\n",
      "Epoch [8/200] Train Loss: 0.0605 Val Loss: 0.0600\n",
      "Epoch [9/200] Train Loss: 0.0599 Val Loss: 0.0592\n",
      "Epoch [10/200] Train Loss: 0.0595 Val Loss: 0.0590\n",
      "Epoch [11/200] Train Loss: 0.0591 Val Loss: 0.0585\n",
      "Epoch [12/200] Train Loss: 0.0585 Val Loss: 0.0584\n",
      "Epoch [13/200] Train Loss: 0.0584 Val Loss: 0.0579\n",
      "Epoch [14/200] Train Loss: 0.0582 Val Loss: 0.0576\n",
      "Epoch [15/200] Train Loss: 0.0577 Val Loss: 0.0580\n",
      "Epoch [16/200] Train Loss: 0.0575 Val Loss: 0.0574\n",
      "Epoch [17/200] Train Loss: 0.0574 Val Loss: 0.0574\n",
      "Epoch [18/200] Train Loss: 0.0571 Val Loss: 0.0574\n",
      "Epoch [19/200] Train Loss: 0.0571 Val Loss: 0.0576\n",
      "Epoch [20/200] Train Loss: 0.0567 Val Loss: 0.0569\n",
      "Epoch [21/200] Train Loss: 0.0570 Val Loss: 0.0567\n",
      "Epoch [22/200] Train Loss: 0.0564 Val Loss: 0.0567\n",
      "Epoch [23/200] Train Loss: 0.0563 Val Loss: 0.0567\n",
      "Epoch [24/200] Train Loss: 0.0561 Val Loss: 0.0569\n",
      "Epoch [25/200] Train Loss: 0.0561 Val Loss: 0.0563\n",
      "Epoch [26/200] Train Loss: 0.0562 Val Loss: 0.0563\n",
      "Epoch [27/200] Train Loss: 0.0561 Val Loss: 0.0571\n",
      "Epoch [28/200] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [29/200] Train Loss: 0.0555 Val Loss: 0.0562\n",
      "Epoch [30/200] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [31/200] Train Loss: 0.0554 Val Loss: 0.0560\n",
      "Epoch [32/200] Train Loss: 0.0552 Val Loss: 0.0566\n",
      "Epoch [33/200] Train Loss: 0.0553 Val Loss: 0.0560\n",
      "Epoch [34/200] Train Loss: 0.0551 Val Loss: 0.0562\n",
      "Epoch [35/200] Train Loss: 0.0549 Val Loss: 0.0557\n",
      "Epoch [36/200] Train Loss: 0.0549 Val Loss: 0.0563\n",
      "Epoch [37/200] Train Loss: 0.0550 Val Loss: 0.0554\n",
      "Epoch [38/200] Train Loss: 0.0548 Val Loss: 0.0555\n",
      "Epoch [39/200] Train Loss: 0.0547 Val Loss: 0.0556\n",
      "Epoch [40/200] Train Loss: 0.0547 Val Loss: 0.0554\n",
      "Epoch [41/200] Train Loss: 0.0545 Val Loss: 0.0556\n",
      "Epoch [42/200] Train Loss: 0.0544 Val Loss: 0.0553\n",
      "Epoch [43/200] Train Loss: 0.0543 Val Loss: 0.0563\n",
      "Epoch [44/200] Train Loss: 0.0546 Val Loss: 0.0554\n",
      "Epoch [45/200] Train Loss: 0.0542 Val Loss: 0.0571\n",
      "Epoch [46/200] Train Loss: 0.0542 Val Loss: 0.0552\n",
      "Epoch [47/200] Train Loss: 0.0539 Val Loss: 0.0551\n",
      "Epoch [48/200] Train Loss: 0.0539 Val Loss: 0.0551\n",
      "Epoch [49/200] Train Loss: 0.0540 Val Loss: 0.0551\n",
      "Epoch [50/200] Train Loss: 0.0539 Val Loss: 0.0549\n",
      "Epoch [51/200] Train Loss: 0.0538 Val Loss: 0.0570\n",
      "Epoch [52/200] Train Loss: 0.0538 Val Loss: 0.0564\n",
      "Epoch [53/200] Train Loss: 0.0537 Val Loss: 0.0550\n",
      "Epoch [54/200] Train Loss: 0.0536 Val Loss: 0.0549\n",
      "Epoch [55/200] Train Loss: 0.0535 Val Loss: 0.0557\n",
      "Epoch [56/200] Train Loss: 0.0536 Val Loss: 0.0548\n",
      "Epoch [57/200] Train Loss: 0.0534 Val Loss: 0.0546\n",
      "Epoch [58/200] Train Loss: 0.0533 Val Loss: 0.0554\n",
      "Epoch [59/200] Train Loss: 0.0533 Val Loss: 0.0548\n",
      "Epoch [60/200] Train Loss: 0.0531 Val Loss: 0.0547\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0973 Val Loss: 0.0804\n",
      "Epoch [2/200] Train Loss: 0.0747 Val Loss: 0.0655\n",
      "Epoch [3/200] Train Loss: 0.0644 Val Loss: 0.0600\n",
      "Epoch [4/200] Train Loss: 0.0602 Val Loss: 0.0575\n",
      "Epoch [5/200] Train Loss: 0.0578 Val Loss: 0.0553\n",
      "Epoch [6/200] Train Loss: 0.0564 Val Loss: 0.0544\n",
      "Epoch [7/200] Train Loss: 0.0552 Val Loss: 0.0540\n",
      "Epoch [8/200] Train Loss: 0.0545 Val Loss: 0.0535\n",
      "Epoch [9/200] Train Loss: 0.0539 Val Loss: 0.0523\n",
      "Epoch [10/200] Train Loss: 0.0537 Val Loss: 0.0520\n",
      "Epoch [11/200] Train Loss: 0.0533 Val Loss: 0.0517\n",
      "Epoch [12/200] Train Loss: 0.0529 Val Loss: 0.0520\n",
      "Epoch [13/200] Train Loss: 0.0526 Val Loss: 0.0536\n",
      "Epoch [14/200] Train Loss: 0.0523 Val Loss: 0.0510\n",
      "Epoch [15/200] Train Loss: 0.0520 Val Loss: 0.0513\n",
      "Epoch [16/200] Train Loss: 0.0518 Val Loss: 0.0519\n",
      "Epoch [17/200] Train Loss: 0.0519 Val Loss: 0.0507\n",
      "Epoch [18/200] Train Loss: 0.0515 Val Loss: 0.0505\n",
      "Epoch [19/200] Train Loss: 0.0513 Val Loss: 0.0504\n",
      "Epoch [20/200] Train Loss: 0.0511 Val Loss: 0.0506\n",
      "Epoch [21/200] Train Loss: 0.0509 Val Loss: 0.0501\n",
      "Epoch [22/200] Train Loss: 0.0508 Val Loss: 0.0504\n",
      "Epoch [23/200] Train Loss: 0.0507 Val Loss: 0.0499\n",
      "Epoch [24/200] Train Loss: 0.0506 Val Loss: 0.0500\n",
      "Epoch [25/200] Train Loss: 0.0505 Val Loss: 0.0497\n",
      "Epoch [26/200] Train Loss: 0.0505 Val Loss: 0.0505\n",
      "Epoch [27/200] Train Loss: 0.0503 Val Loss: 0.0513\n",
      "Epoch [28/200] Train Loss: 0.0502 Val Loss: 0.0493\n",
      "Epoch [29/200] Train Loss: 0.0502 Val Loss: 0.0500\n",
      "Epoch [30/200] Train Loss: 0.0501 Val Loss: 0.0495\n",
      "Epoch [31/200] Train Loss: 0.0500 Val Loss: 0.0493\n",
      "Epoch [32/200] Train Loss: 0.0498 Val Loss: 0.0494\n",
      "Epoch [33/200] Train Loss: 0.0498 Val Loss: 0.0493\n",
      "Epoch [34/200] Train Loss: 0.0498 Val Loss: 0.0498\n",
      "Epoch [35/200] Train Loss: 0.0496 Val Loss: 0.0492\n",
      "Epoch [36/200] Train Loss: 0.0498 Val Loss: 0.0492\n",
      "Epoch [37/200] Train Loss: 0.0496 Val Loss: 0.0490\n",
      "Epoch [38/200] Train Loss: 0.0493 Val Loss: 0.0493\n",
      "Epoch [39/200] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [40/200] Train Loss: 0.0492 Val Loss: 0.0488\n",
      "Epoch [41/200] Train Loss: 0.0493 Val Loss: 0.0488\n",
      "Epoch [42/200] Train Loss: 0.0495 Val Loss: 0.0489\n",
      "Epoch [43/200] Train Loss: 0.0491 Val Loss: 0.0491\n",
      "Epoch [44/200] Train Loss: 0.0490 Val Loss: 0.0486\n",
      "Epoch [45/200] Train Loss: 0.0490 Val Loss: 0.0488\n",
      "Epoch [46/200] Train Loss: 0.0491 Val Loss: 0.0489\n",
      "Epoch [47/200] Train Loss: 0.0491 Val Loss: 0.0494\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1014 Val Loss: 0.0825\n",
      "Epoch [2/200] Train Loss: 0.0787 Val Loss: 0.0697\n",
      "Epoch [3/200] Train Loss: 0.0678 Val Loss: 0.0636\n",
      "Epoch [4/200] Train Loss: 0.0632 Val Loss: 0.0604\n",
      "Epoch [5/200] Train Loss: 0.0609 Val Loss: 0.0598\n",
      "Epoch [6/200] Train Loss: 0.0596 Val Loss: 0.0569\n",
      "Epoch [7/200] Train Loss: 0.0584 Val Loss: 0.0560\n",
      "Epoch [8/200] Train Loss: 0.0576 Val Loss: 0.0552\n",
      "Epoch [9/200] Train Loss: 0.0570 Val Loss: 0.0550\n",
      "Epoch [10/200] Train Loss: 0.0568 Val Loss: 0.0544\n",
      "Epoch [11/200] Train Loss: 0.0561 Val Loss: 0.0541\n",
      "Epoch [12/200] Train Loss: 0.0558 Val Loss: 0.0536\n",
      "Epoch [13/200] Train Loss: 0.0555 Val Loss: 0.0536\n",
      "Epoch [14/200] Train Loss: 0.0551 Val Loss: 0.0537\n",
      "Epoch [15/200] Train Loss: 0.0547 Val Loss: 0.0536\n",
      "Epoch [16/200] Train Loss: 0.0546 Val Loss: 0.0541\n",
      "Epoch [17/200] Train Loss: 0.0545 Val Loss: 0.0529\n",
      "Epoch [18/200] Train Loss: 0.0541 Val Loss: 0.0527\n",
      "Epoch [19/200] Train Loss: 0.0540 Val Loss: 0.0525\n",
      "Epoch [20/200] Train Loss: 0.0539 Val Loss: 0.0523\n",
      "Epoch [21/200] Train Loss: 0.0534 Val Loss: 0.0523\n",
      "Epoch [22/200] Train Loss: 0.0536 Val Loss: 0.0521\n",
      "Epoch [23/200] Train Loss: 0.0533 Val Loss: 0.0520\n",
      "Epoch [24/200] Train Loss: 0.0531 Val Loss: 0.0524\n",
      "Epoch [25/200] Train Loss: 0.0530 Val Loss: 0.0519\n",
      "Epoch [26/200] Train Loss: 0.0528 Val Loss: 0.0522\n",
      "Epoch [27/200] Train Loss: 0.0530 Val Loss: 0.0517\n",
      "Epoch [28/200] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [29/200] Train Loss: 0.0525 Val Loss: 0.0517\n",
      "Epoch [30/200] Train Loss: 0.0524 Val Loss: 0.0519\n",
      "Epoch [31/200] Train Loss: 0.0523 Val Loss: 0.0517\n",
      "Epoch [32/200] Train Loss: 0.0523 Val Loss: 0.0515\n",
      "Epoch [33/200] Train Loss: 0.0522 Val Loss: 0.0515\n",
      "Epoch [34/200] Train Loss: 0.0519 Val Loss: 0.0514\n",
      "Epoch [35/200] Train Loss: 0.0518 Val Loss: 0.0521\n",
      "Epoch [36/200] Train Loss: 0.0518 Val Loss: 0.0511\n",
      "Epoch [37/200] Train Loss: 0.0516 Val Loss: 0.0515\n",
      "Epoch [38/200] Train Loss: 0.0516 Val Loss: 0.0513\n",
      "Epoch [39/200] Train Loss: 0.0517 Val Loss: 0.0513\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0958 Val Loss: 0.0799\n",
      "Epoch [2/200] Train Loss: 0.0749 Val Loss: 0.0644\n",
      "Epoch [3/200] Train Loss: 0.0625 Val Loss: 0.0570\n",
      "Epoch [4/200] Train Loss: 0.0576 Val Loss: 0.0545\n",
      "Epoch [5/200] Train Loss: 0.0554 Val Loss: 0.0527\n",
      "Epoch [6/200] Train Loss: 0.0538 Val Loss: 0.0513\n",
      "Epoch [7/200] Train Loss: 0.0530 Val Loss: 0.0506\n",
      "Epoch [8/200] Train Loss: 0.0521 Val Loss: 0.0505\n",
      "Epoch [9/200] Train Loss: 0.0515 Val Loss: 0.0506\n",
      "Epoch [10/200] Train Loss: 0.0510 Val Loss: 0.0494\n",
      "Epoch [11/200] Train Loss: 0.0508 Val Loss: 0.0496\n",
      "Epoch [12/200] Train Loss: 0.0502 Val Loss: 0.0488\n",
      "Epoch [13/200] Train Loss: 0.0500 Val Loss: 0.0485\n",
      "Epoch [14/200] Train Loss: 0.0498 Val Loss: 0.0485\n",
      "Epoch [15/200] Train Loss: 0.0493 Val Loss: 0.0483\n",
      "Epoch [16/200] Train Loss: 0.0490 Val Loss: 0.0480\n",
      "Epoch [17/200] Train Loss: 0.0490 Val Loss: 0.0479\n",
      "Epoch [18/200] Train Loss: 0.0487 Val Loss: 0.0477\n",
      "Epoch [19/200] Train Loss: 0.0485 Val Loss: 0.0476\n",
      "Epoch [20/200] Train Loss: 0.0485 Val Loss: 0.0480\n",
      "Epoch [21/200] Train Loss: 0.0484 Val Loss: 0.0474\n",
      "Epoch [22/200] Train Loss: 0.0481 Val Loss: 0.0474\n",
      "Epoch [23/200] Train Loss: 0.0480 Val Loss: 0.0482\n",
      "Epoch [24/200] Train Loss: 0.0477 Val Loss: 0.0477\n",
      "Epoch [25/200] Train Loss: 0.0479 Val Loss: 0.0472\n",
      "Epoch [26/200] Train Loss: 0.0474 Val Loss: 0.0471\n",
      "Epoch [27/200] Train Loss: 0.0475 Val Loss: 0.0472\n",
      "Epoch [28/200] Train Loss: 0.0473 Val Loss: 0.0470\n",
      "Epoch [29/200] Train Loss: 0.0472 Val Loss: 0.0479\n",
      "Epoch [30/200] Train Loss: 0.0472 Val Loss: 0.0483\n",
      "Epoch [31/200] Train Loss: 0.0470 Val Loss: 0.0483\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1041 Val Loss: 0.0847\n",
      "Epoch [2/200] Train Loss: 0.0790 Val Loss: 0.0696\n",
      "Epoch [3/200] Train Loss: 0.0676 Val Loss: 0.0636\n",
      "Epoch [4/200] Train Loss: 0.0629 Val Loss: 0.0599\n",
      "Epoch [5/200] Train Loss: 0.0605 Val Loss: 0.0581\n",
      "Epoch [6/200] Train Loss: 0.0589 Val Loss: 0.0568\n",
      "Epoch [7/200] Train Loss: 0.0579 Val Loss: 0.0561\n",
      "Epoch [8/200] Train Loss: 0.0571 Val Loss: 0.0554\n",
      "Epoch [9/200] Train Loss: 0.0566 Val Loss: 0.0556\n",
      "Epoch [10/200] Train Loss: 0.0559 Val Loss: 0.0548\n",
      "Epoch [11/200] Train Loss: 0.0555 Val Loss: 0.0546\n",
      "Epoch [12/200] Train Loss: 0.0552 Val Loss: 0.0539\n",
      "Epoch [13/200] Train Loss: 0.0548 Val Loss: 0.0537\n",
      "Epoch [14/200] Train Loss: 0.0545 Val Loss: 0.0534\n",
      "Epoch [15/200] Train Loss: 0.0544 Val Loss: 0.0540\n",
      "Epoch [16/200] Train Loss: 0.0540 Val Loss: 0.0529\n",
      "Epoch [17/200] Train Loss: 0.0538 Val Loss: 0.0533\n",
      "Epoch [18/200] Train Loss: 0.0539 Val Loss: 0.0534\n",
      "Epoch [19/200] Train Loss: 0.0535 Val Loss: 0.0526\n",
      "Epoch [20/200] Train Loss: 0.0533 Val Loss: 0.0525\n",
      "Epoch [21/200] Train Loss: 0.0533 Val Loss: 0.0535\n",
      "Epoch [22/200] Train Loss: 0.0530 Val Loss: 0.0531\n",
      "Epoch [23/200] Train Loss: 0.0529 Val Loss: 0.0523\n",
      "Epoch [24/200] Train Loss: 0.0528 Val Loss: 0.0522\n",
      "Epoch [25/200] Train Loss: 0.0525 Val Loss: 0.0533\n",
      "Epoch [26/200] Train Loss: 0.0525 Val Loss: 0.0519\n",
      "Epoch [27/200] Train Loss: 0.0527 Val Loss: 0.0520\n",
      "Epoch [28/200] Train Loss: 0.0521 Val Loss: 0.0517\n",
      "Epoch [29/200] Train Loss: 0.0521 Val Loss: 0.0521\n",
      "Epoch [30/200] Train Loss: 0.0521 Val Loss: 0.0520\n",
      "Epoch [31/200] Train Loss: 0.0520 Val Loss: 0.0518\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "server.local_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0451 Val Loss: 0.0445\n",
      "Epoch [2/20] Train Loss: 0.0451 Val Loss: 0.0442\n",
      "Epoch [3/20] Train Loss: 0.0451 Val Loss: 0.0442\n",
      "Epoch [4/20] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [5/20] Train Loss: 0.0450 Val Loss: 0.0442\n",
      "Epoch [6/20] Train Loss: 0.0449 Val Loss: 0.0442\n",
      "Epoch [7/20] Train Loss: 0.0449 Val Loss: 0.0441\n",
      "Epoch [8/20] Train Loss: 0.0449 Val Loss: 0.0442\n",
      "Epoch [9/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [10/20] Train Loss: 0.0448 Val Loss: 0.0445\n",
      "Epoch [11/20] Train Loss: 0.0448 Val Loss: 0.0441\n",
      "Epoch [12/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [13/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [14/20] Train Loss: 0.0447 Val Loss: 0.0440\n",
      "Epoch [15/20] Train Loss: 0.0447 Val Loss: 0.0442\n",
      "Epoch [16/20] Train Loss: 0.0446 Val Loss: 0.0440\n",
      "Epoch [17/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [18/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [19/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "Epoch [20/20] Train Loss: 0.0446 Val Loss: 0.0439\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0485 Val Loss: 0.0481\n",
      "Epoch [2/20] Train Loss: 0.0486 Val Loss: 0.0480\n",
      "Epoch [3/20] Train Loss: 0.0485 Val Loss: 0.0479\n",
      "Epoch [4/20] Train Loss: 0.0484 Val Loss: 0.0482\n",
      "Epoch [5/20] Train Loss: 0.0483 Val Loss: 0.0478\n",
      "Epoch [6/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [7/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [8/20] Train Loss: 0.0482 Val Loss: 0.0477\n",
      "Epoch [9/20] Train Loss: 0.0481 Val Loss: 0.0476\n",
      "Epoch [10/20] Train Loss: 0.0481 Val Loss: 0.0478\n",
      "Epoch [11/20] Train Loss: 0.0480 Val Loss: 0.0477\n",
      "Epoch [12/20] Train Loss: 0.0480 Val Loss: 0.0476\n",
      "Epoch [13/20] Train Loss: 0.0480 Val Loss: 0.0475\n",
      "Epoch [14/20] Train Loss: 0.0479 Val Loss: 0.0477\n",
      "Epoch [15/20] Train Loss: 0.0479 Val Loss: 0.0476\n",
      "Epoch [16/20] Train Loss: 0.0479 Val Loss: 0.0475\n",
      "Epoch [17/20] Train Loss: 0.0478 Val Loss: 0.0476\n",
      "Epoch [18/20] Train Loss: 0.0478 Val Loss: 0.0474\n",
      "Epoch [19/20] Train Loss: 0.0478 Val Loss: 0.0476\n",
      "Epoch [20/20] Train Loss: 0.0478 Val Loss: 0.0475\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0558 Val Loss: 0.0563\n",
      "Epoch [2/20] Train Loss: 0.0560 Val Loss: 0.0563\n",
      "Epoch [3/20] Train Loss: 0.0559 Val Loss: 0.0562\n",
      "Epoch [4/20] Train Loss: 0.0559 Val Loss: 0.0562\n",
      "Epoch [5/20] Train Loss: 0.0558 Val Loss: 0.0562\n",
      "Epoch [6/20] Train Loss: 0.0558 Val Loss: 0.0563\n",
      "Epoch [7/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [8/20] Train Loss: 0.0558 Val Loss: 0.0564\n",
      "Epoch [9/20] Train Loss: 0.0557 Val Loss: 0.0561\n",
      "Epoch [10/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [11/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [12/20] Train Loss: 0.0556 Val Loss: 0.0561\n",
      "Epoch [13/20] Train Loss: 0.0556 Val Loss: 0.0560\n",
      "Epoch [14/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [15/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [16/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [17/20] Train Loss: 0.0555 Val Loss: 0.0560\n",
      "Epoch [18/20] Train Loss: 0.0554 Val Loss: 0.0560\n",
      "Epoch [19/20] Train Loss: 0.0554 Val Loss: 0.0561\n",
      "Epoch [20/20] Train Loss: 0.0554 Val Loss: 0.0560\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0498 Val Loss: 0.0490\n",
      "Epoch [2/20] Train Loss: 0.0499 Val Loss: 0.0490\n",
      "Epoch [3/20] Train Loss: 0.0498 Val Loss: 0.0489\n",
      "Epoch [4/20] Train Loss: 0.0497 Val Loss: 0.0490\n",
      "Epoch [5/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [6/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [7/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [8/20] Train Loss: 0.0497 Val Loss: 0.0489\n",
      "Epoch [9/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [10/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [11/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [12/20] Train Loss: 0.0496 Val Loss: 0.0488\n",
      "Epoch [13/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [14/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [15/20] Train Loss: 0.0495 Val Loss: 0.0489\n",
      "Epoch [16/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [17/20] Train Loss: 0.0495 Val Loss: 0.0488\n",
      "Epoch [18/20] Train Loss: 0.0494 Val Loss: 0.0490\n",
      "Epoch [19/20] Train Loss: 0.0494 Val Loss: 0.0488\n",
      "Epoch [20/20] Train Loss: 0.0494 Val Loss: 0.0488\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0532 Val Loss: 0.0521\n",
      "Epoch [2/20] Train Loss: 0.0533 Val Loss: 0.0520\n",
      "Epoch [3/20] Train Loss: 0.0532 Val Loss: 0.0520\n",
      "Epoch [4/20] Train Loss: 0.0532 Val Loss: 0.0519\n",
      "Epoch [5/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [6/20] Train Loss: 0.0531 Val Loss: 0.0519\n",
      "Epoch [7/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [8/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [9/20] Train Loss: 0.0530 Val Loss: 0.0518\n",
      "Epoch [10/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [11/20] Train Loss: 0.0529 Val Loss: 0.0518\n",
      "Epoch [12/20] Train Loss: 0.0529 Val Loss: 0.0517\n",
      "Epoch [13/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [14/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [15/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [16/20] Train Loss: 0.0528 Val Loss: 0.0517\n",
      "Epoch [17/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [18/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [19/20] Train Loss: 0.0527 Val Loss: 0.0516\n",
      "Epoch [20/20] Train Loss: 0.0527 Val Loss: 0.0515\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0477 Val Loss: 0.0471\n",
      "Epoch [2/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [3/20] Train Loss: 0.0478 Val Loss: 0.0471\n",
      "Epoch [4/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [5/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [6/20] Train Loss: 0.0477 Val Loss: 0.0471\n",
      "Epoch [7/20] Train Loss: 0.0477 Val Loss: 0.0470\n",
      "Epoch [8/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [9/20] Train Loss: 0.0476 Val Loss: 0.0470\n",
      "Epoch [10/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "Epoch [11/20] Train Loss: 0.0476 Val Loss: 0.0469\n",
      "Epoch [12/20] Train Loss: 0.0475 Val Loss: 0.0470\n",
      "Epoch [13/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [14/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [15/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [16/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [17/20] Train Loss: 0.0475 Val Loss: 0.0469\n",
      "Epoch [18/20] Train Loss: 0.0474 Val Loss: 0.0469\n",
      "Epoch [19/20] Train Loss: 0.0474 Val Loss: 0.0472\n",
      "Epoch [20/20] Train Loss: 0.0474 Val Loss: 0.0469\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0524 Val Loss: 0.0520\n",
      "Epoch [2/20] Train Loss: 0.0525 Val Loss: 0.0518\n",
      "Epoch [3/20] Train Loss: 0.0525 Val Loss: 0.0520\n",
      "Epoch [4/20] Train Loss: 0.0525 Val Loss: 0.0517\n",
      "Epoch [5/20] Train Loss: 0.0525 Val Loss: 0.0519\n",
      "Epoch [6/20] Train Loss: 0.0523 Val Loss: 0.0519\n",
      "Epoch [7/20] Train Loss: 0.0524 Val Loss: 0.0516\n",
      "Epoch [8/20] Train Loss: 0.0523 Val Loss: 0.0517\n",
      "Epoch [9/20] Train Loss: 0.0523 Val Loss: 0.0518\n",
      "Epoch [10/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [11/20] Train Loss: 0.0523 Val Loss: 0.0516\n",
      "Epoch [12/20] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "Epoch [13/20] Train Loss: 0.0522 Val Loss: 0.0515\n",
      "Epoch [14/20] Train Loss: 0.0522 Val Loss: 0.0516\n",
      "Epoch [15/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [16/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [17/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [18/20] Train Loss: 0.0521 Val Loss: 0.0516\n",
      "Epoch [19/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "Epoch [20/20] Train Loss: 0.0521 Val Loss: 0.0515\n",
      "[0.04672484316748299, 0.05219567769959773, 0.05765293501572658, 0.05168611156971079, 0.05303531432562597, 0.05059497972491057, 0.05391620361080317]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_losses=[]\n",
    "local_fine_tune_preds=[]\n",
    "local_fine_tune_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(args_train.number_clients):\n",
    "    local_fine_tune_pred,local_fine_tune_loss,local_fine_tune_model=clients[i].local_fine_tune()\n",
    "    local_fine_tune_losses.append(local_fine_tune_loss)\n",
    "    local_fine_tune_preds.append(local_fine_tune_pred)\n",
    "    local_fine_tune_models.append(local_fine_tune_model)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04738055886573171, 0.05263746971238966, 0.05798239472692143, 0.05267014441220728, 0.05327578918523576, 0.05104644261683299, 0.05414957301818753]\n",
      "0.05273462464821519\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(args_train.number_clients):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)\n",
    "print(np.mean(fed_local_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047523875287032294, 0.053075658706055115, 0.0580338093337335, 0.05174410740576991, 0.05369003568712163, 0.05185019247846244, 0.05432962053391623]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    local_pred,local_loss,local_model=clients[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04773888712085477, 0.055938527995899115, 0.05991943744457748, 0.053290099919811915, 0.055807694607758765, 0.05220189785635839, 0.056752502771528206]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    central_pred,central_loss,central_model=server.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047523875287032294, 0.053075658706055115, 0.0580338093337335, 0.05174410740576991, 0.05369003568712163, 0.05185019247846244, 0.05432962053391623]\n",
      "[0.04773888712085477, 0.055938527995899115, 0.05991943744457748, 0.053290099919811915, 0.055807694607758765, 0.05220189785635839, 0.056752502771528206]\n",
      "[0.04738055886573171, 0.05263746971238966, 0.05798239472692143, 0.05267014441220728, 0.05327578918523576, 0.05104644261683299, 0.05414957301818753]\n",
      "[0.04672484316748299, 0.05219567769959773, 0.05765293501572658, 0.05168611156971079, 0.05303531432562597, 0.05059497972491057, 0.05391620361080317]\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_losses': local_fine_tune_losses\n",
    "})\n",
    "df.T.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the server object\n",
    "with open('../result/4/server_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "# Save the clients object\n",
    "with open('../result/4/clients_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save server and clients\n",
    "with open('server.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "with open('clients.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)\n",
    "\n",
    "# Load server and clients\n",
    "with open('server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "with open('clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
