{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Data_loader import Dataset_Custom\n",
    "import argparse\n",
    "import warnings\n",
    "from tools import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_data\n",
    "from Model import ANN\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Server import  Server\n",
    "from Clients import Client\n",
    "from Train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=2)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--ensemble_flag', type=bool, default=True)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model12/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "args_train = parser_train.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "clients=[]\n",
    "for path in tqdm(args_train.dataset_paths):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths=path\n",
    "    clients.append(Client(args_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(args_train,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: [0.1678826533870338, 0.1676450884097243, 0.19594906395530864, 0.18001982141030978, 0.1723320048863757, 0.18455327748742006, 0.18167505989948365]\n",
      "Epoch: 0 | Loss: 0.0850\n",
      "Epoch: 0 | Loss: 0.0630\n",
      "Epoch: 0 | Loss: 0.1112\n",
      "Epoch: 0 | Loss: 0.0855\n",
      "Epoch: 0 | Loss: 0.0872\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0827\n",
      "Federated training Epoch [1/200] Val Loss: 0.0716\n",
      "test performance: [0.07541269663568229, 0.08067317516224025, 0.0936548399853788, 0.08484051999163954, 0.0873298574851392, 0.0839765701851208, 0.08878994756059287]\n",
      "Epoch: 0 | Loss: 0.0500\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Epoch: 0 | Loss: 0.0580\n",
      "Epoch: 0 | Loss: 0.0551\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0712\n",
      "Federated training Epoch [2/200] Val Loss: 0.0560\n",
      "test performance: [0.05982974134640743, 0.06668905890865685, 0.07294007199369881, 0.06689255068410341, 0.06950944847082846, 0.06527284491960317, 0.07013873660248028]\n",
      "Epoch: 0 | Loss: 0.0509\n",
      "Epoch: 0 | Loss: 0.0518\n",
      "Epoch: 0 | Loss: 0.0590\n",
      "Epoch: 0 | Loss: 0.0551\n",
      "Epoch: 0 | Loss: 0.0552\n",
      "Epoch: 0 | Loss: 0.0464\n",
      "Epoch: 0 | Loss: 0.0666\n",
      "Federated training Epoch [3/200] Val Loss: 0.0458\n",
      "test performance: [0.04765244244840251, 0.05445331685908445, 0.05883225252571171, 0.05341201583689002, 0.05513208238918283, 0.05194258601213359, 0.05675933306577475]\n",
      "Epoch: 0 | Loss: 0.0450\n",
      "Epoch: 0 | Loss: 0.0440\n",
      "Epoch: 0 | Loss: 0.0476\n",
      "Epoch: 0 | Loss: 0.0509\n",
      "Epoch: 0 | Loss: 0.0506\n",
      "Epoch: 0 | Loss: 0.0387\n",
      "Epoch: 0 | Loss: 0.0459\n",
      "Federated training Epoch [4/200] Val Loss: 0.0415\n",
      "test performance: [0.04304076467755519, 0.04808858924940841, 0.05282133603340959, 0.04787390405465275, 0.04879960235310336, 0.04658163861971196, 0.05030292773313106]\n",
      "Epoch: 0 | Loss: 0.0451\n",
      "Epoch: 0 | Loss: 0.0516\n",
      "Epoch: 0 | Loss: 0.0471\n",
      "Epoch: 0 | Loss: 0.0400\n",
      "Epoch: 0 | Loss: 0.0378\n",
      "Epoch: 0 | Loss: 0.0530\n",
      "Epoch: 0 | Loss: 0.0475\n",
      "Federated training Epoch [5/200] Val Loss: 0.0394\n",
      "test performance: [0.04102015056430477, 0.04480051156133413, 0.04985575434713535, 0.045247239915475454, 0.04557860734528058, 0.04405222857396488, 0.04677905156340909]\n",
      "Epoch: 0 | Loss: 0.0387\n",
      "Epoch: 0 | Loss: 0.0341\n",
      "Epoch: 0 | Loss: 0.0408\n",
      "Epoch: 0 | Loss: 0.0351\n",
      "Epoch: 0 | Loss: 0.0539\n",
      "Epoch: 0 | Loss: 0.0478\n",
      "Epoch: 0 | Loss: 0.0419\n",
      "Federated training Epoch [6/200] Val Loss: 0.0369\n",
      "test performance: [0.03858044619786821, 0.04266920930718723, 0.047806459906421704, 0.042590174326443506, 0.0438816748393623, 0.041631142678989534, 0.044756170655664514]\n",
      "Epoch: 0 | Loss: 0.0378\n",
      "Epoch: 0 | Loss: 0.0422\n",
      "Epoch: 0 | Loss: 0.0427\n",
      "Epoch: 0 | Loss: 0.0475\n",
      "Epoch: 0 | Loss: 0.0491\n",
      "Epoch: 0 | Loss: 0.0387\n",
      "Epoch: 0 | Loss: 0.0429\n",
      "Federated training Epoch [7/200] Val Loss: 0.0360\n",
      "test performance: [0.03785936237784893, 0.041192153028582465, 0.046386953232784384, 0.04162403652827217, 0.04239423315308682, 0.04063782169904611, 0.04313175890543689]\n",
      "Epoch: 0 | Loss: 0.0281\n",
      "Epoch: 0 | Loss: 0.0417\n",
      "Epoch: 0 | Loss: 0.0450\n",
      "Epoch: 0 | Loss: 0.0406\n",
      "Epoch: 0 | Loss: 0.0479\n",
      "Epoch: 0 | Loss: 0.0463\n",
      "Epoch: 0 | Loss: 0.0437\n",
      "Federated training Epoch [8/200] Val Loss: 0.0348\n",
      "test performance: [0.03676961625173484, 0.04021683418230243, 0.04539493449695715, 0.04040586355792945, 0.041577419444714506, 0.03948249724655323, 0.042188116036712714]\n",
      "Epoch: 0 | Loss: 0.0351\n",
      "Epoch: 0 | Loss: 0.0340\n",
      "Epoch: 0 | Loss: 0.0542\n",
      "Epoch: 0 | Loss: 0.0413\n",
      "Epoch: 0 | Loss: 0.0494\n",
      "Epoch: 0 | Loss: 0.0415\n",
      "Epoch: 0 | Loss: 0.0419\n",
      "Federated training Epoch [9/200] Val Loss: 0.0341\n",
      "test performance: [0.03616286326863178, 0.03951034430822689, 0.044666634815145434, 0.03970940414280312, 0.04095473474095741, 0.03878030845614737, 0.04145197658990956]\n",
      "Epoch: 0 | Loss: 0.0372\n",
      "Epoch: 0 | Loss: 0.0424\n",
      "Epoch: 0 | Loss: 0.0479\n",
      "Epoch: 0 | Loss: 0.0402\n",
      "Epoch: 0 | Loss: 0.0397\n",
      "Epoch: 0 | Loss: 0.0418\n",
      "Epoch: 0 | Loss: 0.0358\n",
      "Federated training Epoch [10/200] Val Loss: 0.0336\n",
      "test performance: [0.035697484318779345, 0.038844467070566054, 0.044023880460746075, 0.039197606512996024, 0.04040678050879338, 0.03829301421670881, 0.040881087104088235]\n",
      "Epoch: 0 | Loss: 0.0414\n",
      "Epoch: 0 | Loss: 0.0321\n",
      "Epoch: 0 | Loss: 0.0606\n",
      "Epoch: 0 | Loss: 0.0342\n",
      "Epoch: 0 | Loss: 0.0553\n",
      "Epoch: 0 | Loss: 0.0412\n",
      "Epoch: 0 | Loss: 0.0480\n",
      "Federated training Epoch [11/200] Val Loss: 0.0332\n",
      "test performance: [0.03534118154354087, 0.03867907950348438, 0.04374494996840415, 0.038888176399193806, 0.04044037349266957, 0.03796696994606763, 0.04077426909966624]\n",
      "Epoch: 0 | Loss: 0.0281\n",
      "Epoch: 0 | Loss: 0.0304\n",
      "Epoch: 0 | Loss: 0.0427\n",
      "Epoch: 0 | Loss: 0.0375\n",
      "Epoch: 0 | Loss: 0.0375\n",
      "Epoch: 0 | Loss: 0.0400\n",
      "Epoch: 0 | Loss: 0.0469\n",
      "Federated training Epoch [12/200] Val Loss: 0.0330\n",
      "test performance: [0.03523570792877102, 0.037905174677502616, 0.043092929471080024, 0.038545494522797326, 0.03942761698112606, 0.03770672673780522, 0.039895477883313615]\n",
      "Epoch: 0 | Loss: 0.0419\n",
      "Epoch: 0 | Loss: 0.0308\n",
      "Epoch: 0 | Loss: 0.0456\n",
      "Epoch: 0 | Loss: 0.0280\n",
      "Epoch: 0 | Loss: 0.0441\n",
      "Epoch: 0 | Loss: 0.0336\n",
      "Epoch: 0 | Loss: 0.0364\n",
      "Federated training Epoch [13/200] Val Loss: 0.0329\n",
      "test performance: [0.03511919140216116, 0.03763259935180006, 0.04289131560554243, 0.03827546439643898, 0.03905000428235388, 0.037446365927741546, 0.03951374790950181]\n",
      "Epoch: 0 | Loss: 0.0305\n",
      "Epoch: 0 | Loss: 0.0309\n",
      "Epoch: 0 | Loss: 0.0463\n",
      "Epoch: 0 | Loss: 0.0354\n",
      "Epoch: 0 | Loss: 0.0367\n",
      "Epoch: 0 | Loss: 0.0416\n",
      "Epoch: 0 | Loss: 0.0388\n",
      "Federated training Epoch [14/200] Val Loss: 0.0326\n",
      "test performance: [0.03480623629620324, 0.037281645570358596, 0.042442050252160794, 0.03796564146225685, 0.03884713724255562, 0.037139933852938144, 0.03926446003686279]\n",
      "Epoch: 0 | Loss: 0.0399\n",
      "Epoch: 0 | Loss: 0.0415\n",
      "Epoch: 0 | Loss: 0.0347\n",
      "Epoch: 0 | Loss: 0.0342\n",
      "Epoch: 0 | Loss: 0.0462\n",
      "Epoch: 0 | Loss: 0.0323\n",
      "Epoch: 0 | Loss: 0.0311\n",
      "Federated training Epoch [15/200] Val Loss: 0.0323\n",
      "test performance: [0.034571196826506556, 0.03702250871271508, 0.04220389609850229, 0.0377125546993203, 0.03859547799618991, 0.03685036050602283, 0.038990797436706824]\n",
      "Epoch: 0 | Loss: 0.0301\n",
      "Epoch: 0 | Loss: 0.0360\n",
      "Epoch: 0 | Loss: 0.0441\n",
      "Epoch: 0 | Loss: 0.0398\n",
      "Epoch: 0 | Loss: 0.0384\n",
      "Epoch: 0 | Loss: 0.0359\n",
      "Epoch: 0 | Loss: 0.0336\n",
      "Federated training Epoch [16/200] Val Loss: 0.0322\n",
      "test performance: [0.03447845265307553, 0.03683904081276835, 0.04194481166283766, 0.03758492124624142, 0.03844875333613831, 0.03674151801044912, 0.038807095953403675]\n",
      "Epoch: 0 | Loss: 0.0325\n",
      "Epoch: 0 | Loss: 0.0356\n",
      "Epoch: 0 | Loss: 0.0514\n",
      "Epoch: 0 | Loss: 0.0408\n",
      "Epoch: 0 | Loss: 0.0465\n",
      "Epoch: 0 | Loss: 0.0327\n",
      "Epoch: 0 | Loss: 0.0324\n",
      "Federated training Epoch [17/200] Val Loss: 0.0320\n",
      "test performance: [0.03431488548354437, 0.03657424455661684, 0.04174383176643759, 0.03739727052783415, 0.03822564091278265, 0.03657025720426583, 0.03863614541159509]\n",
      "Epoch: 0 | Loss: 0.0323\n",
      "Epoch: 0 | Loss: 0.0336\n",
      "Epoch: 0 | Loss: 0.0418\n",
      "Epoch: 0 | Loss: 0.0349\n",
      "Epoch: 0 | Loss: 0.0424\n",
      "Epoch: 0 | Loss: 0.0333\n",
      "Epoch: 0 | Loss: 0.0452\n",
      "Federated training Epoch [18/200] Val Loss: 0.0317\n",
      "test performance: [0.034061723366405255, 0.036505641804829445, 0.04160454577753601, 0.03714309781088098, 0.03823407790432238, 0.03632477659106969, 0.03855451519843446]\n",
      "Epoch: 0 | Loss: 0.0387\n",
      "Epoch: 0 | Loss: 0.0291\n",
      "Epoch: 0 | Loss: 0.0337\n",
      "Epoch: 0 | Loss: 0.0289\n",
      "Epoch: 0 | Loss: 0.0420\n",
      "Epoch: 0 | Loss: 0.0398\n",
      "Epoch: 0 | Loss: 0.0379\n",
      "Federated training Epoch [19/200] Val Loss: 0.0316\n",
      "test performance: [0.03400470232527244, 0.03628605888033771, 0.04144764743260529, 0.03702566627578886, 0.03796602994815944, 0.03622056027134395, 0.03832593466448661]\n",
      "Epoch: 0 | Loss: 0.0407\n",
      "Epoch: 0 | Loss: 0.0375\n",
      "Epoch: 0 | Loss: 0.0384\n",
      "Epoch: 0 | Loss: 0.0372\n",
      "Epoch: 0 | Loss: 0.0345\n",
      "Epoch: 0 | Loss: 0.0337\n",
      "Epoch: 0 | Loss: 0.0446\n",
      "Federated training Epoch [20/200] Val Loss: 0.0315\n",
      "test performance: [0.03389737387315357, 0.03617406636476517, 0.041275604838530905, 0.03695616236096886, 0.037956180510251486, 0.036127204171139495, 0.03832441981090871]\n",
      "Epoch: 0 | Loss: 0.0390\n",
      "Epoch: 0 | Loss: 0.0388\n",
      "Epoch: 0 | Loss: 0.0465\n",
      "Epoch: 0 | Loss: 0.0349\n",
      "Epoch: 0 | Loss: 0.0441\n",
      "Epoch: 0 | Loss: 0.0366\n",
      "Epoch: 0 | Loss: 0.0407\n",
      "Federated training Epoch [21/200] Val Loss: 0.0314\n",
      "test performance: [0.03375823430921117, 0.03619548440464351, 0.04128974849638874, 0.03688911189746163, 0.038126491915995944, 0.03602472904508244, 0.03845406398942618]\n",
      "Epoch: 0 | Loss: 0.0384\n",
      "Epoch: 0 | Loss: 0.0264\n",
      "Epoch: 0 | Loss: 0.0332\n",
      "Epoch: 0 | Loss: 0.0311\n",
      "Epoch: 0 | Loss: 0.0413\n",
      "Epoch: 0 | Loss: 0.0443\n",
      "Epoch: 0 | Loss: 0.0302\n",
      "Federated training Epoch [22/200] Val Loss: 0.0315\n",
      "test performance: [0.03389110498462025, 0.03589038716705695, 0.0410661282183679, 0.03686960266061025, 0.0375800736835354, 0.036041721436896754, 0.03802269345710743]\n",
      "Epoch: 0 | Loss: 0.0368\n",
      "Epoch: 0 | Loss: 0.0313\n",
      "Epoch: 0 | Loss: 0.0365\n",
      "Epoch: 0 | Loss: 0.0298\n",
      "Epoch: 0 | Loss: 0.0421\n",
      "Epoch: 0 | Loss: 0.0287\n",
      "Epoch: 0 | Loss: 0.0396\n",
      "Federated training Epoch [23/200] Val Loss: 0.0313\n",
      "test performance: [0.03374259211824671, 0.03579871875093612, 0.04099298855417395, 0.036697684752129735, 0.03753238260962886, 0.03588568853622634, 0.03792154845105459]\n",
      "Epoch: 0 | Loss: 0.0251\n",
      "Epoch: 0 | Loss: 0.0384\n",
      "Epoch: 0 | Loss: 0.0435\n",
      "Epoch: 0 | Loss: 0.0315\n",
      "Epoch: 0 | Loss: 0.0428\n",
      "Epoch: 0 | Loss: 0.0348\n",
      "Epoch: 0 | Loss: 0.0364\n",
      "Federated training Epoch [24/200] Val Loss: 0.0311\n",
      "test performance: [0.033557916050521684, 0.035808806626559934, 0.040930672344585806, 0.03653109519844492, 0.03762611130584184, 0.035727357577966294, 0.03792249501606271]\n",
      "Epoch: 0 | Loss: 0.0303\n",
      "Epoch: 0 | Loss: 0.0267\n",
      "Epoch: 0 | Loss: 0.0345\n",
      "Epoch: 0 | Loss: 0.0383\n",
      "Epoch: 0 | Loss: 0.0358\n",
      "Epoch: 0 | Loss: 0.0310\n",
      "Epoch: 0 | Loss: 0.0526\n",
      "Federated training Epoch [25/200] Val Loss: 0.0311\n",
      "test performance: [0.033577811617200096, 0.035600580772614646, 0.04080136917684585, 0.03653420075416973, 0.03738402612252186, 0.03570183138171696, 0.03777760862092143]\n",
      "Epoch: 0 | Loss: 0.0390\n",
      "Epoch: 0 | Loss: 0.0385\n",
      "Epoch: 0 | Loss: 0.0323\n",
      "Epoch: 0 | Loss: 0.0288\n",
      "Epoch: 0 | Loss: 0.0432\n",
      "Epoch: 0 | Loss: 0.0273\n",
      "Epoch: 0 | Loss: 0.0453\n",
      "Federated training Epoch [26/200] Val Loss: 0.0312\n",
      "test performance: [0.03360237348633372, 0.03554594674313517, 0.040728502670838815, 0.03653396688057237, 0.037304610275497585, 0.03574258256512248, 0.03772090387224437]\n",
      "Epoch: 0 | Loss: 0.0283\n",
      "Epoch: 0 | Loss: 0.0340\n",
      "Epoch: 0 | Loss: 0.0377\n",
      "Epoch: 0 | Loss: 0.0372\n",
      "Epoch: 0 | Loss: 0.0391\n",
      "Epoch: 0 | Loss: 0.0399\n",
      "Epoch: 0 | Loss: 0.0292\n",
      "Federated training Epoch [27/200] Val Loss: 0.0314\n",
      "test performance: [0.033850499090725836, 0.03552996875616173, 0.040807140206484356, 0.03673063593674196, 0.03716567206859895, 0.03594075545847212, 0.037655609535461626]\n",
      "Epoch: 0 | Loss: 0.0278\n",
      "Epoch: 0 | Loss: 0.0339\n",
      "Epoch: 0 | Loss: 0.0350\n",
      "Epoch: 0 | Loss: 0.0318\n",
      "Epoch: 0 | Loss: 0.0546\n",
      "Epoch: 0 | Loss: 0.0344\n",
      "Epoch: 0 | Loss: 0.0312\n",
      "Federated training Epoch [28/200] Val Loss: 0.0309\n",
      "test performance: [0.033347222006764925, 0.0355625813061448, 0.04071924253047941, 0.03627720192929551, 0.03749344188860324, 0.03547210062292647, 0.03779486397102679]\n",
      "Epoch: 0 | Loss: 0.0343\n",
      "Epoch: 0 | Loss: 0.0425\n",
      "Epoch: 0 | Loss: 0.0458\n",
      "Epoch: 0 | Loss: 0.0263\n",
      "Epoch: 0 | Loss: 0.0470\n",
      "Epoch: 0 | Loss: 0.0408\n",
      "Epoch: 0 | Loss: 0.0305\n",
      "Federated training Epoch [29/200] Val Loss: 0.0310\n",
      "test performance: [0.03348469611078705, 0.03529917036042842, 0.040550323503015386, 0.03637255542618159, 0.03706649142917093, 0.03558723557077042, 0.03752815832179805]\n",
      "Epoch: 0 | Loss: 0.0292\n",
      "Epoch: 0 | Loss: 0.0352\n",
      "Epoch: 0 | Loss: 0.0334\n",
      "Epoch: 0 | Loss: 0.0375\n",
      "Epoch: 0 | Loss: 0.0273\n",
      "Epoch: 0 | Loss: 0.0283\n",
      "Epoch: 0 | Loss: 0.0401\n",
      "Federated training Epoch [30/200] Val Loss: 0.0308\n",
      "test performance: [0.03327737389170654, 0.0353353936863068, 0.04052950333395641, 0.03623043787538087, 0.03722646929744682, 0.03540522471223384, 0.03763107716246215]\n",
      "Epoch: 0 | Loss: 0.0281\n",
      "Epoch: 0 | Loss: 0.0294\n",
      "Epoch: 0 | Loss: 0.0512\n",
      "Epoch: 0 | Loss: 0.0459\n",
      "Epoch: 0 | Loss: 0.0364\n",
      "Epoch: 0 | Loss: 0.0306\n",
      "Epoch: 0 | Loss: 0.0350\n",
      "Federated training Epoch [31/200] Val Loss: 0.0309\n",
      "test performance: [0.033392121648569015, 0.03519291676300233, 0.04047507789521797, 0.03627339717357943, 0.037012573682435164, 0.03547515271690815, 0.037477347668107244]\n",
      "Epoch: 0 | Loss: 0.0358\n",
      "Epoch: 0 | Loss: 0.0358\n",
      "Epoch: 0 | Loss: 0.0370\n",
      "Epoch: 0 | Loss: 0.0373\n",
      "Epoch: 0 | Loss: 0.0393\n",
      "Epoch: 0 | Loss: 0.0262\n",
      "Epoch: 0 | Loss: 0.0396\n",
      "Federated training Epoch [32/200] Val Loss: 0.0311\n",
      "test performance: [0.033506087930107564, 0.035198862562338784, 0.04042654217233601, 0.03631378438846808, 0.03691662726796245, 0.0355609334871326, 0.037372363795054285]\n",
      "Epoch: 0 | Loss: 0.0255\n",
      "Epoch: 0 | Loss: 0.0251\n",
      "Epoch: 0 | Loss: 0.0410\n",
      "Epoch: 0 | Loss: 0.0337\n",
      "Epoch: 0 | Loss: 0.0411\n",
      "Epoch: 0 | Loss: 0.0340\n",
      "Epoch: 0 | Loss: 0.0427\n",
      "Federated training Epoch [33/200] Val Loss: 0.0312\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.fed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.0883 Val Loss: 0.0707\n",
      "Epoch [2/200] Train Loss: 0.0658 Val Loss: 0.0569\n",
      "Epoch [3/200] Train Loss: 0.0517 Val Loss: 0.0459\n",
      "Epoch [4/200] Train Loss: 0.0447 Val Loss: 0.0411\n",
      "Epoch [5/200] Train Loss: 0.0413 Val Loss: 0.0386\n",
      "Epoch [6/200] Train Loss: 0.0392 Val Loss: 0.0372\n",
      "Epoch [7/200] Train Loss: 0.0378 Val Loss: 0.0372\n",
      "Epoch [8/200] Train Loss: 0.0369 Val Loss: 0.0353\n",
      "Epoch [9/200] Train Loss: 0.0363 Val Loss: 0.0349\n",
      "Epoch [10/200] Train Loss: 0.0355 Val Loss: 0.0338\n",
      "Epoch [11/200] Train Loss: 0.0351 Val Loss: 0.0333\n",
      "Epoch [12/200] Train Loss: 0.0349 Val Loss: 0.0332\n",
      "Epoch [13/200] Train Loss: 0.0343 Val Loss: 0.0336\n",
      "Epoch [14/200] Train Loss: 0.0342 Val Loss: 0.0324\n",
      "Epoch [15/200] Train Loss: 0.0338 Val Loss: 0.0328\n",
      "Epoch [16/200] Train Loss: 0.0336 Val Loss: 0.0339\n",
      "Epoch [17/200] Train Loss: 0.0336 Val Loss: 0.0319\n",
      "Epoch [18/200] Train Loss: 0.0334 Val Loss: 0.0319\n",
      "Epoch [19/200] Train Loss: 0.0332 Val Loss: 0.0320\n",
      "Epoch [20/200] Train Loss: 0.0329 Val Loss: 0.0315\n",
      "Epoch [21/200] Train Loss: 0.0327 Val Loss: 0.0316\n",
      "Epoch [22/200] Train Loss: 0.0325 Val Loss: 0.0325\n",
      "Epoch [23/200] Train Loss: 0.0328 Val Loss: 0.0319\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.central_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch Local Training!\n",
      "Epoch [1/200] Train Loss: 0.0856 Val Loss: 0.0683\n",
      "Epoch [2/200] Train Loss: 0.0610 Val Loss: 0.0506\n",
      "Epoch [3/200] Train Loss: 0.0481 Val Loss: 0.0432\n",
      "Epoch [4/200] Train Loss: 0.0423 Val Loss: 0.0419\n",
      "Epoch [5/200] Train Loss: 0.0395 Val Loss: 0.0371\n",
      "Epoch [6/200] Train Loss: 0.0379 Val Loss: 0.0357\n",
      "Epoch [7/200] Train Loss: 0.0368 Val Loss: 0.0349\n",
      "Epoch [8/200] Train Loss: 0.0359 Val Loss: 0.0341\n",
      "Epoch [9/200] Train Loss: 0.0353 Val Loss: 0.0348\n",
      "Epoch [10/200] Train Loss: 0.0351 Val Loss: 0.0339\n",
      "Epoch [11/200] Train Loss: 0.0346 Val Loss: 0.0327\n",
      "Epoch [12/200] Train Loss: 0.0341 Val Loss: 0.0326\n",
      "Epoch [13/200] Train Loss: 0.0340 Val Loss: 0.0325\n",
      "Epoch [14/200] Train Loss: 0.0340 Val Loss: 0.0326\n",
      "Epoch [15/200] Train Loss: 0.0338 Val Loss: 0.0323\n",
      "Epoch [16/200] Train Loss: 0.0333 Val Loss: 0.0323\n",
      "Epoch [17/200] Train Loss: 0.0331 Val Loss: 0.0337\n",
      "Epoch [18/200] Train Loss: 0.0331 Val Loss: 0.0319\n",
      "Epoch [19/200] Train Loss: 0.0329 Val Loss: 0.0332\n",
      "Epoch [20/200] Train Loss: 0.0328 Val Loss: 0.0311\n",
      "Epoch [21/200] Train Loss: 0.0326 Val Loss: 0.0314\n",
      "Epoch [22/200] Train Loss: 0.0325 Val Loss: 0.0310\n",
      "Epoch [23/200] Train Loss: 0.0325 Val Loss: 0.0313\n",
      "Epoch [24/200] Train Loss: 0.0325 Val Loss: 0.0310\n",
      "Epoch [25/200] Train Loss: 0.0322 Val Loss: 0.0312\n",
      "Epoch [26/200] Train Loss: 0.0325 Val Loss: 0.0323\n",
      "Epoch [27/200] Train Loss: 0.0322 Val Loss: 0.0308\n",
      "Epoch [28/200] Train Loss: 0.0321 Val Loss: 0.0313\n",
      "Epoch [29/200] Train Loss: 0.0320 Val Loss: 0.0306\n",
      "Epoch [30/200] Train Loss: 0.0321 Val Loss: 0.0306\n",
      "Epoch [31/200] Train Loss: 0.0319 Val Loss: 0.0310\n",
      "Epoch [32/200] Train Loss: 0.0319 Val Loss: 0.0306\n",
      "Epoch [33/200] Train Loss: 0.0320 Val Loss: 0.0307\n",
      "Epoch [34/200] Train Loss: 0.0318 Val Loss: 0.0319\n",
      "Epoch [35/200] Train Loss: 0.0320 Val Loss: 0.0323\n",
      "Epoch [36/200] Train Loss: 0.0318 Val Loss: 0.0310\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0891 Val Loss: 0.0745\n",
      "Epoch [2/200] Train Loss: 0.0650 Val Loss: 0.0564\n",
      "Epoch [3/200] Train Loss: 0.0519 Val Loss: 0.0474\n",
      "Epoch [4/200] Train Loss: 0.0458 Val Loss: 0.0430\n",
      "Epoch [5/200] Train Loss: 0.0421 Val Loss: 0.0413\n",
      "Epoch [6/200] Train Loss: 0.0402 Val Loss: 0.0388\n",
      "Epoch [7/200] Train Loss: 0.0386 Val Loss: 0.0374\n",
      "Epoch [8/200] Train Loss: 0.0376 Val Loss: 0.0379\n",
      "Epoch [9/200] Train Loss: 0.0366 Val Loss: 0.0358\n",
      "Epoch [10/200] Train Loss: 0.0363 Val Loss: 0.0354\n",
      "Epoch [11/200] Train Loss: 0.0357 Val Loss: 0.0347\n",
      "Epoch [12/200] Train Loss: 0.0354 Val Loss: 0.0353\n",
      "Epoch [13/200] Train Loss: 0.0349 Val Loss: 0.0342\n",
      "Epoch [14/200] Train Loss: 0.0344 Val Loss: 0.0341\n",
      "Epoch [15/200] Train Loss: 0.0343 Val Loss: 0.0337\n",
      "Epoch [16/200] Train Loss: 0.0342 Val Loss: 0.0335\n",
      "Epoch [17/200] Train Loss: 0.0338 Val Loss: 0.0334\n",
      "Epoch [18/200] Train Loss: 0.0337 Val Loss: 0.0334\n",
      "Epoch [19/200] Train Loss: 0.0335 Val Loss: 0.0331\n",
      "Epoch [20/200] Train Loss: 0.0333 Val Loss: 0.0329\n",
      "Epoch [21/200] Train Loss: 0.0333 Val Loss: 0.0326\n",
      "Epoch [22/200] Train Loss: 0.0331 Val Loss: 0.0326\n",
      "Epoch [23/200] Train Loss: 0.0330 Val Loss: 0.0324\n",
      "Epoch [24/200] Train Loss: 0.0330 Val Loss: 0.0324\n",
      "Epoch [25/200] Train Loss: 0.0329 Val Loss: 0.0341\n",
      "Epoch [26/200] Train Loss: 0.0328 Val Loss: 0.0323\n",
      "Epoch [27/200] Train Loss: 0.0326 Val Loss: 0.0322\n",
      "Epoch [28/200] Train Loss: 0.0326 Val Loss: 0.0323\n",
      "Epoch [29/200] Train Loss: 0.0324 Val Loss: 0.0320\n",
      "Epoch [30/200] Train Loss: 0.0323 Val Loss: 0.0319\n",
      "Epoch [31/200] Train Loss: 0.0323 Val Loss: 0.0321\n",
      "Epoch [32/200] Train Loss: 0.0324 Val Loss: 0.0318\n",
      "Epoch [33/200] Train Loss: 0.0325 Val Loss: 0.0322\n",
      "Epoch [34/200] Train Loss: 0.0323 Val Loss: 0.0317\n",
      "Epoch [35/200] Train Loss: 0.0323 Val Loss: 0.0324\n",
      "Epoch [36/200] Train Loss: 0.0321 Val Loss: 0.0318\n",
      "Epoch [37/200] Train Loss: 0.0320 Val Loss: 0.0317\n",
      "Epoch [38/200] Train Loss: 0.0321 Val Loss: 0.0319\n",
      "Epoch [39/200] Train Loss: 0.0318 Val Loss: 0.0318\n",
      "Epoch [40/200] Train Loss: 0.0320 Val Loss: 0.0315\n",
      "Epoch [41/200] Train Loss: 0.0318 Val Loss: 0.0315\n",
      "Epoch [42/200] Train Loss: 0.0318 Val Loss: 0.0318\n",
      "Epoch [43/200] Train Loss: 0.0318 Val Loss: 0.0314\n",
      "Epoch [44/200] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [45/200] Train Loss: 0.0319 Val Loss: 0.0314\n",
      "Epoch [46/200] Train Loss: 0.0317 Val Loss: 0.0322\n",
      "Epoch [47/200] Train Loss: 0.0315 Val Loss: 0.0322\n",
      "Epoch [48/200] Train Loss: 0.0316 Val Loss: 0.0316\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1144 Val Loss: 0.0908\n",
      "Epoch [2/200] Train Loss: 0.0797 Val Loss: 0.0664\n",
      "Epoch [3/200] Train Loss: 0.0604 Val Loss: 0.0548\n",
      "Epoch [4/200] Train Loss: 0.0534 Val Loss: 0.0502\n",
      "Epoch [5/200] Train Loss: 0.0502 Val Loss: 0.0485\n",
      "Epoch [6/200] Train Loss: 0.0485 Val Loss: 0.0476\n",
      "Epoch [7/200] Train Loss: 0.0472 Val Loss: 0.0449\n",
      "Epoch [8/200] Train Loss: 0.0458 Val Loss: 0.0439\n",
      "Epoch [9/200] Train Loss: 0.0452 Val Loss: 0.0438\n",
      "Epoch [10/200] Train Loss: 0.0445 Val Loss: 0.0428\n",
      "Epoch [11/200] Train Loss: 0.0439 Val Loss: 0.0422\n",
      "Epoch [12/200] Train Loss: 0.0437 Val Loss: 0.0417\n",
      "Epoch [13/200] Train Loss: 0.0430 Val Loss: 0.0421\n",
      "Epoch [14/200] Train Loss: 0.0426 Val Loss: 0.0417\n",
      "Epoch [15/200] Train Loss: 0.0423 Val Loss: 0.0414\n",
      "Epoch [16/200] Train Loss: 0.0420 Val Loss: 0.0406\n",
      "Epoch [17/200] Train Loss: 0.0418 Val Loss: 0.0411\n",
      "Epoch [18/200] Train Loss: 0.0423 Val Loss: 0.0405\n",
      "Epoch [19/200] Train Loss: 0.0415 Val Loss: 0.0401\n",
      "Epoch [20/200] Train Loss: 0.0413 Val Loss: 0.0400\n",
      "Epoch [21/200] Train Loss: 0.0411 Val Loss: 0.0413\n",
      "Epoch [22/200] Train Loss: 0.0413 Val Loss: 0.0403\n",
      "Epoch [23/200] Train Loss: 0.0410 Val Loss: 0.0402\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0962 Val Loss: 0.0779\n",
      "Epoch [2/200] Train Loss: 0.0690 Val Loss: 0.0558\n",
      "Epoch [3/200] Train Loss: 0.0539 Val Loss: 0.0474\n",
      "Epoch [4/200] Train Loss: 0.0481 Val Loss: 0.0437\n",
      "Epoch [5/200] Train Loss: 0.0448 Val Loss: 0.0414\n",
      "Epoch [6/200] Train Loss: 0.0428 Val Loss: 0.0412\n",
      "Epoch [7/200] Train Loss: 0.0418 Val Loss: 0.0411\n",
      "Epoch [8/200] Train Loss: 0.0412 Val Loss: 0.0384\n",
      "Epoch [9/200] Train Loss: 0.0399 Val Loss: 0.0380\n",
      "Epoch [10/200] Train Loss: 0.0392 Val Loss: 0.0374\n",
      "Epoch [11/200] Train Loss: 0.0388 Val Loss: 0.0372\n",
      "Epoch [12/200] Train Loss: 0.0384 Val Loss: 0.0374\n",
      "Epoch [13/200] Train Loss: 0.0380 Val Loss: 0.0377\n",
      "Epoch [14/200] Train Loss: 0.0378 Val Loss: 0.0362\n",
      "Epoch [15/200] Train Loss: 0.0375 Val Loss: 0.0360\n",
      "Epoch [16/200] Train Loss: 0.0375 Val Loss: 0.0364\n",
      "Epoch [17/200] Train Loss: 0.0369 Val Loss: 0.0357\n",
      "Epoch [18/200] Train Loss: 0.0369 Val Loss: 0.0356\n",
      "Epoch [19/200] Train Loss: 0.0368 Val Loss: 0.0368\n",
      "Epoch [20/200] Train Loss: 0.0367 Val Loss: 0.0360\n",
      "Epoch [21/200] Train Loss: 0.0366 Val Loss: 0.0361\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1004 Val Loss: 0.0799\n",
      "Epoch [2/200] Train Loss: 0.0730 Val Loss: 0.0615\n",
      "Epoch [3/200] Train Loss: 0.0586 Val Loss: 0.0530\n",
      "Epoch [4/200] Train Loss: 0.0523 Val Loss: 0.0479\n",
      "Epoch [5/200] Train Loss: 0.0485 Val Loss: 0.0447\n",
      "Epoch [6/200] Train Loss: 0.0460 Val Loss: 0.0425\n",
      "Epoch [7/200] Train Loss: 0.0442 Val Loss: 0.0411\n",
      "Epoch [8/200] Train Loss: 0.0431 Val Loss: 0.0421\n",
      "Epoch [9/200] Train Loss: 0.0422 Val Loss: 0.0394\n",
      "Epoch [10/200] Train Loss: 0.0414 Val Loss: 0.0390\n",
      "Epoch [11/200] Train Loss: 0.0410 Val Loss: 0.0388\n",
      "Epoch [12/200] Train Loss: 0.0405 Val Loss: 0.0381\n",
      "Epoch [13/200] Train Loss: 0.0402 Val Loss: 0.0384\n",
      "Epoch [14/200] Train Loss: 0.0397 Val Loss: 0.0378\n",
      "Epoch [15/200] Train Loss: 0.0398 Val Loss: 0.0375\n",
      "Epoch [16/200] Train Loss: 0.0394 Val Loss: 0.0376\n",
      "Epoch [17/200] Train Loss: 0.0390 Val Loss: 0.0373\n",
      "Epoch [18/200] Train Loss: 0.0388 Val Loss: 0.0372\n",
      "Epoch [19/200] Train Loss: 0.0388 Val Loss: 0.0370\n",
      "Epoch [20/200] Train Loss: 0.0388 Val Loss: 0.0368\n",
      "Epoch [21/200] Train Loss: 0.0384 Val Loss: 0.0367\n",
      "Epoch [22/200] Train Loss: 0.0383 Val Loss: 0.0367\n",
      "Epoch [23/200] Train Loss: 0.0382 Val Loss: 0.0371\n",
      "Epoch [24/200] Train Loss: 0.0380 Val Loss: 0.0368\n",
      "Epoch [25/200] Train Loss: 0.0379 Val Loss: 0.0371\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0947 Val Loss: 0.0798\n",
      "Epoch [2/200] Train Loss: 0.0719 Val Loss: 0.0602\n",
      "Epoch [3/200] Train Loss: 0.0552 Val Loss: 0.0482\n",
      "Epoch [4/200] Train Loss: 0.0479 Val Loss: 0.0434\n",
      "Epoch [5/200] Train Loss: 0.0444 Val Loss: 0.0408\n",
      "Epoch [6/200] Train Loss: 0.0419 Val Loss: 0.0392\n",
      "Epoch [7/200] Train Loss: 0.0404 Val Loss: 0.0382\n",
      "Epoch [8/200] Train Loss: 0.0394 Val Loss: 0.0379\n",
      "Epoch [9/200] Train Loss: 0.0387 Val Loss: 0.0369\n",
      "Epoch [10/200] Train Loss: 0.0380 Val Loss: 0.0365\n",
      "Epoch [11/200] Train Loss: 0.0375 Val Loss: 0.0360\n",
      "Epoch [12/200] Train Loss: 0.0372 Val Loss: 0.0359\n",
      "Epoch [13/200] Train Loss: 0.0368 Val Loss: 0.0355\n",
      "Epoch [14/200] Train Loss: 0.0366 Val Loss: 0.0352\n",
      "Epoch [15/200] Train Loss: 0.0362 Val Loss: 0.0351\n",
      "Epoch [16/200] Train Loss: 0.0362 Val Loss: 0.0360\n",
      "Epoch [17/200] Train Loss: 0.0360 Val Loss: 0.0348\n",
      "Epoch [18/200] Train Loss: 0.0357 Val Loss: 0.0355\n",
      "Epoch [19/200] Train Loss: 0.0357 Val Loss: 0.0346\n",
      "Epoch [20/200] Train Loss: 0.0355 Val Loss: 0.0352\n",
      "Epoch [21/200] Train Loss: 0.0353 Val Loss: 0.0345\n",
      "Epoch [22/200] Train Loss: 0.0353 Val Loss: 0.0344\n",
      "Epoch [23/200] Train Loss: 0.0351 Val Loss: 0.0346\n",
      "Epoch [24/200] Train Loss: 0.0351 Val Loss: 0.0351\n",
      "Epoch [25/200] Train Loss: 0.0351 Val Loss: 0.0345\n",
      "Epoch [26/200] Train Loss: 0.0349 Val Loss: 0.0341\n",
      "Epoch [27/200] Train Loss: 0.0349 Val Loss: 0.0342\n",
      "Epoch [28/200] Train Loss: 0.0350 Val Loss: 0.0341\n",
      "Epoch [29/200] Train Loss: 0.0348 Val Loss: 0.0346\n",
      "Epoch [30/200] Train Loss: 0.0346 Val Loss: 0.0339\n",
      "Epoch [31/200] Train Loss: 0.0346 Val Loss: 0.0346\n",
      "Epoch [32/200] Train Loss: 0.0346 Val Loss: 0.0373\n",
      "Epoch [33/200] Train Loss: 0.0344 Val Loss: 0.0339\n",
      "Epoch [34/200] Train Loss: 0.0345 Val Loss: 0.0344\n",
      "Epoch [35/200] Train Loss: 0.0345 Val Loss: 0.0338\n",
      "Epoch [36/200] Train Loss: 0.0345 Val Loss: 0.0338\n",
      "Epoch [37/200] Train Loss: 0.0343 Val Loss: 0.0339\n",
      "Epoch [38/200] Train Loss: 0.0345 Val Loss: 0.0340\n",
      "Epoch [39/200] Train Loss: 0.0341 Val Loss: 0.0339\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1043 Val Loss: 0.0826\n",
      "Epoch [2/200] Train Loss: 0.0729 Val Loss: 0.0601\n",
      "Epoch [3/200] Train Loss: 0.0563 Val Loss: 0.0522\n",
      "Epoch [4/200] Train Loss: 0.0499 Val Loss: 0.0463\n",
      "Epoch [5/200] Train Loss: 0.0464 Val Loss: 0.0444\n",
      "Epoch [6/200] Train Loss: 0.0446 Val Loss: 0.0428\n",
      "Epoch [7/200] Train Loss: 0.0432 Val Loss: 0.0422\n",
      "Epoch [8/200] Train Loss: 0.0422 Val Loss: 0.0411\n",
      "Epoch [9/200] Train Loss: 0.0414 Val Loss: 0.0406\n",
      "Epoch [10/200] Train Loss: 0.0410 Val Loss: 0.0402\n",
      "Epoch [11/200] Train Loss: 0.0404 Val Loss: 0.0403\n",
      "Epoch [12/200] Train Loss: 0.0403 Val Loss: 0.0391\n",
      "Epoch [13/200] Train Loss: 0.0397 Val Loss: 0.0396\n",
      "Epoch [14/200] Train Loss: 0.0398 Val Loss: 0.0393\n",
      "Epoch [15/200] Train Loss: 0.0392 Val Loss: 0.0395\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "server.local_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0318 Val Loss: 0.0306\n",
      "Epoch [2/30] Train Loss: 0.0320 Val Loss: 0.0306\n",
      "Epoch [3/30] Train Loss: 0.0319 Val Loss: 0.0306\n",
      "Epoch [4/30] Train Loss: 0.0319 Val Loss: 0.0305\n",
      "Epoch [5/30] Train Loss: 0.0318 Val Loss: 0.0304\n",
      "Epoch [6/30] Train Loss: 0.0318 Val Loss: 0.0306\n",
      "Epoch [7/30] Train Loss: 0.0318 Val Loss: 0.0304\n",
      "Epoch [8/30] Train Loss: 0.0318 Val Loss: 0.0305\n",
      "Epoch [9/30] Train Loss: 0.0318 Val Loss: 0.0304\n",
      "Epoch [10/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [11/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [12/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [13/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [14/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [15/30] Train Loss: 0.0317 Val Loss: 0.0305\n",
      "Epoch [16/30] Train Loss: 0.0317 Val Loss: 0.0303\n",
      "Epoch [17/30] Train Loss: 0.0317 Val Loss: 0.0304\n",
      "Epoch [18/30] Train Loss: 0.0317 Val Loss: 0.0303\n",
      "Epoch [19/30] Train Loss: 0.0317 Val Loss: 0.0303\n",
      "Epoch [20/30] Train Loss: 0.0316 Val Loss: 0.0304\n",
      "Epoch [21/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [22/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [23/30] Train Loss: 0.0317 Val Loss: 0.0303\n",
      "Epoch [24/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [25/30] Train Loss: 0.0316 Val Loss: 0.0304\n",
      "Epoch [26/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [27/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [28/30] Train Loss: 0.0315 Val Loss: 0.0303\n",
      "Epoch [29/30] Train Loss: 0.0316 Val Loss: 0.0303\n",
      "Epoch [30/30] Train Loss: 0.0315 Val Loss: 0.0304\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0324 Val Loss: 0.0321\n",
      "Epoch [2/30] Train Loss: 0.0324 Val Loss: 0.0319\n",
      "Epoch [3/30] Train Loss: 0.0323 Val Loss: 0.0319\n",
      "Epoch [4/30] Train Loss: 0.0323 Val Loss: 0.0320\n",
      "Epoch [5/30] Train Loss: 0.0323 Val Loss: 0.0318\n",
      "Epoch [6/30] Train Loss: 0.0322 Val Loss: 0.0318\n",
      "Epoch [7/30] Train Loss: 0.0322 Val Loss: 0.0317\n",
      "Epoch [8/30] Train Loss: 0.0322 Val Loss: 0.0317\n",
      "Epoch [9/30] Train Loss: 0.0321 Val Loss: 0.0317\n",
      "Epoch [10/30] Train Loss: 0.0321 Val Loss: 0.0317\n",
      "Epoch [11/30] Train Loss: 0.0321 Val Loss: 0.0317\n",
      "Epoch [12/30] Train Loss: 0.0321 Val Loss: 0.0317\n",
      "Epoch [13/30] Train Loss: 0.0321 Val Loss: 0.0317\n",
      "Epoch [14/30] Train Loss: 0.0320 Val Loss: 0.0316\n",
      "Epoch [15/30] Train Loss: 0.0320 Val Loss: 0.0317\n",
      "Epoch [16/30] Train Loss: 0.0320 Val Loss: 0.0318\n",
      "Epoch [17/30] Train Loss: 0.0320 Val Loss: 0.0317\n",
      "Epoch [18/30] Train Loss: 0.0320 Val Loss: 0.0316\n",
      "Epoch [19/30] Train Loss: 0.0320 Val Loss: 0.0316\n",
      "Epoch [20/30] Train Loss: 0.0320 Val Loss: 0.0318\n",
      "Epoch [21/30] Train Loss: 0.0320 Val Loss: 0.0315\n",
      "Epoch [22/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [23/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [24/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [25/30] Train Loss: 0.0319 Val Loss: 0.0317\n",
      "Epoch [26/30] Train Loss: 0.0319 Val Loss: 0.0316\n",
      "Epoch [27/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [28/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "Epoch [29/30] Train Loss: 0.0318 Val Loss: 0.0317\n",
      "Epoch [30/30] Train Loss: 0.0319 Val Loss: 0.0315\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0401 Val Loss: 0.0393\n",
      "Epoch [2/30] Train Loss: 0.0402 Val Loss: 0.0392\n",
      "Epoch [3/30] Train Loss: 0.0401 Val Loss: 0.0394\n",
      "Epoch [4/30] Train Loss: 0.0401 Val Loss: 0.0393\n",
      "Epoch [5/30] Train Loss: 0.0401 Val Loss: 0.0393\n",
      "Epoch [6/30] Train Loss: 0.0400 Val Loss: 0.0392\n",
      "Epoch [7/30] Train Loss: 0.0400 Val Loss: 0.0392\n",
      "Epoch [8/30] Train Loss: 0.0400 Val Loss: 0.0391\n",
      "Epoch [9/30] Train Loss: 0.0400 Val Loss: 0.0392\n",
      "Epoch [10/30] Train Loss: 0.0400 Val Loss: 0.0392\n",
      "Epoch [11/30] Train Loss: 0.0399 Val Loss: 0.0391\n",
      "Epoch [12/30] Train Loss: 0.0399 Val Loss: 0.0391\n",
      "Epoch [13/30] Train Loss: 0.0399 Val Loss: 0.0391\n",
      "Epoch [14/30] Train Loss: 0.0399 Val Loss: 0.0392\n",
      "Epoch [15/30] Train Loss: 0.0399 Val Loss: 0.0393\n",
      "Epoch [16/30] Train Loss: 0.0399 Val Loss: 0.0392\n",
      "Epoch [17/30] Train Loss: 0.0398 Val Loss: 0.0391\n",
      "Epoch [18/30] Train Loss: 0.0398 Val Loss: 0.0390\n",
      "Epoch [19/30] Train Loss: 0.0398 Val Loss: 0.0390\n",
      "Epoch [20/30] Train Loss: 0.0398 Val Loss: 0.0391\n",
      "Epoch [21/30] Train Loss: 0.0398 Val Loss: 0.0390\n",
      "Epoch [22/30] Train Loss: 0.0398 Val Loss: 0.0390\n",
      "Epoch [23/30] Train Loss: 0.0398 Val Loss: 0.0391\n",
      "Epoch [24/30] Train Loss: 0.0397 Val Loss: 0.0390\n",
      "Epoch [25/30] Train Loss: 0.0398 Val Loss: 0.0390\n",
      "Epoch [26/30] Train Loss: 0.0397 Val Loss: 0.0390\n",
      "Epoch [27/30] Train Loss: 0.0397 Val Loss: 0.0390\n",
      "Epoch [28/30] Train Loss: 0.0397 Val Loss: 0.0390\n",
      "Epoch [29/30] Train Loss: 0.0397 Val Loss: 0.0390\n",
      "Epoch [30/30] Train Loss: 0.0397 Val Loss: 0.0391\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0351 Val Loss: 0.0342\n",
      "Epoch [2/30] Train Loss: 0.0352 Val Loss: 0.0342\n",
      "Epoch [3/30] Train Loss: 0.0352 Val Loss: 0.0341\n",
      "Epoch [4/30] Train Loss: 0.0351 Val Loss: 0.0341\n",
      "Epoch [5/30] Train Loss: 0.0351 Val Loss: 0.0341\n",
      "Epoch [6/30] Train Loss: 0.0351 Val Loss: 0.0345\n",
      "Epoch [7/30] Train Loss: 0.0351 Val Loss: 0.0341\n",
      "Epoch [8/30] Train Loss: 0.0351 Val Loss: 0.0341\n",
      "Epoch [9/30] Train Loss: 0.0351 Val Loss: 0.0341\n",
      "Epoch [10/30] Train Loss: 0.0351 Val Loss: 0.0342\n",
      "Epoch [11/30] Train Loss: 0.0351 Val Loss: 0.0343\n",
      "Epoch [12/30] Train Loss: 0.0350 Val Loss: 0.0340\n",
      "Epoch [13/30] Train Loss: 0.0350 Val Loss: 0.0341\n",
      "Epoch [14/30] Train Loss: 0.0350 Val Loss: 0.0341\n",
      "Epoch [15/30] Train Loss: 0.0350 Val Loss: 0.0340\n",
      "Epoch [16/30] Train Loss: 0.0350 Val Loss: 0.0341\n",
      "Epoch [17/30] Train Loss: 0.0350 Val Loss: 0.0340\n",
      "Epoch [18/30] Train Loss: 0.0350 Val Loss: 0.0340\n",
      "Epoch [19/30] Train Loss: 0.0350 Val Loss: 0.0344\n",
      "Epoch [20/30] Train Loss: 0.0350 Val Loss: 0.0341\n",
      "Epoch [21/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [22/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [23/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [24/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [25/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [26/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [27/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [28/30] Train Loss: 0.0349 Val Loss: 0.0340\n",
      "Epoch [29/30] Train Loss: 0.0349 Val Loss: 0.0341\n",
      "Epoch [30/30] Train Loss: 0.0349 Val Loss: 0.0339\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0375 Val Loss: 0.0362\n",
      "Epoch [2/30] Train Loss: 0.0376 Val Loss: 0.0361\n",
      "Epoch [3/30] Train Loss: 0.0376 Val Loss: 0.0361\n",
      "Epoch [4/30] Train Loss: 0.0375 Val Loss: 0.0361\n",
      "Epoch [5/30] Train Loss: 0.0375 Val Loss: 0.0361\n",
      "Epoch [6/30] Train Loss: 0.0376 Val Loss: 0.0360\n",
      "Epoch [7/30] Train Loss: 0.0375 Val Loss: 0.0360\n",
      "Epoch [8/30] Train Loss: 0.0375 Val Loss: 0.0362\n",
      "Epoch [9/30] Train Loss: 0.0374 Val Loss: 0.0362\n",
      "Epoch [10/30] Train Loss: 0.0374 Val Loss: 0.0360\n",
      "Epoch [11/30] Train Loss: 0.0374 Val Loss: 0.0360\n",
      "Epoch [12/30] Train Loss: 0.0374 Val Loss: 0.0362\n",
      "Epoch [13/30] Train Loss: 0.0374 Val Loss: 0.0360\n",
      "Epoch [14/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [15/30] Train Loss: 0.0373 Val Loss: 0.0360\n",
      "Epoch [16/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [17/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [18/30] Train Loss: 0.0373 Val Loss: 0.0360\n",
      "Epoch [19/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [20/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [21/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [22/30] Train Loss: 0.0372 Val Loss: 0.0359\n",
      "Epoch [23/30] Train Loss: 0.0372 Val Loss: 0.0360\n",
      "Epoch [24/30] Train Loss: 0.0372 Val Loss: 0.0359\n",
      "Epoch [25/30] Train Loss: 0.0373 Val Loss: 0.0359\n",
      "Epoch [26/30] Train Loss: 0.0372 Val Loss: 0.0360\n",
      "Epoch [27/30] Train Loss: 0.0372 Val Loss: 0.0359\n",
      "Epoch [28/30] Train Loss: 0.0372 Val Loss: 0.0361\n",
      "Epoch [29/30] Train Loss: 0.0371 Val Loss: 0.0358\n",
      "Epoch [30/30] Train Loss: 0.0372 Val Loss: 0.0360\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0341 Val Loss: 0.0335\n",
      "Epoch [2/30] Train Loss: 0.0342 Val Loss: 0.0334\n",
      "Epoch [3/30] Train Loss: 0.0342 Val Loss: 0.0336\n",
      "Epoch [4/30] Train Loss: 0.0342 Val Loss: 0.0335\n",
      "Epoch [5/30] Train Loss: 0.0342 Val Loss: 0.0334\n",
      "Epoch [6/30] Train Loss: 0.0342 Val Loss: 0.0335\n",
      "Epoch [7/30] Train Loss: 0.0341 Val Loss: 0.0335\n",
      "Epoch [8/30] Train Loss: 0.0341 Val Loss: 0.0336\n",
      "Epoch [9/30] Train Loss: 0.0342 Val Loss: 0.0334\n",
      "Epoch [10/30] Train Loss: 0.0342 Val Loss: 0.0335\n",
      "Epoch [11/30] Train Loss: 0.0341 Val Loss: 0.0334\n",
      "Epoch [12/30] Train Loss: 0.0341 Val Loss: 0.0334\n",
      "Epoch [13/30] Train Loss: 0.0341 Val Loss: 0.0334\n",
      "Epoch [14/30] Train Loss: 0.0341 Val Loss: 0.0334\n",
      "Epoch [15/30] Train Loss: 0.0340 Val Loss: 0.0335\n",
      "Epoch [16/30] Train Loss: 0.0340 Val Loss: 0.0336\n",
      "Epoch [17/30] Train Loss: 0.0340 Val Loss: 0.0334\n",
      "Epoch [18/30] Train Loss: 0.0340 Val Loss: 0.0334\n",
      "Epoch [19/30] Train Loss: 0.0340 Val Loss: 0.0334\n",
      "Epoch [20/30] Train Loss: 0.0340 Val Loss: 0.0335\n",
      "Epoch [21/30] Train Loss: 0.0340 Val Loss: 0.0333\n",
      "Epoch [22/30] Train Loss: 0.0340 Val Loss: 0.0333\n",
      "Epoch [23/30] Train Loss: 0.0340 Val Loss: 0.0334\n",
      "Epoch [24/30] Train Loss: 0.0339 Val Loss: 0.0334\n",
      "Epoch [25/30] Train Loss: 0.0340 Val Loss: 0.0334\n",
      "Epoch [26/30] Train Loss: 0.0339 Val Loss: 0.0333\n",
      "Epoch [27/30] Train Loss: 0.0339 Val Loss: 0.0335\n",
      "Epoch [28/30] Train Loss: 0.0339 Val Loss: 0.0334\n",
      "Epoch [29/30] Train Loss: 0.0339 Val Loss: 0.0333\n",
      "Epoch [30/30] Train Loss: 0.0339 Val Loss: 0.0333\n",
      "1e-05 0\n",
      "Epoch [1/30] Train Loss: 0.0371 Val Loss: 0.0369\n",
      "Epoch [2/30] Train Loss: 0.0372 Val Loss: 0.0369\n",
      "Epoch [3/30] Train Loss: 0.0372 Val Loss: 0.0369\n",
      "Epoch [4/30] Train Loss: 0.0372 Val Loss: 0.0369\n",
      "Epoch [5/30] Train Loss: 0.0372 Val Loss: 0.0369\n",
      "Epoch [6/30] Train Loss: 0.0371 Val Loss: 0.0369\n",
      "Epoch [7/30] Train Loss: 0.0371 Val Loss: 0.0370\n",
      "Epoch [8/30] Train Loss: 0.0371 Val Loss: 0.0369\n",
      "Epoch [9/30] Train Loss: 0.0371 Val Loss: 0.0369\n",
      "Epoch [10/30] Train Loss: 0.0371 Val Loss: 0.0369\n",
      "Epoch [11/30] Train Loss: 0.0370 Val Loss: 0.0369\n",
      "Epoch [12/30] Train Loss: 0.0371 Val Loss: 0.0370\n",
      "Epoch [13/30] Train Loss: 0.0370 Val Loss: 0.0370\n",
      "Epoch [14/30] Train Loss: 0.0370 Val Loss: 0.0372\n",
      "Epoch [15/30] Train Loss: 0.0370 Val Loss: 0.0370\n",
      "Epoch [16/30] Train Loss: 0.0370 Val Loss: 0.0368\n",
      "Epoch [17/30] Train Loss: 0.0370 Val Loss: 0.0371\n",
      "Epoch [18/30] Train Loss: 0.0370 Val Loss: 0.0369\n",
      "Epoch [19/30] Train Loss: 0.0370 Val Loss: 0.0368\n",
      "Epoch [20/30] Train Loss: 0.0370 Val Loss: 0.0369\n",
      "Epoch [21/30] Train Loss: 0.0370 Val Loss: 0.0370\n",
      "Epoch [22/30] Train Loss: 0.0369 Val Loss: 0.0369\n",
      "Epoch [23/30] Train Loss: 0.0370 Val Loss: 0.0368\n",
      "Epoch [24/30] Train Loss: 0.0369 Val Loss: 0.0368\n",
      "Epoch [25/30] Train Loss: 0.0369 Val Loss: 0.0368\n",
      "Epoch [26/30] Train Loss: 0.0369 Val Loss: 0.0368\n",
      "Epoch [27/30] Train Loss: 0.0369 Val Loss: 0.0372\n",
      "Epoch [28/30] Train Loss: 0.0369 Val Loss: 0.0369\n",
      "Epoch [29/30] Train Loss: 0.0369 Val Loss: 0.0368\n",
      "Epoch [30/30] Train Loss: 0.0368 Val Loss: 0.0368\n",
      "[0.033156270237818156, 0.034703732302335845, 0.040098275313128345, 0.03563182553187115, 0.0366086190126275, 0.0351469776567633, 0.03705518918226741]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_losses=[]\n",
    "local_fine_tune_preds=[]\n",
    "local_fine_tune_models=[]\n",
    "seed_everything(1)\n",
    "for i in range(args_train.number_clients):\n",
    "    local_fine_tune_pred,local_fine_tune_loss,local_fine_tune_model=clients[i].local_fine_tune(fine_tune_epochs=30)\n",
    "    local_fine_tune_losses.append(local_fine_tune_loss)\n",
    "    local_fine_tune_preds.append(local_fine_tune_pred)\n",
    "    local_fine_tune_models.append(local_fine_tune_model)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import utils import plot_prob_result\n",
    "args_temp=copy.deepcopy(args_train)\n",
    "args_temp.dataset_paths='wf7'\n",
    "test_data, test_loader = get_data(args_train,flag='test')\n",
    "actual_y=[]\n",
    "for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "    actual_y.append(seq_y)\n",
    "actual_y = torch.cat([torch.flatten(t) for t in actual_y])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03369925597886721, 0.03519536803591333, 0.040536052285859436, 0.03649028574996819, 0.03685282969732501, 0.035751149217815026, 0.037361839212068954]\n",
      "0.03655525431111674\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)\n",
    "print(np.mean(fed_local_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03371577699706979, 0.03496455265949034, 0.04149482667139948, 0.03731695264705444, 0.03818863684834581, 0.03602887266506887, 0.03983221346933446]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    local_pred,local_loss,local_model=clients[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03443343698187438, 0.03807613101775107, 0.043018698928341884, 0.03778318985531183, 0.040288623243774455, 0.03716161004459, 0.04036670377197331]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    central_pred,central_loss,central_model=server.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03371577699706979, 0.03496455265949034, 0.04149482667139948, 0.03731695264705444, 0.03818863684834581, 0.03602887266506887, 0.03983221346933446]\n",
      "[0.03443343698187438, 0.03807613101775107, 0.043018698928341884, 0.03778318985531183, 0.040288623243774455, 0.03716161004459, 0.04036670377197331]\n",
      "[0.03369925597886721, 0.03519536803591333, 0.040536052285859436, 0.03649028574996819, 0.03685282969732501, 0.035751149217815026, 0.037361839212068954]\n",
      "[0.033156270237818156, 0.034703732302335845, 0.040098275313128345, 0.03563182553187115, 0.0366086190126275, 0.0351469776567633, 0.03705518918226741]\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_losses': local_fine_tune_losses\n",
    "})\n",
    "df.T.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y_lst=[]\n",
    "\n",
    "for i in range(args_train.number_clients):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths='wf'+str(i+1)\n",
    "    test_data, test_loader = get_data(args_temp,flag='test')\n",
    "    actual_y=[]\n",
    "    for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "        actual_y.append(seq_y)\n",
    "    actual_y = torch.cat([torch.flatten(t) for t in actual_y])\n",
    "    actual_y_lst.append(actual_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the server object\n",
    "with open('../result/2/server_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "# Save the clients object\n",
    "with open('../result/2/clients_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save server and clients\n",
    "with open('server.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "with open('clients.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)\n",
    "\n",
    "# Load server and clients\n",
    "with open('server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "with open('clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
