{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Data_loader import Dataset_Custom\n",
    "import argparse\n",
    "import warnings\n",
    "from tools import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_data\n",
    "from Model import ANN\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Server import  Server\n",
    "from Clients import Client\n",
    "from Train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=24)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--ensemble_flag', type=bool, default=True)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model12/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "args_train = parser_train.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "clients=[]\n",
    "for path in tqdm(args_train.dataset_paths):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths=path\n",
    "    clients.append(Client(args_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(args_train,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: [0.15398186090568158, 0.15518323406066797, 0.18186280572761412, 0.1677203120995466, 0.16007199120541957, 0.17050389876614694, 0.17021395301778022]\n",
      "Epoch: 0 | Loss: 0.0856\n",
      "Epoch: 0 | Loss: 0.0827\n",
      "Epoch: 0 | Loss: 0.0859\n",
      "Epoch: 0 | Loss: 0.0919\n",
      "Epoch: 0 | Loss: 0.0810\n",
      "Epoch: 0 | Loss: 0.0917\n",
      "Epoch: 0 | Loss: 0.0967\n",
      "Federated training Epoch [1/200] Val Loss: 0.0783\n",
      "test performance: [0.07958247453892885, 0.08523698648667499, 0.09913526021250307, 0.08969417051093219, 0.09241833662843868, 0.08902055344046796, 0.09390376788908489]\n",
      "Epoch: 0 | Loss: 0.0951\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0900\n",
      "Epoch: 0 | Loss: 0.0821\n",
      "Epoch: 0 | Loss: 0.1112\n",
      "Epoch: 0 | Loss: 0.0947\n",
      "Epoch: 0 | Loss: 0.0816\n",
      "Federated training Epoch [2/200] Val Loss: 0.0778\n",
      "test performance: [0.07946541198618608, 0.08441940621051887, 0.09633762085784787, 0.08913191456398735, 0.09221236619537007, 0.0880411188181949, 0.09237669844639627]\n",
      "Epoch: 0 | Loss: 0.0833\n",
      "Epoch: 0 | Loss: 0.0750\n",
      "Epoch: 0 | Loss: 0.0897\n",
      "Epoch: 0 | Loss: 0.0816\n",
      "Epoch: 0 | Loss: 0.0803\n",
      "Epoch: 0 | Loss: 0.0819\n",
      "Epoch: 0 | Loss: 0.0920\n",
      "Federated training Epoch [3/200] Val Loss: 0.0767\n",
      "test performance: [0.0792041480643292, 0.08444958445552277, 0.09546762738019636, 0.08835777477042316, 0.09208334148032209, 0.08722213923624933, 0.09168336560873136]\n",
      "Epoch: 0 | Loss: 0.0711\n",
      "Epoch: 0 | Loss: 0.0777\n",
      "Epoch: 0 | Loss: 0.0761\n",
      "Epoch: 0 | Loss: 0.1065\n",
      "Epoch: 0 | Loss: 0.1050\n",
      "Epoch: 0 | Loss: 0.0865\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Federated training Epoch [4/200] Val Loss: 0.0755\n",
      "test performance: [0.07903070316637216, 0.08455751486734985, 0.09492207660454594, 0.0881665987153984, 0.09156679897888066, 0.08683256611023864, 0.09103761894041545]\n",
      "Epoch: 0 | Loss: 0.0746\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0989\n",
      "Epoch: 0 | Loss: 0.0895\n",
      "Epoch: 0 | Loss: 0.0879\n",
      "Epoch: 0 | Loss: 0.0805\n",
      "Epoch: 0 | Loss: 0.0955\n",
      "Federated training Epoch [5/200] Val Loss: 0.0748\n",
      "test performance: [0.07878880802388877, 0.08482703413457086, 0.09446871364871932, 0.08801381147071106, 0.0913759759844166, 0.08652174222755106, 0.09065081322029846]\n",
      "Epoch: 0 | Loss: 0.0621\n",
      "Epoch: 0 | Loss: 0.0791\n",
      "Epoch: 0 | Loss: 0.1028\n",
      "Epoch: 0 | Loss: 0.0902\n",
      "Epoch: 0 | Loss: 0.1058\n",
      "Epoch: 0 | Loss: 0.0687\n",
      "Epoch: 0 | Loss: 0.0909\n",
      "Federated training Epoch [6/200] Val Loss: 0.0745\n",
      "test performance: [0.07893271186733491, 0.08492369465019604, 0.0942160518304126, 0.08846027233114798, 0.09083137806658059, 0.08659139237277312, 0.09031166651681678]\n",
      "Epoch: 0 | Loss: 0.0568\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0991\n",
      "Epoch: 0 | Loss: 0.0855\n",
      "Epoch: 0 | Loss: 0.0777\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Epoch: 0 | Loss: 0.0854\n",
      "Federated training Epoch [7/200] Val Loss: 0.0738\n",
      "test performance: [0.07836403801067643, 0.08571974324598296, 0.0941039656276164, 0.08732160114466328, 0.09142096215629414, 0.08582955458494898, 0.09041986668048656]\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0921\n",
      "Epoch: 0 | Loss: 0.0969\n",
      "Epoch: 0 | Loss: 0.0715\n",
      "Epoch: 0 | Loss: 0.0738\n",
      "Epoch: 0 | Loss: 0.1034\n",
      "Federated training Epoch [8/200] Val Loss: 0.0735\n",
      "test performance: [0.07832411580961453, 0.08547156020896891, 0.09385321610798575, 0.08749632149526518, 0.09056976708034946, 0.08574362632448543, 0.0898391355645575]\n",
      "Epoch: 0 | Loss: 0.0724\n",
      "Epoch: 0 | Loss: 0.0832\n",
      "Epoch: 0 | Loss: 0.0929\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0937\n",
      "Epoch: 0 | Loss: 0.0712\n",
      "Epoch: 0 | Loss: 0.0774\n",
      "Federated training Epoch [9/200] Val Loss: 0.0731\n",
      "test performance: [0.07818248825887703, 0.08593870977526658, 0.0938785294555638, 0.08715291864761751, 0.09060914495526111, 0.08541100694198314, 0.0897462000840739]\n",
      "Epoch: 0 | Loss: 0.0754\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Epoch: 0 | Loss: 0.0988\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0896\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Epoch: 0 | Loss: 0.0788\n",
      "Federated training Epoch [10/200] Val Loss: 0.0728\n",
      "test performance: [0.07801913796630625, 0.08599026143959124, 0.09373298052647343, 0.08690854644224252, 0.09026884453447714, 0.08512707480726993, 0.08944094551038252]\n",
      "Epoch: 0 | Loss: 0.0808\n",
      "Epoch: 0 | Loss: 0.0817\n",
      "Epoch: 0 | Loss: 0.0983\n",
      "Epoch: 0 | Loss: 0.0776\n",
      "Epoch: 0 | Loss: 0.0843\n",
      "Epoch: 0 | Loss: 0.0745\n",
      "Epoch: 0 | Loss: 0.0876\n",
      "Federated training Epoch [11/200] Val Loss: 0.0727\n",
      "test performance: [0.07802902680043489, 0.08580251663208824, 0.09377542485113013, 0.08732614561609209, 0.08961514528993875, 0.08528626044217037, 0.08917171539968416]\n",
      "Epoch: 0 | Loss: 0.0739\n",
      "Epoch: 0 | Loss: 0.0908\n",
      "Epoch: 0 | Loss: 0.0928\n",
      "Epoch: 0 | Loss: 0.0891\n",
      "Epoch: 0 | Loss: 0.0883\n",
      "Epoch: 0 | Loss: 0.0882\n",
      "Epoch: 0 | Loss: 0.0798\n",
      "Federated training Epoch [12/200] Val Loss: 0.0725\n",
      "test performance: [0.07800889077711187, 0.08594808301390851, 0.09352044164113803, 0.08731214953458881, 0.08943102464130888, 0.08505501650426894, 0.08876466552076274]\n",
      "Epoch: 0 | Loss: 0.0603\n",
      "Epoch: 0 | Loss: 0.0738\n",
      "Epoch: 0 | Loss: 0.1026\n",
      "Epoch: 0 | Loss: 0.1014\n",
      "Epoch: 0 | Loss: 0.0993\n",
      "Epoch: 0 | Loss: 0.0723\n",
      "Epoch: 0 | Loss: 0.0843\n",
      "Federated training Epoch [13/200] Val Loss: 0.0718\n",
      "test performance: [0.07741860943297817, 0.08689811024559688, 0.09360817189596288, 0.08601412655505007, 0.09000451059067903, 0.08422945771519452, 0.0888857872238102]\n",
      "Epoch: 0 | Loss: 0.0646\n",
      "Epoch: 0 | Loss: 0.0677\n",
      "Epoch: 0 | Loss: 0.1041\n",
      "Epoch: 0 | Loss: 0.0828\n",
      "Epoch: 0 | Loss: 0.0919\n",
      "Epoch: 0 | Loss: 0.0743\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Federated training Epoch [14/200] Val Loss: 0.0714\n",
      "test performance: [0.07712633218871404, 0.08694493636642009, 0.09368823540129073, 0.08574482338018205, 0.08973967340098668, 0.08404826589745201, 0.08872752225868506]\n",
      "Epoch: 0 | Loss: 0.0686\n",
      "Epoch: 0 | Loss: 0.0938\n",
      "Epoch: 0 | Loss: 0.0948\n",
      "Epoch: 0 | Loss: 0.0881\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0845\n",
      "Epoch: 0 | Loss: 0.0728\n",
      "Federated training Epoch [15/200] Val Loss: 0.0715\n",
      "test performance: [0.07732252882512873, 0.08654107588458143, 0.09356443716646874, 0.08650557062754484, 0.08894374687224627, 0.08432836451708045, 0.0883268528579645]\n",
      "Epoch: 0 | Loss: 0.0709\n",
      "Epoch: 0 | Loss: 0.0863\n",
      "Epoch: 0 | Loss: 0.0939\n",
      "Epoch: 0 | Loss: 0.0769\n",
      "Epoch: 0 | Loss: 0.0972\n",
      "Epoch: 0 | Loss: 0.0754\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Federated training Epoch [16/200] Val Loss: 0.0710\n",
      "test performance: [0.0768808186283871, 0.08687142007154962, 0.09356936835364936, 0.0856787784356777, 0.08898668650741855, 0.08380691451977378, 0.08820509622256233]\n",
      "Epoch: 0 | Loss: 0.0740\n",
      "Epoch: 0 | Loss: 0.0761\n",
      "Epoch: 0 | Loss: 0.0865\n",
      "Epoch: 0 | Loss: 0.0733\n",
      "Epoch: 0 | Loss: 0.0810\n",
      "Epoch: 0 | Loss: 0.0869\n",
      "Epoch: 0 | Loss: 0.0822\n",
      "Federated training Epoch [17/200] Val Loss: 0.0705\n",
      "test performance: [0.07663323953492593, 0.08785022578317009, 0.09377500287269892, 0.08489049372760808, 0.08970336866093008, 0.08332512745863363, 0.08848170411760269]\n",
      "Epoch: 0 | Loss: 0.0696\n",
      "Epoch: 0 | Loss: 0.0705\n",
      "Epoch: 0 | Loss: 0.0863\n",
      "Epoch: 0 | Loss: 0.0932\n",
      "Epoch: 0 | Loss: 0.0766\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.1041\n",
      "Federated training Epoch [18/200] Val Loss: 0.0703\n",
      "test performance: [0.07652529695211616, 0.08760958463463882, 0.09337512444551677, 0.08500765276792115, 0.08904659260727771, 0.083162948501947, 0.0879482022575932]\n",
      "Epoch: 0 | Loss: 0.0701\n",
      "Epoch: 0 | Loss: 0.0935\n",
      "Epoch: 0 | Loss: 0.0898\n",
      "Epoch: 0 | Loss: 0.0808\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0759\n",
      "Epoch: 0 | Loss: 0.0803\n",
      "Federated training Epoch [19/200] Val Loss: 0.0701\n",
      "test performance: [0.07627749008011736, 0.08772121232054005, 0.09330822478928795, 0.08456495878835248, 0.08889981889969682, 0.08284527050611908, 0.08777704435617548]\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Epoch: 0 | Loss: 0.0841\n",
      "Epoch: 0 | Loss: 0.0817\n",
      "Epoch: 0 | Loss: 0.0769\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Epoch: 0 | Loss: 0.1008\n",
      "Federated training Epoch [20/200] Val Loss: 0.0700\n",
      "test performance: [0.07615446562126074, 0.08746010414643647, 0.09318290490095746, 0.08480252778438264, 0.08844793520306479, 0.08288594910696354, 0.08758821323105734]\n",
      "Epoch: 0 | Loss: 0.0688\n",
      "Epoch: 0 | Loss: 0.0754\n",
      "Epoch: 0 | Loss: 0.0968\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0791\n",
      "Epoch: 0 | Loss: 0.0778\n",
      "Epoch: 0 | Loss: 0.0931\n",
      "Federated training Epoch [21/200] Val Loss: 0.0697\n",
      "test performance: [0.07599612325429916, 0.08770283055172799, 0.09309956122649042, 0.08445081005051527, 0.08835693466642948, 0.08257398429033283, 0.08735302549927201]\n",
      "Epoch: 0 | Loss: 0.0679\n",
      "Epoch: 0 | Loss: 0.0902\n",
      "Epoch: 0 | Loss: 0.0859\n",
      "Epoch: 0 | Loss: 0.0699\n",
      "Epoch: 0 | Loss: 0.0798\n",
      "Epoch: 0 | Loss: 0.0837\n",
      "Epoch: 0 | Loss: 0.0782\n",
      "Federated training Epoch [22/200] Val Loss: 0.0694\n",
      "test performance: [0.07580299294005109, 0.08813112198489986, 0.09332873646731246, 0.08401843642637338, 0.08858156875286201, 0.08224578187737154, 0.08732955967879867]\n",
      "Epoch: 0 | Loss: 0.0750\n",
      "Epoch: 0 | Loss: 0.0706\n",
      "Epoch: 0 | Loss: 0.1050\n",
      "Epoch: 0 | Loss: 0.0718\n",
      "Epoch: 0 | Loss: 0.0661\n",
      "Epoch: 0 | Loss: 0.0756\n",
      "Epoch: 0 | Loss: 0.0890\n",
      "Federated training Epoch [23/200] Val Loss: 0.0690\n",
      "test performance: [0.07567868093411399, 0.08869253546764998, 0.0935202310856891, 0.08390672900395034, 0.08892565984789232, 0.08223930036980812, 0.08753319306630794]\n",
      "Epoch: 0 | Loss: 0.0638\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0803\n",
      "Epoch: 0 | Loss: 0.0858\n",
      "Epoch: 0 | Loss: 0.0724\n",
      "Epoch: 0 | Loss: 0.0745\n",
      "Epoch: 0 | Loss: 0.0924\n",
      "Federated training Epoch [24/200] Val Loss: 0.0689\n",
      "test performance: [0.07568579884118413, 0.0891338748449128, 0.09348213335830871, 0.08370544541984389, 0.08897437694903514, 0.0819189624274022, 0.08736688921814911]\n",
      "Epoch: 0 | Loss: 0.0610\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0823\n",
      "Epoch: 0 | Loss: 0.1034\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0769\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Federated training Epoch [25/200] Val Loss: 0.0690\n",
      "test performance: [0.07561694438990256, 0.08843712229281664, 0.09301022654526854, 0.08410135113111097, 0.08812298841315182, 0.08188307031427752, 0.08676421151126493]\n",
      "Epoch: 0 | Loss: 0.0720\n",
      "Epoch: 0 | Loss: 0.0658\n",
      "Epoch: 0 | Loss: 0.0919\n",
      "Epoch: 0 | Loss: 0.0716\n",
      "Epoch: 0 | Loss: 0.0868\n",
      "Epoch: 0 | Loss: 0.0857\n",
      "Epoch: 0 | Loss: 0.0709\n",
      "Federated training Epoch [26/200] Val Loss: 0.0686\n",
      "test performance: [0.07535305046412634, 0.0887546543516729, 0.09318631823646696, 0.08376552688901963, 0.08825656610911023, 0.08172205278063066, 0.08688516155752825]\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0898\n",
      "Epoch: 0 | Loss: 0.0861\n",
      "Epoch: 0 | Loss: 0.0947\n",
      "Epoch: 0 | Loss: 0.0754\n",
      "Epoch: 0 | Loss: 0.0862\n",
      "Federated training Epoch [27/200] Val Loss: 0.0686\n",
      "test performance: [0.07518981943187648, 0.08846935516299859, 0.09285197251361527, 0.08372400440785983, 0.08800730426885085, 0.08168881631469073, 0.0867661224576096]\n",
      "Epoch: 0 | Loss: 0.0603\n",
      "Epoch: 0 | Loss: 0.0835\n",
      "Epoch: 0 | Loss: 0.1111\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0903\n",
      "Epoch: 0 | Loss: 0.0887\n",
      "Epoch: 0 | Loss: 0.0799\n",
      "Federated training Epoch [28/200] Val Loss: 0.0681\n",
      "test performance: [0.0748320394131827, 0.0893943401130095, 0.09325665069667444, 0.08273436777191619, 0.08867044776218803, 0.08128743801163892, 0.08712976790760478]\n",
      "Epoch: 0 | Loss: 0.0809\n",
      "Epoch: 0 | Loss: 0.0775\n",
      "Epoch: 0 | Loss: 0.1071\n",
      "Epoch: 0 | Loss: 0.0804\n",
      "Epoch: 0 | Loss: 0.0714\n",
      "Epoch: 0 | Loss: 0.0612\n",
      "Epoch: 0 | Loss: 0.0821\n",
      "Federated training Epoch [29/200] Val Loss: 0.0680\n",
      "test performance: [0.07482374663630577, 0.08926094741853949, 0.09301127554619149, 0.08280060194075516, 0.08837528522955636, 0.08107923246818045, 0.08679492742282478]\n",
      "Epoch: 0 | Loss: 0.0699\n",
      "Epoch: 0 | Loss: 0.0776\n",
      "Epoch: 0 | Loss: 0.0915\n",
      "Epoch: 0 | Loss: 0.0834\n",
      "Epoch: 0 | Loss: 0.0737\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0968\n",
      "Federated training Epoch [30/200] Val Loss: 0.0679\n",
      "test performance: [0.07468400810117999, 0.08936034313926142, 0.09288863171759533, 0.08284551049391292, 0.08837459487712955, 0.08100985129657265, 0.08672856049586648]\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Epoch: 0 | Loss: 0.0679\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0932\n",
      "Epoch: 0 | Loss: 0.0816\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Epoch: 0 | Loss: 0.0806\n",
      "Federated training Epoch [31/200] Val Loss: 0.0680\n",
      "test performance: [0.07470408679075437, 0.08890067678812431, 0.09285070652729027, 0.08305243962788827, 0.08788919063565666, 0.08102653048013987, 0.08630695819140297]\n",
      "Epoch: 0 | Loss: 0.0528\n",
      "Epoch: 0 | Loss: 0.0755\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0718\n",
      "Epoch: 0 | Loss: 0.0831\n",
      "Epoch: 0 | Loss: 0.0682\n",
      "Epoch: 0 | Loss: 0.0768\n",
      "Federated training Epoch [32/200] Val Loss: 0.0676\n",
      "test performance: [0.07456263289978243, 0.08972138449651738, 0.09266604146320526, 0.08247823932858771, 0.088604914777185, 0.08074025367067693, 0.08666239094550479]\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Epoch: 0 | Loss: 0.0817\n",
      "Epoch: 0 | Loss: 0.0918\n",
      "Epoch: 0 | Loss: 0.0750\n",
      "Epoch: 0 | Loss: 0.0790\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0823\n",
      "Federated training Epoch [33/200] Val Loss: 0.0680\n",
      "test performance: [0.07484097172799584, 0.08912110591485892, 0.09243867635624865, 0.08326739907162646, 0.0877782021559877, 0.08090251556610407, 0.08614137296705213]\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Epoch: 0 | Loss: 0.0959\n",
      "Epoch: 0 | Loss: 0.0947\n",
      "Epoch: 0 | Loss: 0.0793\n",
      "Epoch: 0 | Loss: 0.0776\n",
      "Epoch: 0 | Loss: 0.0772\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Federated training Epoch [34/200] Val Loss: 0.0679\n",
      "test performance: [0.07476671691983938, 0.08918756199362751, 0.09264426300786946, 0.0832374424176061, 0.08770437263054391, 0.08084718254075883, 0.08612684217881258]\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0811\n",
      "Epoch: 0 | Loss: 0.0859\n",
      "Epoch: 0 | Loss: 0.0772\n",
      "Epoch: 0 | Loss: 0.0649\n",
      "Epoch: 0 | Loss: 0.0813\n",
      "Federated training Epoch [35/200] Val Loss: 0.0674\n",
      "test performance: [0.07413034605449192, 0.08941857710684815, 0.0925550634577258, 0.08219332749355737, 0.08801025950847423, 0.08052608432018593, 0.08640968686046258]\n",
      "Epoch: 0 | Loss: 0.0663\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Epoch: 0 | Loss: 0.0878\n",
      "Epoch: 0 | Loss: 0.0761\n",
      "Epoch: 0 | Loss: 0.0780\n",
      "Epoch: 0 | Loss: 0.0826\n",
      "Epoch: 0 | Loss: 0.0802\n",
      "Federated training Epoch [36/200] Val Loss: 0.0669\n",
      "test performance: [0.0742066973718266, 0.09047220264599748, 0.09286609855927017, 0.08192021756956022, 0.08893877359693997, 0.08033302965994975, 0.08669394702484755]\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0840\n",
      "Epoch: 0 | Loss: 0.0938\n",
      "Epoch: 0 | Loss: 0.0786\n",
      "Epoch: 0 | Loss: 0.0736\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Federated training Epoch [37/200] Val Loss: 0.0675\n",
      "test performance: [0.07434574513351672, 0.08909356430785297, 0.09232591407705251, 0.0826530466876822, 0.08754278716873633, 0.0805338580824741, 0.0859134958112893]\n",
      "Epoch: 0 | Loss: 0.0774\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0809\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.0746\n",
      "Epoch: 0 | Loss: 0.0642\n",
      "Epoch: 0 | Loss: 0.0796\n",
      "Federated training Epoch [38/200] Val Loss: 0.0670\n",
      "test performance: [0.07415337051736982, 0.08993721670432858, 0.09231608235978916, 0.0819717898633178, 0.08837864083582408, 0.08017648302324831, 0.08624444382698977]\n",
      "Epoch: 0 | Loss: 0.0662\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Epoch: 0 | Loss: 0.0711\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0793\n",
      "Epoch: 0 | Loss: 0.0653\n",
      "Epoch: 0 | Loss: 0.0949\n",
      "Federated training Epoch [39/200] Val Loss: 0.0676\n",
      "test performance: [0.0745319573667972, 0.08906732267406706, 0.09246963383757496, 0.0827580216557604, 0.08743157769770246, 0.08052775096658567, 0.08588165696710348]\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0911\n",
      "Epoch: 0 | Loss: 0.0898\n",
      "Epoch: 0 | Loss: 0.0785\n",
      "Epoch: 0 | Loss: 0.0815\n",
      "Epoch: 0 | Loss: 0.0619\n",
      "Epoch: 0 | Loss: 0.0770\n",
      "Federated training Epoch [40/200] Val Loss: 0.0671\n",
      "test performance: [0.07416304616793377, 0.08954180385407111, 0.09193106226572027, 0.08205197828069125, 0.08804716787313761, 0.08005389721732434, 0.08590050030193508]\n",
      "Epoch: 0 | Loss: 0.0547\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Epoch: 0 | Loss: 0.0886\n",
      "Epoch: 0 | Loss: 0.0840\n",
      "Epoch: 0 | Loss: 0.0792\n",
      "Epoch: 0 | Loss: 0.0677\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Federated training Epoch [41/200] Val Loss: 0.0666\n",
      "test performance: [0.07389030207509864, 0.09035092828259485, 0.09224312721866451, 0.0814944705096622, 0.08901239456991626, 0.07993966196855046, 0.08661214149978062]\n",
      "Epoch: 0 | Loss: 0.0680\n",
      "Epoch: 0 | Loss: 0.0759\n",
      "Epoch: 0 | Loss: 0.0742\n",
      "Epoch: 0 | Loss: 0.0789\n",
      "Epoch: 0 | Loss: 0.0688\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Federated training Epoch [42/200] Val Loss: 0.0661\n",
      "test performance: [0.07373985608605897, 0.09123708801471615, 0.09269127864645768, 0.08118823494711151, 0.09016014135455432, 0.0801237724619369, 0.08742356050300272]\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0686\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0741\n",
      "Epoch: 0 | Loss: 0.0829\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Epoch: 0 | Loss: 0.0719\n",
      "Federated training Epoch [43/200] Val Loss: 0.0670\n",
      "test performance: [0.07402400284597319, 0.08906223859689007, 0.09194843055144565, 0.08214268492764398, 0.08761911336587716, 0.08012736419037189, 0.08587623034182885]\n",
      "Epoch: 0 | Loss: 0.0627\n",
      "Epoch: 0 | Loss: 0.0717\n",
      "Epoch: 0 | Loss: 0.0977\n",
      "Epoch: 0 | Loss: 0.0682\n",
      "Epoch: 0 | Loss: 0.0736\n",
      "Epoch: 0 | Loss: 0.0669\n",
      "Epoch: 0 | Loss: 0.0871\n",
      "Federated training Epoch [44/200] Val Loss: 0.0663\n",
      "test performance: [0.07392438298269902, 0.09047164859837048, 0.09187989166542275, 0.08123410056817205, 0.08915082870485032, 0.07967214824708357, 0.08637138300460495]\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0771\n",
      "Epoch: 0 | Loss: 0.0968\n",
      "Epoch: 0 | Loss: 0.0653\n",
      "Epoch: 0 | Loss: 0.0796\n",
      "Epoch: 0 | Loss: 0.0715\n",
      "Epoch: 0 | Loss: 0.0763\n",
      "Federated training Epoch [45/200] Val Loss: 0.0661\n",
      "test performance: [0.07373659904689005, 0.09032031447205642, 0.09195282853731554, 0.08138011126740746, 0.08912945792640317, 0.07978471817627344, 0.08651409415553694]\n",
      "Epoch: 0 | Loss: 0.0685\n",
      "Epoch: 0 | Loss: 0.0668\n",
      "Epoch: 0 | Loss: 0.0964\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0725\n",
      "Epoch: 0 | Loss: 0.0672\n",
      "Epoch: 0 | Loss: 0.0898\n",
      "Federated training Epoch [46/200] Val Loss: 0.0667\n",
      "test performance: [0.07412515967216803, 0.08939942223823642, 0.09184243698438553, 0.08218602868026659, 0.08780926117699032, 0.0800323562620029, 0.08576742437196104]\n",
      "Epoch: 0 | Loss: 0.0713\n",
      "Epoch: 0 | Loss: 0.0622\n",
      "Epoch: 0 | Loss: 0.0922\n",
      "Epoch: 0 | Loss: 0.0616\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0683\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Federated training Epoch [47/200] Val Loss: 0.0663\n",
      "test performance: [0.07383512725977048, 0.08979468464800348, 0.09133472388023384, 0.08129401749944033, 0.0887017536347043, 0.0795339085161686, 0.08601910546932319]\n",
      "Epoch: 0 | Loss: 0.0597\n",
      "Epoch: 0 | Loss: 0.0700\n",
      "Epoch: 0 | Loss: 0.0868\n",
      "Epoch: 0 | Loss: 0.0605\n",
      "Epoch: 0 | Loss: 0.0785\n",
      "Epoch: 0 | Loss: 0.0621\n",
      "Epoch: 0 | Loss: 0.0841\n",
      "Federated training Epoch [48/200] Val Loss: 0.0661\n",
      "test performance: [0.07377369405880366, 0.08958693056635253, 0.09174765109352462, 0.08130974738463147, 0.0886266072446557, 0.0795038413154344, 0.08605869438448181]\n",
      "Epoch: 0 | Loss: 0.0686\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Epoch: 0 | Loss: 0.0921\n",
      "Epoch: 0 | Loss: 0.0705\n",
      "Epoch: 0 | Loss: 0.0673\n",
      "Epoch: 0 | Loss: 0.0719\n",
      "Epoch: 0 | Loss: 0.0843\n",
      "Federated training Epoch [49/200] Val Loss: 0.0664\n",
      "test performance: [0.07369376044108035, 0.08889063406888753, 0.09140535229689455, 0.0817339198970019, 0.08778162932099953, 0.07972908950066321, 0.08578959976208128]\n",
      "Epoch: 0 | Loss: 0.0611\n",
      "Epoch: 0 | Loss: 0.0668\n",
      "Epoch: 0 | Loss: 0.0791\n",
      "Epoch: 0 | Loss: 0.0625\n",
      "Epoch: 0 | Loss: 0.0770\n",
      "Epoch: 0 | Loss: 0.0775\n",
      "Epoch: 0 | Loss: 0.0818\n",
      "Federated training Epoch [50/200] Val Loss: 0.0663\n",
      "test performance: [0.07391221573805973, 0.0893878374912151, 0.09157603134541478, 0.08167515186057107, 0.08795032817360064, 0.07960449891445572, 0.08573392409933349]\n",
      "Epoch: 0 | Loss: 0.0585\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0696\n",
      "Epoch: 0 | Loss: 0.0602\n",
      "Epoch: 0 | Loss: 0.0727\n",
      "Epoch: 0 | Loss: 0.0624\n",
      "Epoch: 0 | Loss: 0.0904\n",
      "Federated training Epoch [51/200] Val Loss: 0.0666\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.fed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.0884 Val Loss: 0.0779\n",
      "Epoch [2/200] Train Loss: 0.0774 Val Loss: 0.0772\n",
      "Epoch [3/200] Train Loss: 0.0767 Val Loss: 0.0765\n",
      "Epoch [4/200] Train Loss: 0.0760 Val Loss: 0.0758\n",
      "Epoch [5/200] Train Loss: 0.0753 Val Loss: 0.0748\n",
      "Epoch [6/200] Train Loss: 0.0746 Val Loss: 0.0740\n",
      "Epoch [7/200] Train Loss: 0.0739 Val Loss: 0.0740\n",
      "Epoch [8/200] Train Loss: 0.0733 Val Loss: 0.0736\n",
      "Epoch [9/200] Train Loss: 0.0726 Val Loss: 0.0721\n",
      "Epoch [10/200] Train Loss: 0.0720 Val Loss: 0.0718\n",
      "Epoch [11/200] Train Loss: 0.0714 Val Loss: 0.0707\n",
      "Epoch [12/200] Train Loss: 0.0710 Val Loss: 0.0702\n",
      "Epoch [13/200] Train Loss: 0.0707 Val Loss: 0.0699\n",
      "Epoch [14/200] Train Loss: 0.0700 Val Loss: 0.0692\n",
      "Epoch [15/200] Train Loss: 0.0696 Val Loss: 0.0690\n",
      "Epoch [16/200] Train Loss: 0.0690 Val Loss: 0.0683\n",
      "Epoch [17/200] Train Loss: 0.0687 Val Loss: 0.0687\n",
      "Epoch [18/200] Train Loss: 0.0681 Val Loss: 0.0675\n",
      "Epoch [19/200] Train Loss: 0.0676 Val Loss: 0.0672\n",
      "Epoch [20/200] Train Loss: 0.0673 Val Loss: 0.0675\n",
      "Epoch [21/200] Train Loss: 0.0668 Val Loss: 0.0666\n",
      "Epoch [22/200] Train Loss: 0.0664 Val Loss: 0.0668\n",
      "Epoch [23/200] Train Loss: 0.0659 Val Loss: 0.0656\n",
      "Epoch [24/200] Train Loss: 0.0656 Val Loss: 0.0656\n",
      "Epoch [25/200] Train Loss: 0.0653 Val Loss: 0.0662\n",
      "Epoch [26/200] Train Loss: 0.0652 Val Loss: 0.0651\n",
      "Epoch [27/200] Train Loss: 0.0646 Val Loss: 0.0656\n",
      "Epoch [28/200] Train Loss: 0.0642 Val Loss: 0.0654\n",
      "Epoch [29/200] Train Loss: 0.0639 Val Loss: 0.0645\n",
      "Epoch [30/200] Train Loss: 0.0637 Val Loss: 0.0640\n",
      "Epoch [31/200] Train Loss: 0.0632 Val Loss: 0.0639\n",
      "Epoch [32/200] Train Loss: 0.0631 Val Loss: 0.0656\n",
      "Epoch [33/200] Train Loss: 0.0629 Val Loss: 0.0638\n",
      "Epoch [34/200] Train Loss: 0.0627 Val Loss: 0.0645\n",
      "Epoch [35/200] Train Loss: 0.0623 Val Loss: 0.0632\n",
      "Epoch [36/200] Train Loss: 0.0620 Val Loss: 0.0630\n",
      "Epoch [37/200] Train Loss: 0.0617 Val Loss: 0.0634\n",
      "Epoch [38/200] Train Loss: 0.0617 Val Loss: 0.0637\n",
      "Epoch [39/200] Train Loss: 0.0612 Val Loss: 0.0633\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.central_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch Local Training!\n",
      "Epoch [1/200] Train Loss: 0.0922 Val Loss: 0.0780\n",
      "Epoch [2/200] Train Loss: 0.0775 Val Loss: 0.0773\n",
      "Epoch [3/200] Train Loss: 0.0766 Val Loss: 0.0763\n",
      "Epoch [4/200] Train Loss: 0.0758 Val Loss: 0.0754\n",
      "Epoch [5/200] Train Loss: 0.0749 Val Loss: 0.0746\n",
      "Epoch [6/200] Train Loss: 0.0742 Val Loss: 0.0738\n",
      "Epoch [7/200] Train Loss: 0.0734 Val Loss: 0.0728\n",
      "Epoch [8/200] Train Loss: 0.0729 Val Loss: 0.0722\n",
      "Epoch [9/200] Train Loss: 0.0721 Val Loss: 0.0717\n",
      "Epoch [10/200] Train Loss: 0.0717 Val Loss: 0.0710\n",
      "Epoch [11/200] Train Loss: 0.0711 Val Loss: 0.0703\n",
      "Epoch [12/200] Train Loss: 0.0704 Val Loss: 0.0698\n",
      "Epoch [13/200] Train Loss: 0.0698 Val Loss: 0.0702\n",
      "Epoch [14/200] Train Loss: 0.0694 Val Loss: 0.0685\n",
      "Epoch [15/200] Train Loss: 0.0683 Val Loss: 0.0678\n",
      "Epoch [16/200] Train Loss: 0.0679 Val Loss: 0.0673\n",
      "Epoch [17/200] Train Loss: 0.0670 Val Loss: 0.0667\n",
      "Epoch [18/200] Train Loss: 0.0665 Val Loss: 0.0673\n",
      "Epoch [19/200] Train Loss: 0.0656 Val Loss: 0.0657\n",
      "Epoch [20/200] Train Loss: 0.0653 Val Loss: 0.0657\n",
      "Epoch [21/200] Train Loss: 0.0647 Val Loss: 0.0648\n",
      "Epoch [22/200] Train Loss: 0.0642 Val Loss: 0.0652\n",
      "Epoch [23/200] Train Loss: 0.0640 Val Loss: 0.0641\n",
      "Epoch [24/200] Train Loss: 0.0633 Val Loss: 0.0650\n",
      "Epoch [25/200] Train Loss: 0.0629 Val Loss: 0.0638\n",
      "Epoch [26/200] Train Loss: 0.0626 Val Loss: 0.0639\n",
      "Epoch [27/200] Train Loss: 0.0622 Val Loss: 0.0631\n",
      "Epoch [28/200] Train Loss: 0.0618 Val Loss: 0.0631\n",
      "Epoch [29/200] Train Loss: 0.0614 Val Loss: 0.0624\n",
      "Epoch [30/200] Train Loss: 0.0613 Val Loss: 0.0623\n",
      "Epoch [31/200] Train Loss: 0.0607 Val Loss: 0.0621\n",
      "Epoch [32/200] Train Loss: 0.0605 Val Loss: 0.0616\n",
      "Epoch [33/200] Train Loss: 0.0602 Val Loss: 0.0624\n",
      "Epoch [34/200] Train Loss: 0.0600 Val Loss: 0.0613\n",
      "Epoch [35/200] Train Loss: 0.0601 Val Loss: 0.0615\n",
      "Epoch [36/200] Train Loss: 0.0594 Val Loss: 0.0607\n",
      "Epoch [37/200] Train Loss: 0.0591 Val Loss: 0.0623\n",
      "Epoch [38/200] Train Loss: 0.0589 Val Loss: 0.0611\n",
      "Epoch [39/200] Train Loss: 0.0585 Val Loss: 0.0605\n",
      "Epoch [40/200] Train Loss: 0.0583 Val Loss: 0.0607\n",
      "Epoch [41/200] Train Loss: 0.0582 Val Loss: 0.0603\n",
      "Epoch [42/200] Train Loss: 0.0578 Val Loss: 0.0599\n",
      "Epoch [43/200] Train Loss: 0.0577 Val Loss: 0.0610\n",
      "Epoch [44/200] Train Loss: 0.0574 Val Loss: 0.0600\n",
      "Epoch [45/200] Train Loss: 0.0570 Val Loss: 0.0591\n",
      "Epoch [46/200] Train Loss: 0.0567 Val Loss: 0.0590\n",
      "Epoch [47/200] Train Loss: 0.0566 Val Loss: 0.0604\n",
      "Epoch [48/200] Train Loss: 0.0566 Val Loss: 0.0588\n",
      "Epoch [49/200] Train Loss: 0.0562 Val Loss: 0.0582\n",
      "Epoch [50/200] Train Loss: 0.0560 Val Loss: 0.0584\n",
      "Epoch [51/200] Train Loss: 0.0556 Val Loss: 0.0584\n",
      "Epoch [52/200] Train Loss: 0.0555 Val Loss: 0.0583\n",
      "Epoch [53/200] Train Loss: 0.0552 Val Loss: 0.0579\n",
      "Epoch [54/200] Train Loss: 0.0551 Val Loss: 0.0575\n",
      "Epoch [55/200] Train Loss: 0.0548 Val Loss: 0.0575\n",
      "Epoch [56/200] Train Loss: 0.0545 Val Loss: 0.0572\n",
      "Epoch [57/200] Train Loss: 0.0546 Val Loss: 0.0568\n",
      "Epoch [58/200] Train Loss: 0.0539 Val Loss: 0.0572\n",
      "Epoch [59/200] Train Loss: 0.0537 Val Loss: 0.0568\n",
      "Epoch [60/200] Train Loss: 0.0540 Val Loss: 0.0567\n",
      "Epoch [61/200] Train Loss: 0.0533 Val Loss: 0.0560\n",
      "Epoch [62/200] Train Loss: 0.0531 Val Loss: 0.0566\n",
      "Epoch [63/200] Train Loss: 0.0529 Val Loss: 0.0558\n",
      "Epoch [64/200] Train Loss: 0.0526 Val Loss: 0.0557\n",
      "Epoch [65/200] Train Loss: 0.0525 Val Loss: 0.0556\n",
      "Epoch [66/200] Train Loss: 0.0522 Val Loss: 0.0556\n",
      "Epoch [67/200] Train Loss: 0.0520 Val Loss: 0.0553\n",
      "Epoch [68/200] Train Loss: 0.0518 Val Loss: 0.0550\n",
      "Epoch [69/200] Train Loss: 0.0517 Val Loss: 0.0548\n",
      "Epoch [70/200] Train Loss: 0.0512 Val Loss: 0.0545\n",
      "Epoch [71/200] Train Loss: 0.0511 Val Loss: 0.0544\n",
      "Epoch [72/200] Train Loss: 0.0512 Val Loss: 0.0540\n",
      "Epoch [73/200] Train Loss: 0.0506 Val Loss: 0.0543\n",
      "Epoch [74/200] Train Loss: 0.0505 Val Loss: 0.0538\n",
      "Epoch [75/200] Train Loss: 0.0501 Val Loss: 0.0543\n",
      "Epoch [76/200] Train Loss: 0.0498 Val Loss: 0.0537\n",
      "Epoch [77/200] Train Loss: 0.0501 Val Loss: 0.0535\n",
      "Epoch [78/200] Train Loss: 0.0498 Val Loss: 0.0536\n",
      "Epoch [79/200] Train Loss: 0.0495 Val Loss: 0.0532\n",
      "Epoch [80/200] Train Loss: 0.0493 Val Loss: 0.0535\n",
      "Epoch [81/200] Train Loss: 0.0490 Val Loss: 0.0531\n",
      "Epoch [82/200] Train Loss: 0.0484 Val Loss: 0.0531\n",
      "Epoch [83/200] Train Loss: 0.0483 Val Loss: 0.0527\n",
      "Epoch [84/200] Train Loss: 0.0482 Val Loss: 0.0521\n",
      "Epoch [85/200] Train Loss: 0.0481 Val Loss: 0.0521\n",
      "Epoch [86/200] Train Loss: 0.0483 Val Loss: 0.0525\n",
      "Epoch [87/200] Train Loss: 0.0478 Val Loss: 0.0521\n",
      "Epoch [88/200] Train Loss: 0.0473 Val Loss: 0.0517\n",
      "Epoch [89/200] Train Loss: 0.0471 Val Loss: 0.0518\n",
      "Epoch [90/200] Train Loss: 0.0472 Val Loss: 0.0511\n",
      "Epoch [91/200] Train Loss: 0.0466 Val Loss: 0.0515\n",
      "Epoch [92/200] Train Loss: 0.0466 Val Loss: 0.0510\n",
      "Epoch [93/200] Train Loss: 0.0463 Val Loss: 0.0505\n",
      "Epoch [94/200] Train Loss: 0.0463 Val Loss: 0.0507\n",
      "Epoch [95/200] Train Loss: 0.0464 Val Loss: 0.0508\n",
      "Epoch [96/200] Train Loss: 0.0457 Val Loss: 0.0503\n",
      "Epoch [97/200] Train Loss: 0.0459 Val Loss: 0.0508\n",
      "Epoch [98/200] Train Loss: 0.0454 Val Loss: 0.0504\n",
      "Epoch [99/200] Train Loss: 0.0451 Val Loss: 0.0505\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0934 Val Loss: 0.0813\n",
      "Epoch [2/200] Train Loss: 0.0822 Val Loss: 0.0806\n",
      "Epoch [3/200] Train Loss: 0.0814 Val Loss: 0.0797\n",
      "Epoch [4/200] Train Loss: 0.0808 Val Loss: 0.0794\n",
      "Epoch [5/200] Train Loss: 0.0802 Val Loss: 0.0787\n",
      "Epoch [6/200] Train Loss: 0.0797 Val Loss: 0.0782\n",
      "Epoch [7/200] Train Loss: 0.0793 Val Loss: 0.0778\n",
      "Epoch [8/200] Train Loss: 0.0788 Val Loss: 0.0775\n",
      "Epoch [9/200] Train Loss: 0.0783 Val Loss: 0.0772\n",
      "Epoch [10/200] Train Loss: 0.0780 Val Loss: 0.0769\n",
      "Epoch [11/200] Train Loss: 0.0777 Val Loss: 0.0767\n",
      "Epoch [12/200] Train Loss: 0.0772 Val Loss: 0.0771\n",
      "Epoch [13/200] Train Loss: 0.0768 Val Loss: 0.0761\n",
      "Epoch [14/200] Train Loss: 0.0763 Val Loss: 0.0758\n",
      "Epoch [15/200] Train Loss: 0.0760 Val Loss: 0.0760\n",
      "Epoch [16/200] Train Loss: 0.0757 Val Loss: 0.0754\n",
      "Epoch [17/200] Train Loss: 0.0754 Val Loss: 0.0759\n",
      "Epoch [18/200] Train Loss: 0.0750 Val Loss: 0.0749\n",
      "Epoch [19/200] Train Loss: 0.0746 Val Loss: 0.0746\n",
      "Epoch [20/200] Train Loss: 0.0744 Val Loss: 0.0749\n",
      "Epoch [21/200] Train Loss: 0.0740 Val Loss: 0.0743\n",
      "Epoch [22/200] Train Loss: 0.0736 Val Loss: 0.0744\n",
      "Epoch [23/200] Train Loss: 0.0734 Val Loss: 0.0735\n",
      "Epoch [24/200] Train Loss: 0.0729 Val Loss: 0.0730\n",
      "Epoch [25/200] Train Loss: 0.0727 Val Loss: 0.0731\n",
      "Epoch [26/200] Train Loss: 0.0722 Val Loss: 0.0728\n",
      "Epoch [27/200] Train Loss: 0.0721 Val Loss: 0.0726\n",
      "Epoch [28/200] Train Loss: 0.0717 Val Loss: 0.0729\n",
      "Epoch [29/200] Train Loss: 0.0713 Val Loss: 0.0717\n",
      "Epoch [30/200] Train Loss: 0.0708 Val Loss: 0.0715\n",
      "Epoch [31/200] Train Loss: 0.0705 Val Loss: 0.0721\n",
      "Epoch [32/200] Train Loss: 0.0702 Val Loss: 0.0711\n",
      "Epoch [33/200] Train Loss: 0.0698 Val Loss: 0.0707\n",
      "Epoch [34/200] Train Loss: 0.0693 Val Loss: 0.0703\n",
      "Epoch [35/200] Train Loss: 0.0692 Val Loss: 0.0704\n",
      "Epoch [36/200] Train Loss: 0.0688 Val Loss: 0.0701\n",
      "Epoch [37/200] Train Loss: 0.0682 Val Loss: 0.0698\n",
      "Epoch [38/200] Train Loss: 0.0679 Val Loss: 0.0696\n",
      "Epoch [39/200] Train Loss: 0.0677 Val Loss: 0.0690\n",
      "Epoch [40/200] Train Loss: 0.0671 Val Loss: 0.0685\n",
      "Epoch [41/200] Train Loss: 0.0669 Val Loss: 0.0690\n",
      "Epoch [42/200] Train Loss: 0.0668 Val Loss: 0.0683\n",
      "Epoch [43/200] Train Loss: 0.0661 Val Loss: 0.0678\n",
      "Epoch [44/200] Train Loss: 0.0656 Val Loss: 0.0680\n",
      "Epoch [45/200] Train Loss: 0.0652 Val Loss: 0.0675\n",
      "Epoch [46/200] Train Loss: 0.0647 Val Loss: 0.0675\n",
      "Epoch [47/200] Train Loss: 0.0648 Val Loss: 0.0685\n",
      "Epoch [48/200] Train Loss: 0.0639 Val Loss: 0.0662\n",
      "Epoch [49/200] Train Loss: 0.0636 Val Loss: 0.0660\n",
      "Epoch [50/200] Train Loss: 0.0632 Val Loss: 0.0654\n",
      "Epoch [51/200] Train Loss: 0.0628 Val Loss: 0.0650\n",
      "Epoch [52/200] Train Loss: 0.0622 Val Loss: 0.0649\n",
      "Epoch [53/200] Train Loss: 0.0617 Val Loss: 0.0651\n",
      "Epoch [54/200] Train Loss: 0.0612 Val Loss: 0.0639\n",
      "Epoch [55/200] Train Loss: 0.0610 Val Loss: 0.0634\n",
      "Epoch [56/200] Train Loss: 0.0602 Val Loss: 0.0629\n",
      "Epoch [57/200] Train Loss: 0.0597 Val Loss: 0.0625\n",
      "Epoch [58/200] Train Loss: 0.0593 Val Loss: 0.0630\n",
      "Epoch [59/200] Train Loss: 0.0591 Val Loss: 0.0616\n",
      "Epoch [60/200] Train Loss: 0.0582 Val Loss: 0.0608\n",
      "Epoch [61/200] Train Loss: 0.0578 Val Loss: 0.0620\n",
      "Epoch [62/200] Train Loss: 0.0572 Val Loss: 0.0598\n",
      "Epoch [63/200] Train Loss: 0.0564 Val Loss: 0.0599\n",
      "Epoch [64/200] Train Loss: 0.0562 Val Loss: 0.0592\n",
      "Epoch [65/200] Train Loss: 0.0553 Val Loss: 0.0590\n",
      "Epoch [66/200] Train Loss: 0.0548 Val Loss: 0.0575\n",
      "Epoch [67/200] Train Loss: 0.0544 Val Loss: 0.0579\n",
      "Epoch [68/200] Train Loss: 0.0538 Val Loss: 0.0571\n",
      "Epoch [69/200] Train Loss: 0.0532 Val Loss: 0.0584\n",
      "Epoch [70/200] Train Loss: 0.0530 Val Loss: 0.0568\n",
      "Epoch [71/200] Train Loss: 0.0523 Val Loss: 0.0554\n",
      "Epoch [72/200] Train Loss: 0.0515 Val Loss: 0.0554\n",
      "Epoch [73/200] Train Loss: 0.0508 Val Loss: 0.0543\n",
      "Epoch [74/200] Train Loss: 0.0507 Val Loss: 0.0539\n",
      "Epoch [75/200] Train Loss: 0.0498 Val Loss: 0.0538\n",
      "Epoch [76/200] Train Loss: 0.0495 Val Loss: 0.0531\n",
      "Epoch [77/200] Train Loss: 0.0489 Val Loss: 0.0535\n",
      "Epoch [78/200] Train Loss: 0.0485 Val Loss: 0.0530\n",
      "Epoch [79/200] Train Loss: 0.0479 Val Loss: 0.0545\n",
      "Epoch [80/200] Train Loss: 0.0475 Val Loss: 0.0513\n",
      "Epoch [81/200] Train Loss: 0.0472 Val Loss: 0.0516\n",
      "Epoch [82/200] Train Loss: 0.0469 Val Loss: 0.0502\n",
      "Epoch [83/200] Train Loss: 0.0463 Val Loss: 0.0503\n",
      "Epoch [84/200] Train Loss: 0.0459 Val Loss: 0.0498\n",
      "Epoch [85/200] Train Loss: 0.0457 Val Loss: 0.0496\n",
      "Epoch [86/200] Train Loss: 0.0450 Val Loss: 0.0512\n",
      "Epoch [87/200] Train Loss: 0.0448 Val Loss: 0.0504\n",
      "Epoch [88/200] Train Loss: 0.0447 Val Loss: 0.0485\n",
      "Epoch [89/200] Train Loss: 0.0442 Val Loss: 0.0489\n",
      "Epoch [90/200] Train Loss: 0.0440 Val Loss: 0.0475\n",
      "Epoch [91/200] Train Loss: 0.0436 Val Loss: 0.0481\n",
      "Epoch [92/200] Train Loss: 0.0434 Val Loss: 0.0480\n",
      "Epoch [93/200] Train Loss: 0.0428 Val Loss: 0.0495\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1149 Val Loss: 0.0989\n",
      "Epoch [2/200] Train Loss: 0.0984 Val Loss: 0.0980\n",
      "Epoch [3/200] Train Loss: 0.0974 Val Loss: 0.0967\n",
      "Epoch [4/200] Train Loss: 0.0965 Val Loss: 0.0954\n",
      "Epoch [5/200] Train Loss: 0.0955 Val Loss: 0.0942\n",
      "Epoch [6/200] Train Loss: 0.0946 Val Loss: 0.0937\n",
      "Epoch [7/200] Train Loss: 0.0940 Val Loss: 0.0935\n",
      "Epoch [8/200] Train Loss: 0.0933 Val Loss: 0.0923\n",
      "Epoch [9/200] Train Loss: 0.0927 Val Loss: 0.0919\n",
      "Epoch [10/200] Train Loss: 0.0918 Val Loss: 0.0914\n",
      "Epoch [11/200] Train Loss: 0.0915 Val Loss: 0.0915\n",
      "Epoch [12/200] Train Loss: 0.0907 Val Loss: 0.0900\n",
      "Epoch [13/200] Train Loss: 0.0903 Val Loss: 0.0904\n",
      "Epoch [14/200] Train Loss: 0.0896 Val Loss: 0.0894\n",
      "Epoch [15/200] Train Loss: 0.0890 Val Loss: 0.0892\n",
      "Epoch [16/200] Train Loss: 0.0885 Val Loss: 0.0884\n",
      "Epoch [17/200] Train Loss: 0.0877 Val Loss: 0.0889\n",
      "Epoch [18/200] Train Loss: 0.0872 Val Loss: 0.0876\n",
      "Epoch [19/200] Train Loss: 0.0864 Val Loss: 0.0878\n",
      "Epoch [20/200] Train Loss: 0.0862 Val Loss: 0.0872\n",
      "Epoch [21/200] Train Loss: 0.0855 Val Loss: 0.0864\n",
      "Epoch [22/200] Train Loss: 0.0849 Val Loss: 0.0861\n",
      "Epoch [23/200] Train Loss: 0.0844 Val Loss: 0.0859\n",
      "Epoch [24/200] Train Loss: 0.0840 Val Loss: 0.0855\n",
      "Epoch [25/200] Train Loss: 0.0833 Val Loss: 0.0865\n",
      "Epoch [26/200] Train Loss: 0.0828 Val Loss: 0.0847\n",
      "Epoch [27/200] Train Loss: 0.0821 Val Loss: 0.0854\n",
      "Epoch [28/200] Train Loss: 0.0818 Val Loss: 0.0842\n",
      "Epoch [29/200] Train Loss: 0.0811 Val Loss: 0.0844\n",
      "Epoch [30/200] Train Loss: 0.0806 Val Loss: 0.0838\n",
      "Epoch [31/200] Train Loss: 0.0801 Val Loss: 0.0832\n",
      "Epoch [32/200] Train Loss: 0.0797 Val Loss: 0.0832\n",
      "Epoch [33/200] Train Loss: 0.0795 Val Loss: 0.0826\n",
      "Epoch [34/200] Train Loss: 0.0791 Val Loss: 0.0826\n",
      "Epoch [35/200] Train Loss: 0.0785 Val Loss: 0.0822\n",
      "Epoch [36/200] Train Loss: 0.0781 Val Loss: 0.0818\n",
      "Epoch [37/200] Train Loss: 0.0778 Val Loss: 0.0834\n",
      "Epoch [38/200] Train Loss: 0.0777 Val Loss: 0.0815\n",
      "Epoch [39/200] Train Loss: 0.0769 Val Loss: 0.0822\n",
      "Epoch [40/200] Train Loss: 0.0769 Val Loss: 0.0831\n",
      "Epoch [41/200] Train Loss: 0.0764 Val Loss: 0.0806\n",
      "Epoch [42/200] Train Loss: 0.0759 Val Loss: 0.0802\n",
      "Epoch [43/200] Train Loss: 0.0753 Val Loss: 0.0804\n",
      "Epoch [44/200] Train Loss: 0.0752 Val Loss: 0.0823\n",
      "Epoch [45/200] Train Loss: 0.0747 Val Loss: 0.0807\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1094 Val Loss: 0.0888\n",
      "Epoch [2/200] Train Loss: 0.0892 Val Loss: 0.0882\n",
      "Epoch [3/200] Train Loss: 0.0884 Val Loss: 0.0871\n",
      "Epoch [4/200] Train Loss: 0.0877 Val Loss: 0.0863\n",
      "Epoch [5/200] Train Loss: 0.0871 Val Loss: 0.0857\n",
      "Epoch [6/200] Train Loss: 0.0865 Val Loss: 0.0850\n",
      "Epoch [7/200] Train Loss: 0.0859 Val Loss: 0.0843\n",
      "Epoch [8/200] Train Loss: 0.0853 Val Loss: 0.0835\n",
      "Epoch [9/200] Train Loss: 0.0848 Val Loss: 0.0831\n",
      "Epoch [10/200] Train Loss: 0.0841 Val Loss: 0.0826\n",
      "Epoch [11/200] Train Loss: 0.0837 Val Loss: 0.0819\n",
      "Epoch [12/200] Train Loss: 0.0831 Val Loss: 0.0815\n",
      "Epoch [13/200] Train Loss: 0.0825 Val Loss: 0.0810\n",
      "Epoch [14/200] Train Loss: 0.0819 Val Loss: 0.0805\n",
      "Epoch [15/200] Train Loss: 0.0814 Val Loss: 0.0800\n",
      "Epoch [16/200] Train Loss: 0.0808 Val Loss: 0.0798\n",
      "Epoch [17/200] Train Loss: 0.0801 Val Loss: 0.0787\n",
      "Epoch [18/200] Train Loss: 0.0794 Val Loss: 0.0789\n",
      "Epoch [19/200] Train Loss: 0.0789 Val Loss: 0.0782\n",
      "Epoch [20/200] Train Loss: 0.0782 Val Loss: 0.0773\n",
      "Epoch [21/200] Train Loss: 0.0777 Val Loss: 0.0774\n",
      "Epoch [22/200] Train Loss: 0.0772 Val Loss: 0.0767\n",
      "Epoch [23/200] Train Loss: 0.0765 Val Loss: 0.0758\n",
      "Epoch [24/200] Train Loss: 0.0762 Val Loss: 0.0757\n",
      "Epoch [25/200] Train Loss: 0.0757 Val Loss: 0.0760\n",
      "Epoch [26/200] Train Loss: 0.0753 Val Loss: 0.0753\n",
      "Epoch [27/200] Train Loss: 0.0746 Val Loss: 0.0744\n",
      "Epoch [28/200] Train Loss: 0.0742 Val Loss: 0.0742\n",
      "Epoch [29/200] Train Loss: 0.0737 Val Loss: 0.0743\n",
      "Epoch [30/200] Train Loss: 0.0732 Val Loss: 0.0739\n",
      "Epoch [31/200] Train Loss: 0.0726 Val Loss: 0.0735\n",
      "Epoch [32/200] Train Loss: 0.0723 Val Loss: 0.0735\n",
      "Epoch [33/200] Train Loss: 0.0717 Val Loss: 0.0734\n",
      "Epoch [34/200] Train Loss: 0.0717 Val Loss: 0.0728\n",
      "Epoch [35/200] Train Loss: 0.0713 Val Loss: 0.0721\n",
      "Epoch [36/200] Train Loss: 0.0712 Val Loss: 0.0723\n",
      "Epoch [37/200] Train Loss: 0.0705 Val Loss: 0.0752\n",
      "Epoch [38/200] Train Loss: 0.0702 Val Loss: 0.0711\n",
      "Epoch [39/200] Train Loss: 0.0699 Val Loss: 0.0730\n",
      "Epoch [40/200] Train Loss: 0.0695 Val Loss: 0.0705\n",
      "Epoch [41/200] Train Loss: 0.0693 Val Loss: 0.0707\n",
      "Epoch [42/200] Train Loss: 0.0688 Val Loss: 0.0725\n",
      "Epoch [43/200] Train Loss: 0.0685 Val Loss: 0.0703\n",
      "Epoch [44/200] Train Loss: 0.0682 Val Loss: 0.0721\n",
      "Epoch [45/200] Train Loss: 0.0680 Val Loss: 0.0693\n",
      "Epoch [46/200] Train Loss: 0.0681 Val Loss: 0.0702\n",
      "Epoch [47/200] Train Loss: 0.0671 Val Loss: 0.0692\n",
      "Epoch [48/200] Train Loss: 0.0673 Val Loss: 0.0690\n",
      "Epoch [49/200] Train Loss: 0.0667 Val Loss: 0.0691\n",
      "Epoch [50/200] Train Loss: 0.0667 Val Loss: 0.0713\n",
      "Epoch [51/200] Train Loss: 0.0664 Val Loss: 0.0685\n",
      "Epoch [52/200] Train Loss: 0.0662 Val Loss: 0.0685\n",
      "Epoch [53/200] Train Loss: 0.0655 Val Loss: 0.0689\n",
      "Epoch [54/200] Train Loss: 0.0650 Val Loss: 0.0683\n",
      "Epoch [55/200] Train Loss: 0.0649 Val Loss: 0.0684\n",
      "Epoch [56/200] Train Loss: 0.0645 Val Loss: 0.0689\n",
      "Epoch [57/200] Train Loss: 0.0644 Val Loss: 0.0673\n",
      "Epoch [58/200] Train Loss: 0.0638 Val Loss: 0.0669\n",
      "Epoch [59/200] Train Loss: 0.0639 Val Loss: 0.0692\n",
      "Epoch [60/200] Train Loss: 0.0634 Val Loss: 0.0673\n",
      "Epoch [61/200] Train Loss: 0.0632 Val Loss: 0.0666\n",
      "Epoch [62/200] Train Loss: 0.0630 Val Loss: 0.0658\n",
      "Epoch [63/200] Train Loss: 0.0624 Val Loss: 0.0657\n",
      "Epoch [64/200] Train Loss: 0.0624 Val Loss: 0.0659\n",
      "Epoch [65/200] Train Loss: 0.0619 Val Loss: 0.0657\n",
      "Epoch [66/200] Train Loss: 0.0615 Val Loss: 0.0659\n",
      "Epoch [67/200] Train Loss: 0.0616 Val Loss: 0.0649\n",
      "Epoch [68/200] Train Loss: 0.0613 Val Loss: 0.0645\n",
      "Epoch [69/200] Train Loss: 0.0609 Val Loss: 0.0657\n",
      "Epoch [70/200] Train Loss: 0.0608 Val Loss: 0.0642\n",
      "Epoch [71/200] Train Loss: 0.0602 Val Loss: 0.0651\n",
      "Epoch [72/200] Train Loss: 0.0598 Val Loss: 0.0633\n",
      "Epoch [73/200] Train Loss: 0.0599 Val Loss: 0.0636\n",
      "Epoch [74/200] Train Loss: 0.0595 Val Loss: 0.0641\n",
      "Epoch [75/200] Train Loss: 0.0589 Val Loss: 0.0633\n",
      "Epoch [76/200] Train Loss: 0.0587 Val Loss: 0.0630\n",
      "Epoch [77/200] Train Loss: 0.0582 Val Loss: 0.0625\n",
      "Epoch [78/200] Train Loss: 0.0584 Val Loss: 0.0619\n",
      "Epoch [79/200] Train Loss: 0.0574 Val Loss: 0.0617\n",
      "Epoch [80/200] Train Loss: 0.0573 Val Loss: 0.0616\n",
      "Epoch [81/200] Train Loss: 0.0571 Val Loss: 0.0622\n",
      "Epoch [82/200] Train Loss: 0.0569 Val Loss: 0.0612\n",
      "Epoch [83/200] Train Loss: 0.0563 Val Loss: 0.0613\n",
      "Epoch [84/200] Train Loss: 0.0562 Val Loss: 0.0611\n",
      "Epoch [85/200] Train Loss: 0.0560 Val Loss: 0.0601\n",
      "Epoch [86/200] Train Loss: 0.0555 Val Loss: 0.0599\n",
      "Epoch [87/200] Train Loss: 0.0550 Val Loss: 0.0620\n",
      "Epoch [88/200] Train Loss: 0.0550 Val Loss: 0.0594\n",
      "Epoch [89/200] Train Loss: 0.0546 Val Loss: 0.0615\n",
      "Epoch [90/200] Train Loss: 0.0542 Val Loss: 0.0586\n",
      "Epoch [91/200] Train Loss: 0.0540 Val Loss: 0.0594\n",
      "Epoch [92/200] Train Loss: 0.0535 Val Loss: 0.0578\n",
      "Epoch [93/200] Train Loss: 0.0532 Val Loss: 0.0575\n",
      "Epoch [94/200] Train Loss: 0.0527 Val Loss: 0.0571\n",
      "Epoch [95/200] Train Loss: 0.0524 Val Loss: 0.0597\n",
      "Epoch [96/200] Train Loss: 0.0522 Val Loss: 0.0575\n",
      "Epoch [97/200] Train Loss: 0.0518 Val Loss: 0.0569\n",
      "Epoch [98/200] Train Loss: 0.0516 Val Loss: 0.0578\n",
      "Epoch [99/200] Train Loss: 0.0515 Val Loss: 0.0567\n",
      "Epoch [100/200] Train Loss: 0.0511 Val Loss: 0.0570\n",
      "Epoch [101/200] Train Loss: 0.0509 Val Loss: 0.0557\n",
      "Epoch [102/200] Train Loss: 0.0507 Val Loss: 0.0548\n",
      "Epoch [103/200] Train Loss: 0.0503 Val Loss: 0.0549\n",
      "Epoch [104/200] Train Loss: 0.0503 Val Loss: 0.0560\n",
      "Epoch [105/200] Train Loss: 0.0499 Val Loss: 0.0543\n",
      "Epoch [106/200] Train Loss: 0.0497 Val Loss: 0.0556\n",
      "Epoch [107/200] Train Loss: 0.0496 Val Loss: 0.0549\n",
      "Epoch [108/200] Train Loss: 0.0494 Val Loss: 0.0535\n",
      "Epoch [109/200] Train Loss: 0.0489 Val Loss: 0.0542\n",
      "Epoch [110/200] Train Loss: 0.0489 Val Loss: 0.0537\n",
      "Epoch [111/200] Train Loss: 0.0485 Val Loss: 0.0531\n",
      "Epoch [112/200] Train Loss: 0.0484 Val Loss: 0.0544\n",
      "Epoch [113/200] Train Loss: 0.0481 Val Loss: 0.0537\n",
      "Epoch [114/200] Train Loss: 0.0477 Val Loss: 0.0525\n",
      "Epoch [115/200] Train Loss: 0.0482 Val Loss: 0.0523\n",
      "Epoch [116/200] Train Loss: 0.0471 Val Loss: 0.0522\n",
      "Epoch [117/200] Train Loss: 0.0470 Val Loss: 0.0532\n",
      "Epoch [118/200] Train Loss: 0.0471 Val Loss: 0.0527\n",
      "Epoch [119/200] Train Loss: 0.0466 Val Loss: 0.0533\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1097 Val Loss: 0.0887\n",
      "Epoch [2/200] Train Loss: 0.0893 Val Loss: 0.0881\n",
      "Epoch [3/200] Train Loss: 0.0886 Val Loss: 0.0874\n",
      "Epoch [4/200] Train Loss: 0.0877 Val Loss: 0.0868\n",
      "Epoch [5/200] Train Loss: 0.0869 Val Loss: 0.0861\n",
      "Epoch [6/200] Train Loss: 0.0861 Val Loss: 0.0857\n",
      "Epoch [7/200] Train Loss: 0.0853 Val Loss: 0.0854\n",
      "Epoch [8/200] Train Loss: 0.0847 Val Loss: 0.0842\n",
      "Epoch [9/200] Train Loss: 0.0839 Val Loss: 0.0836\n",
      "Epoch [10/200] Train Loss: 0.0834 Val Loss: 0.0845\n",
      "Epoch [11/200] Train Loss: 0.0829 Val Loss: 0.0827\n",
      "Epoch [12/200] Train Loss: 0.0823 Val Loss: 0.0823\n",
      "Epoch [13/200] Train Loss: 0.0817 Val Loss: 0.0814\n",
      "Epoch [14/200] Train Loss: 0.0812 Val Loss: 0.0810\n",
      "Epoch [15/200] Train Loss: 0.0806 Val Loss: 0.0803\n",
      "Epoch [16/200] Train Loss: 0.0801 Val Loss: 0.0803\n",
      "Epoch [17/200] Train Loss: 0.0794 Val Loss: 0.0794\n",
      "Epoch [18/200] Train Loss: 0.0788 Val Loss: 0.0788\n",
      "Epoch [19/200] Train Loss: 0.0784 Val Loss: 0.0783\n",
      "Epoch [20/200] Train Loss: 0.0778 Val Loss: 0.0778\n",
      "Epoch [21/200] Train Loss: 0.0773 Val Loss: 0.0773\n",
      "Epoch [22/200] Train Loss: 0.0767 Val Loss: 0.0770\n",
      "Epoch [23/200] Train Loss: 0.0763 Val Loss: 0.0767\n",
      "Epoch [24/200] Train Loss: 0.0757 Val Loss: 0.0775\n",
      "Epoch [25/200] Train Loss: 0.0754 Val Loss: 0.0765\n",
      "Epoch [26/200] Train Loss: 0.0747 Val Loss: 0.0754\n",
      "Epoch [27/200] Train Loss: 0.0744 Val Loss: 0.0747\n",
      "Epoch [28/200] Train Loss: 0.0736 Val Loss: 0.0742\n",
      "Epoch [29/200] Train Loss: 0.0732 Val Loss: 0.0738\n",
      "Epoch [30/200] Train Loss: 0.0725 Val Loss: 0.0739\n",
      "Epoch [31/200] Train Loss: 0.0720 Val Loss: 0.0729\n",
      "Epoch [32/200] Train Loss: 0.0717 Val Loss: 0.0730\n",
      "Epoch [33/200] Train Loss: 0.0709 Val Loss: 0.0722\n",
      "Epoch [34/200] Train Loss: 0.0711 Val Loss: 0.0717\n",
      "Epoch [35/200] Train Loss: 0.0701 Val Loss: 0.0716\n",
      "Epoch [36/200] Train Loss: 0.0695 Val Loss: 0.0714\n",
      "Epoch [37/200] Train Loss: 0.0690 Val Loss: 0.0702\n",
      "Epoch [38/200] Train Loss: 0.0685 Val Loss: 0.0704\n",
      "Epoch [39/200] Train Loss: 0.0682 Val Loss: 0.0725\n",
      "Epoch [40/200] Train Loss: 0.0674 Val Loss: 0.0700\n",
      "Epoch [41/200] Train Loss: 0.0670 Val Loss: 0.0683\n",
      "Epoch [42/200] Train Loss: 0.0663 Val Loss: 0.0680\n",
      "Epoch [43/200] Train Loss: 0.0656 Val Loss: 0.0674\n",
      "Epoch [44/200] Train Loss: 0.0652 Val Loss: 0.0686\n",
      "Epoch [45/200] Train Loss: 0.0647 Val Loss: 0.0685\n",
      "Epoch [46/200] Train Loss: 0.0640 Val Loss: 0.0664\n",
      "Epoch [47/200] Train Loss: 0.0632 Val Loss: 0.0651\n",
      "Epoch [48/200] Train Loss: 0.0627 Val Loss: 0.0654\n",
      "Epoch [49/200] Train Loss: 0.0622 Val Loss: 0.0643\n",
      "Epoch [50/200] Train Loss: 0.0614 Val Loss: 0.0638\n",
      "Epoch [51/200] Train Loss: 0.0611 Val Loss: 0.0634\n",
      "Epoch [52/200] Train Loss: 0.0602 Val Loss: 0.0628\n",
      "Epoch [53/200] Train Loss: 0.0597 Val Loss: 0.0623\n",
      "Epoch [54/200] Train Loss: 0.0593 Val Loss: 0.0621\n",
      "Epoch [55/200] Train Loss: 0.0583 Val Loss: 0.0626\n",
      "Epoch [56/200] Train Loss: 0.0582 Val Loss: 0.0618\n",
      "Epoch [57/200] Train Loss: 0.0578 Val Loss: 0.0603\n",
      "Epoch [58/200] Train Loss: 0.0571 Val Loss: 0.0600\n",
      "Epoch [59/200] Train Loss: 0.0564 Val Loss: 0.0592\n",
      "Epoch [60/200] Train Loss: 0.0559 Val Loss: 0.0587\n",
      "Epoch [61/200] Train Loss: 0.0553 Val Loss: 0.0593\n",
      "Epoch [62/200] Train Loss: 0.0552 Val Loss: 0.0579\n",
      "Epoch [63/200] Train Loss: 0.0549 Val Loss: 0.0578\n",
      "Epoch [64/200] Train Loss: 0.0541 Val Loss: 0.0574\n",
      "Epoch [65/200] Train Loss: 0.0537 Val Loss: 0.0570\n",
      "Epoch [66/200] Train Loss: 0.0535 Val Loss: 0.0579\n",
      "Epoch [67/200] Train Loss: 0.0530 Val Loss: 0.0574\n",
      "Epoch [68/200] Train Loss: 0.0529 Val Loss: 0.0629\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1065 Val Loss: 0.0848\n",
      "Epoch [2/200] Train Loss: 0.0850 Val Loss: 0.0838\n",
      "Epoch [3/200] Train Loss: 0.0841 Val Loss: 0.0829\n",
      "Epoch [4/200] Train Loss: 0.0835 Val Loss: 0.0822\n",
      "Epoch [5/200] Train Loss: 0.0829 Val Loss: 0.0814\n",
      "Epoch [6/200] Train Loss: 0.0823 Val Loss: 0.0809\n",
      "Epoch [7/200] Train Loss: 0.0818 Val Loss: 0.0807\n",
      "Epoch [8/200] Train Loss: 0.0812 Val Loss: 0.0803\n",
      "Epoch [9/200] Train Loss: 0.0807 Val Loss: 0.0792\n",
      "Epoch [10/200] Train Loss: 0.0803 Val Loss: 0.0786\n",
      "Epoch [11/200] Train Loss: 0.0797 Val Loss: 0.0782\n",
      "Epoch [12/200] Train Loss: 0.0795 Val Loss: 0.0784\n",
      "Epoch [13/200] Train Loss: 0.0789 Val Loss: 0.0774\n",
      "Epoch [14/200] Train Loss: 0.0784 Val Loss: 0.0768\n",
      "Epoch [15/200] Train Loss: 0.0777 Val Loss: 0.0765\n",
      "Epoch [16/200] Train Loss: 0.0771 Val Loss: 0.0758\n",
      "Epoch [17/200] Train Loss: 0.0767 Val Loss: 0.0756\n",
      "Epoch [18/200] Train Loss: 0.0762 Val Loss: 0.0749\n",
      "Epoch [19/200] Train Loss: 0.0755 Val Loss: 0.0754\n",
      "Epoch [20/200] Train Loss: 0.0751 Val Loss: 0.0739\n",
      "Epoch [21/200] Train Loss: 0.0745 Val Loss: 0.0736\n",
      "Epoch [22/200] Train Loss: 0.0739 Val Loss: 0.0735\n",
      "Epoch [23/200] Train Loss: 0.0736 Val Loss: 0.0734\n",
      "Epoch [24/200] Train Loss: 0.0730 Val Loss: 0.0729\n",
      "Epoch [25/200] Train Loss: 0.0726 Val Loss: 0.0723\n",
      "Epoch [26/200] Train Loss: 0.0722 Val Loss: 0.0720\n",
      "Epoch [27/200] Train Loss: 0.0718 Val Loss: 0.0720\n",
      "Epoch [28/200] Train Loss: 0.0711 Val Loss: 0.0718\n",
      "Epoch [29/200] Train Loss: 0.0711 Val Loss: 0.0713\n",
      "Epoch [30/200] Train Loss: 0.0705 Val Loss: 0.0709\n",
      "Epoch [31/200] Train Loss: 0.0702 Val Loss: 0.0707\n",
      "Epoch [32/200] Train Loss: 0.0698 Val Loss: 0.0709\n",
      "Epoch [33/200] Train Loss: 0.0695 Val Loss: 0.0712\n",
      "Epoch [34/200] Train Loss: 0.0693 Val Loss: 0.0710\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1178 Val Loss: 0.0929\n",
      "Epoch [2/200] Train Loss: 0.0944 Val Loss: 0.0923\n",
      "Epoch [3/200] Train Loss: 0.0937 Val Loss: 0.0915\n",
      "Epoch [4/200] Train Loss: 0.0930 Val Loss: 0.0908\n",
      "Epoch [5/200] Train Loss: 0.0924 Val Loss: 0.0901\n",
      "Epoch [6/200] Train Loss: 0.0917 Val Loss: 0.0894\n",
      "Epoch [7/200] Train Loss: 0.0910 Val Loss: 0.0884\n",
      "Epoch [8/200] Train Loss: 0.0902 Val Loss: 0.0880\n",
      "Epoch [9/200] Train Loss: 0.0894 Val Loss: 0.0868\n",
      "Epoch [10/200] Train Loss: 0.0886 Val Loss: 0.0860\n",
      "Epoch [11/200] Train Loss: 0.0879 Val Loss: 0.0865\n",
      "Epoch [12/200] Train Loss: 0.0871 Val Loss: 0.0849\n",
      "Epoch [13/200] Train Loss: 0.0865 Val Loss: 0.0840\n",
      "Epoch [14/200] Train Loss: 0.0857 Val Loss: 0.0834\n",
      "Epoch [15/200] Train Loss: 0.0850 Val Loss: 0.0829\n",
      "Epoch [16/200] Train Loss: 0.0844 Val Loss: 0.0824\n",
      "Epoch [17/200] Train Loss: 0.0835 Val Loss: 0.0823\n",
      "Epoch [18/200] Train Loss: 0.0829 Val Loss: 0.0815\n",
      "Epoch [19/200] Train Loss: 0.0823 Val Loss: 0.0807\n",
      "Epoch [20/200] Train Loss: 0.0816 Val Loss: 0.0804\n",
      "Epoch [21/200] Train Loss: 0.0809 Val Loss: 0.0798\n",
      "Epoch [22/200] Train Loss: 0.0802 Val Loss: 0.0792\n",
      "Epoch [23/200] Train Loss: 0.0797 Val Loss: 0.0795\n",
      "Epoch [24/200] Train Loss: 0.0788 Val Loss: 0.0782\n",
      "Epoch [25/200] Train Loss: 0.0785 Val Loss: 0.0790\n",
      "Epoch [26/200] Train Loss: 0.0780 Val Loss: 0.0784\n",
      "Epoch [27/200] Train Loss: 0.0774 Val Loss: 0.0775\n",
      "Epoch [28/200] Train Loss: 0.0771 Val Loss: 0.0774\n",
      "Epoch [29/200] Train Loss: 0.0762 Val Loss: 0.0759\n",
      "Epoch [30/200] Train Loss: 0.0760 Val Loss: 0.0756\n",
      "Epoch [31/200] Train Loss: 0.0754 Val Loss: 0.0752\n",
      "Epoch [32/200] Train Loss: 0.0749 Val Loss: 0.0751\n",
      "Epoch [33/200] Train Loss: 0.0742 Val Loss: 0.0747\n",
      "Epoch [34/200] Train Loss: 0.0739 Val Loss: 0.0745\n",
      "Epoch [35/200] Train Loss: 0.0738 Val Loss: 0.0738\n",
      "Epoch [36/200] Train Loss: 0.0730 Val Loss: 0.0734\n",
      "Epoch [37/200] Train Loss: 0.0727 Val Loss: 0.0734\n",
      "Epoch [38/200] Train Loss: 0.0722 Val Loss: 0.0735\n",
      "Epoch [39/200] Train Loss: 0.0720 Val Loss: 0.0728\n",
      "Epoch [40/200] Train Loss: 0.0711 Val Loss: 0.0724\n",
      "Epoch [41/200] Train Loss: 0.0708 Val Loss: 0.0717\n",
      "Epoch [42/200] Train Loss: 0.0706 Val Loss: 0.0713\n",
      "Epoch [43/200] Train Loss: 0.0700 Val Loss: 0.0726\n",
      "Epoch [44/200] Train Loss: 0.0695 Val Loss: 0.0707\n",
      "Epoch [45/200] Train Loss: 0.0693 Val Loss: 0.0707\n",
      "Epoch [46/200] Train Loss: 0.0688 Val Loss: 0.0718\n",
      "Epoch [47/200] Train Loss: 0.0680 Val Loss: 0.0702\n",
      "Epoch [48/200] Train Loss: 0.0681 Val Loss: 0.0702\n",
      "Epoch [49/200] Train Loss: 0.0673 Val Loss: 0.0704\n",
      "Epoch [50/200] Train Loss: 0.0668 Val Loss: 0.0695\n",
      "Epoch [51/200] Train Loss: 0.0664 Val Loss: 0.0688\n",
      "Epoch [52/200] Train Loss: 0.0661 Val Loss: 0.0695\n",
      "Epoch [53/200] Train Loss: 0.0656 Val Loss: 0.0683\n",
      "Epoch [54/200] Train Loss: 0.0654 Val Loss: 0.0695\n",
      "Epoch [55/200] Train Loss: 0.0650 Val Loss: 0.0675\n",
      "Epoch [56/200] Train Loss: 0.0646 Val Loss: 0.0674\n",
      "Epoch [57/200] Train Loss: 0.0648 Val Loss: 0.0677\n",
      "Epoch [58/200] Train Loss: 0.0636 Val Loss: 0.0674\n",
      "Epoch [59/200] Train Loss: 0.0636 Val Loss: 0.0708\n",
      "Epoch [60/200] Train Loss: 0.0631 Val Loss: 0.0661\n",
      "Epoch [61/200] Train Loss: 0.0625 Val Loss: 0.0658\n",
      "Epoch [62/200] Train Loss: 0.0622 Val Loss: 0.0656\n",
      "Epoch [63/200] Train Loss: 0.0620 Val Loss: 0.0654\n",
      "Epoch [64/200] Train Loss: 0.0615 Val Loss: 0.0667\n",
      "Epoch [65/200] Train Loss: 0.0613 Val Loss: 0.0652\n",
      "Epoch [66/200] Train Loss: 0.0603 Val Loss: 0.0649\n",
      "Epoch [67/200] Train Loss: 0.0601 Val Loss: 0.0648\n",
      "Epoch [68/200] Train Loss: 0.0600 Val Loss: 0.0642\n",
      "Epoch [69/200] Train Loss: 0.0596 Val Loss: 0.0638\n",
      "Epoch [70/200] Train Loss: 0.0589 Val Loss: 0.0636\n",
      "Epoch [71/200] Train Loss: 0.0589 Val Loss: 0.0631\n",
      "Epoch [72/200] Train Loss: 0.0585 Val Loss: 0.0630\n",
      "Epoch [73/200] Train Loss: 0.0580 Val Loss: 0.0633\n",
      "Epoch [74/200] Train Loss: 0.0578 Val Loss: 0.0627\n",
      "Epoch [75/200] Train Loss: 0.0575 Val Loss: 0.0642\n",
      "Epoch [76/200] Train Loss: 0.0581 Val Loss: 0.0615\n",
      "Epoch [77/200] Train Loss: 0.0565 Val Loss: 0.0613\n",
      "Epoch [78/200] Train Loss: 0.0570 Val Loss: 0.0620\n",
      "Epoch [79/200] Train Loss: 0.0559 Val Loss: 0.0610\n",
      "Epoch [80/200] Train Loss: 0.0558 Val Loss: 0.0615\n",
      "Epoch [81/200] Train Loss: 0.0555 Val Loss: 0.0612\n",
      "Epoch [82/200] Train Loss: 0.0548 Val Loss: 0.0603\n",
      "Epoch [83/200] Train Loss: 0.0548 Val Loss: 0.0601\n",
      "Epoch [84/200] Train Loss: 0.0542 Val Loss: 0.0601\n",
      "Epoch [85/200] Train Loss: 0.0539 Val Loss: 0.0596\n",
      "Epoch [86/200] Train Loss: 0.0538 Val Loss: 0.0596\n",
      "Epoch [87/200] Train Loss: 0.0532 Val Loss: 0.0588\n",
      "Epoch [88/200] Train Loss: 0.0526 Val Loss: 0.0588\n",
      "Epoch [89/200] Train Loss: 0.0524 Val Loss: 0.0582\n",
      "Epoch [90/200] Train Loss: 0.0523 Val Loss: 0.0574\n",
      "Epoch [91/200] Train Loss: 0.0519 Val Loss: 0.0571\n",
      "Epoch [92/200] Train Loss: 0.0517 Val Loss: 0.0569\n",
      "Epoch [93/200] Train Loss: 0.0512 Val Loss: 0.0571\n",
      "Epoch [94/200] Train Loss: 0.0515 Val Loss: 0.0570\n",
      "Epoch [95/200] Train Loss: 0.0507 Val Loss: 0.0560\n",
      "Epoch [96/200] Train Loss: 0.0503 Val Loss: 0.0562\n",
      "Epoch [97/200] Train Loss: 0.0498 Val Loss: 0.0557\n",
      "Epoch [98/200] Train Loss: 0.0495 Val Loss: 0.0560\n",
      "Epoch [99/200] Train Loss: 0.0494 Val Loss: 0.0560\n",
      "Epoch [100/200] Train Loss: 0.0491 Val Loss: 0.0553\n",
      "Epoch [101/200] Train Loss: 0.0485 Val Loss: 0.0543\n",
      "Epoch [102/200] Train Loss: 0.0484 Val Loss: 0.0539\n",
      "Epoch [103/200] Train Loss: 0.0484 Val Loss: 0.0557\n",
      "Epoch [104/200] Train Loss: 0.0474 Val Loss: 0.0539\n",
      "Epoch [105/200] Train Loss: 0.0469 Val Loss: 0.0527\n",
      "Epoch [106/200] Train Loss: 0.0469 Val Loss: 0.0521\n",
      "Epoch [107/200] Train Loss: 0.0465 Val Loss: 0.0534\n",
      "Epoch [108/200] Train Loss: 0.0465 Val Loss: 0.0521\n",
      "Epoch [109/200] Train Loss: 0.0462 Val Loss: 0.0516\n",
      "Epoch [110/200] Train Loss: 0.0455 Val Loss: 0.0515\n",
      "Epoch [111/200] Train Loss: 0.0458 Val Loss: 0.0513\n",
      "Epoch [112/200] Train Loss: 0.0455 Val Loss: 0.0509\n",
      "Epoch [113/200] Train Loss: 0.0448 Val Loss: 0.0510\n",
      "Epoch [114/200] Train Loss: 0.0446 Val Loss: 0.0511\n",
      "Epoch [115/200] Train Loss: 0.0443 Val Loss: 0.0513\n",
      "Epoch [116/200] Train Loss: 0.0442 Val Loss: 0.0532\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "server.local_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0640 Val Loss: 0.0645\n",
      "Epoch [2/20] Train Loss: 0.0636 Val Loss: 0.0642\n",
      "Epoch [3/20] Train Loss: 0.0633 Val Loss: 0.0640\n",
      "Epoch [4/20] Train Loss: 0.0631 Val Loss: 0.0641\n",
      "Epoch [5/20] Train Loss: 0.0629 Val Loss: 0.0637\n",
      "Epoch [6/20] Train Loss: 0.0628 Val Loss: 0.0636\n",
      "Epoch [7/20] Train Loss: 0.0627 Val Loss: 0.0637\n",
      "Epoch [8/20] Train Loss: 0.0625 Val Loss: 0.0635\n",
      "Epoch [9/20] Train Loss: 0.0625 Val Loss: 0.0634\n",
      "Epoch [10/20] Train Loss: 0.0624 Val Loss: 0.0633\n",
      "Epoch [11/20] Train Loss: 0.0623 Val Loss: 0.0632\n",
      "Epoch [12/20] Train Loss: 0.0622 Val Loss: 0.0632\n",
      "Epoch [13/20] Train Loss: 0.0622 Val Loss: 0.0631\n",
      "Epoch [14/20] Train Loss: 0.0621 Val Loss: 0.0630\n",
      "Epoch [15/20] Train Loss: 0.0620 Val Loss: 0.0630\n",
      "Epoch [16/20] Train Loss: 0.0619 Val Loss: 0.0630\n",
      "Epoch [17/20] Train Loss: 0.0618 Val Loss: 0.0630\n",
      "Epoch [18/20] Train Loss: 0.0618 Val Loss: 0.0627\n",
      "Epoch [19/20] Train Loss: 0.0617 Val Loss: 0.0628\n",
      "Epoch [20/20] Train Loss: 0.0616 Val Loss: 0.0627\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0760 Val Loss: 0.0761\n",
      "Epoch [2/20] Train Loss: 0.0756 Val Loss: 0.0756\n",
      "Epoch [3/20] Train Loss: 0.0752 Val Loss: 0.0753\n",
      "Epoch [4/20] Train Loss: 0.0750 Val Loss: 0.0751\n",
      "Epoch [5/20] Train Loss: 0.0748 Val Loss: 0.0750\n",
      "Epoch [6/20] Train Loss: 0.0747 Val Loss: 0.0749\n",
      "Epoch [7/20] Train Loss: 0.0746 Val Loss: 0.0748\n",
      "Epoch [8/20] Train Loss: 0.0744 Val Loss: 0.0747\n",
      "Epoch [9/20] Train Loss: 0.0743 Val Loss: 0.0746\n",
      "Epoch [10/20] Train Loss: 0.0742 Val Loss: 0.0745\n",
      "Epoch [11/20] Train Loss: 0.0740 Val Loss: 0.0745\n",
      "Epoch [12/20] Train Loss: 0.0739 Val Loss: 0.0746\n",
      "Epoch [13/20] Train Loss: 0.0738 Val Loss: 0.0743\n",
      "Epoch [14/20] Train Loss: 0.0738 Val Loss: 0.0742\n",
      "Epoch [15/20] Train Loss: 0.0736 Val Loss: 0.0741\n",
      "Epoch [16/20] Train Loss: 0.0735 Val Loss: 0.0741\n",
      "Epoch [17/20] Train Loss: 0.0734 Val Loss: 0.0739\n",
      "Epoch [18/20] Train Loss: 0.0733 Val Loss: 0.0739\n",
      "Epoch [19/20] Train Loss: 0.0732 Val Loss: 0.0738\n",
      "Epoch [20/20] Train Loss: 0.0732 Val Loss: 0.0737\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0863 Val Loss: 0.0872\n",
      "Epoch [2/20] Train Loss: 0.0854 Val Loss: 0.0867\n",
      "Epoch [3/20] Train Loss: 0.0848 Val Loss: 0.0861\n",
      "Epoch [4/20] Train Loss: 0.0843 Val Loss: 0.0858\n",
      "Epoch [5/20] Train Loss: 0.0840 Val Loss: 0.0855\n",
      "Epoch [6/20] Train Loss: 0.0838 Val Loss: 0.0853\n",
      "Epoch [7/20] Train Loss: 0.0835 Val Loss: 0.0852\n",
      "Epoch [8/20] Train Loss: 0.0833 Val Loss: 0.0852\n",
      "Epoch [9/20] Train Loss: 0.0832 Val Loss: 0.0848\n",
      "Epoch [10/20] Train Loss: 0.0831 Val Loss: 0.0848\n",
      "Epoch [11/20] Train Loss: 0.0830 Val Loss: 0.0847\n",
      "Epoch [12/20] Train Loss: 0.0828 Val Loss: 0.0848\n",
      "Epoch [13/20] Train Loss: 0.0827 Val Loss: 0.0845\n",
      "Epoch [14/20] Train Loss: 0.0826 Val Loss: 0.0844\n",
      "Epoch [15/20] Train Loss: 0.0826 Val Loss: 0.0843\n",
      "Epoch [16/20] Train Loss: 0.0824 Val Loss: 0.0842\n",
      "Epoch [17/20] Train Loss: 0.0824 Val Loss: 0.0841\n",
      "Epoch [18/20] Train Loss: 0.0822 Val Loss: 0.0841\n",
      "Epoch [19/20] Train Loss: 0.0821 Val Loss: 0.0841\n",
      "Epoch [20/20] Train Loss: 0.0820 Val Loss: 0.0839\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0723 Val Loss: 0.0727\n",
      "Epoch [2/20] Train Loss: 0.0716 Val Loss: 0.0721\n",
      "Epoch [3/20] Train Loss: 0.0713 Val Loss: 0.0724\n",
      "Epoch [4/20] Train Loss: 0.0711 Val Loss: 0.0719\n",
      "Epoch [5/20] Train Loss: 0.0710 Val Loss: 0.0717\n",
      "Epoch [6/20] Train Loss: 0.0709 Val Loss: 0.0716\n",
      "Epoch [7/20] Train Loss: 0.0708 Val Loss: 0.0715\n",
      "Epoch [8/20] Train Loss: 0.0707 Val Loss: 0.0716\n",
      "Epoch [9/20] Train Loss: 0.0706 Val Loss: 0.0714\n",
      "Epoch [10/20] Train Loss: 0.0705 Val Loss: 0.0714\n",
      "Epoch [11/20] Train Loss: 0.0704 Val Loss: 0.0715\n",
      "Epoch [12/20] Train Loss: 0.0703 Val Loss: 0.0713\n",
      "Epoch [13/20] Train Loss: 0.0702 Val Loss: 0.0713\n",
      "Epoch [14/20] Train Loss: 0.0702 Val Loss: 0.0711\n",
      "Epoch [15/20] Train Loss: 0.0701 Val Loss: 0.0711\n",
      "Epoch [16/20] Train Loss: 0.0700 Val Loss: 0.0714\n",
      "Epoch [17/20] Train Loss: 0.0699 Val Loss: 0.0710\n",
      "Epoch [18/20] Train Loss: 0.0698 Val Loss: 0.0709\n",
      "Epoch [19/20] Train Loss: 0.0697 Val Loss: 0.0709\n",
      "Epoch [20/20] Train Loss: 0.0697 Val Loss: 0.0710\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0742 Val Loss: 0.0754\n",
      "Epoch [2/20] Train Loss: 0.0739 Val Loss: 0.0751\n",
      "Epoch [3/20] Train Loss: 0.0735 Val Loss: 0.0749\n",
      "Epoch [4/20] Train Loss: 0.0733 Val Loss: 0.0747\n",
      "Epoch [5/20] Train Loss: 0.0731 Val Loss: 0.0745\n",
      "Epoch [6/20] Train Loss: 0.0729 Val Loss: 0.0744\n",
      "Epoch [7/20] Train Loss: 0.0727 Val Loss: 0.0744\n",
      "Epoch [8/20] Train Loss: 0.0726 Val Loss: 0.0742\n",
      "Epoch [9/20] Train Loss: 0.0725 Val Loss: 0.0741\n",
      "Epoch [10/20] Train Loss: 0.0723 Val Loss: 0.0740\n",
      "Epoch [11/20] Train Loss: 0.0722 Val Loss: 0.0739\n",
      "Epoch [12/20] Train Loss: 0.0721 Val Loss: 0.0738\n",
      "Epoch [13/20] Train Loss: 0.0719 Val Loss: 0.0737\n",
      "Epoch [14/20] Train Loss: 0.0719 Val Loss: 0.0736\n",
      "Epoch [15/20] Train Loss: 0.0718 Val Loss: 0.0735\n",
      "Epoch [16/20] Train Loss: 0.0716 Val Loss: 0.0735\n",
      "Epoch [17/20] Train Loss: 0.0715 Val Loss: 0.0733\n",
      "Epoch [18/20] Train Loss: 0.0714 Val Loss: 0.0733\n",
      "Epoch [19/20] Train Loss: 0.0713 Val Loss: 0.0732\n",
      "Epoch [20/20] Train Loss: 0.0712 Val Loss: 0.0736\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0687 Val Loss: 0.0695\n",
      "Epoch [2/20] Train Loss: 0.0687 Val Loss: 0.0693\n",
      "Epoch [3/20] Train Loss: 0.0685 Val Loss: 0.0693\n",
      "Epoch [4/20] Train Loss: 0.0684 Val Loss: 0.0692\n",
      "Epoch [5/20] Train Loss: 0.0682 Val Loss: 0.0691\n",
      "Epoch [6/20] Train Loss: 0.0682 Val Loss: 0.0691\n",
      "Epoch [7/20] Train Loss: 0.0681 Val Loss: 0.0692\n",
      "Epoch [8/20] Train Loss: 0.0680 Val Loss: 0.0691\n",
      "Epoch [9/20] Train Loss: 0.0679 Val Loss: 0.0691\n",
      "Epoch [10/20] Train Loss: 0.0678 Val Loss: 0.0691\n",
      "Epoch [11/20] Train Loss: 0.0678 Val Loss: 0.0688\n",
      "Epoch [12/20] Train Loss: 0.0676 Val Loss: 0.0687\n",
      "Epoch [13/20] Train Loss: 0.0676 Val Loss: 0.0686\n",
      "Epoch [14/20] Train Loss: 0.0675 Val Loss: 0.0687\n",
      "Epoch [15/20] Train Loss: 0.0674 Val Loss: 0.0689\n",
      "Epoch [16/20] Train Loss: 0.0674 Val Loss: 0.0687\n",
      "Epoch [17/20] Train Loss: 0.0673 Val Loss: 0.0685\n",
      "Epoch [18/20] Train Loss: 0.0672 Val Loss: 0.0685\n",
      "Epoch [19/20] Train Loss: 0.0672 Val Loss: 0.0685\n",
      "Epoch [20/20] Train Loss: 0.0671 Val Loss: 0.0685\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0758 Val Loss: 0.0757\n",
      "Epoch [2/20] Train Loss: 0.0756 Val Loss: 0.0756\n",
      "Epoch [3/20] Train Loss: 0.0755 Val Loss: 0.0753\n",
      "Epoch [4/20] Train Loss: 0.0753 Val Loss: 0.0753\n",
      "Epoch [5/20] Train Loss: 0.0751 Val Loss: 0.0754\n",
      "Epoch [6/20] Train Loss: 0.0750 Val Loss: 0.0751\n",
      "Epoch [7/20] Train Loss: 0.0749 Val Loss: 0.0754\n",
      "Epoch [8/20] Train Loss: 0.0748 Val Loss: 0.0750\n",
      "Epoch [9/20] Train Loss: 0.0748 Val Loss: 0.0749\n",
      "Epoch [10/20] Train Loss: 0.0746 Val Loss: 0.0749\n",
      "Epoch [11/20] Train Loss: 0.0745 Val Loss: 0.0747\n",
      "Epoch [12/20] Train Loss: 0.0744 Val Loss: 0.0748\n",
      "Epoch [13/20] Train Loss: 0.0743 Val Loss: 0.0747\n",
      "Epoch [14/20] Train Loss: 0.0743 Val Loss: 0.0745\n",
      "Epoch [15/20] Train Loss: 0.0741 Val Loss: 0.0747\n",
      "Epoch [16/20] Train Loss: 0.0741 Val Loss: 0.0744\n",
      "Epoch [17/20] Train Loss: 0.0740 Val Loss: 0.0744\n",
      "Epoch [18/20] Train Loss: 0.0739 Val Loss: 0.0743\n",
      "Epoch [19/20] Train Loss: 0.0738 Val Loss: 0.0742\n",
      "Epoch [20/20] Train Loss: 0.0738 Val Loss: 0.0745\n",
      "[0.07415353316711644, 0.08799371513703914, 0.08981677175384678, 0.08206316067084465, 0.08298212394424498, 0.08085841480132243, 0.08618789921476416]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_losses=[]\n",
    "local_fine_tune_preds=[]\n",
    "local_fine_tune_models=[]\n",
    "seed_everything(1)\n",
    "for i in range(args_train.number_clients):\n",
    "    local_fine_tune_pred,local_fine_tune_loss,local_fine_tune_model=clients[i].local_fine_tune(fine_tune_epochs=20)\n",
    "    local_fine_tune_losses.append(local_fine_tune_loss)\n",
    "    local_fine_tune_preds.append(local_fine_tune_pred)\n",
    "    local_fine_tune_models.append(local_fine_tune_model)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import utils import plot_prob_result\n",
    "args_temp=copy.deepcopy(args_train)\n",
    "args_temp.dataset_paths='wf7'\n",
    "test_data, test_loader = get_data(args_train,flag='test')\n",
    "actual_y=[]\n",
    "for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "    actual_y.append(seq_y)\n",
    "actual_y = torch.cat([torch.flatten(t) for t in actual_y])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07414915443283238, 0.08909740545212814, 0.09162906327680366, 0.08211914724223826, 0.08759168286693014, 0.07979674568425303, 0.08564044110323876]\n",
      "0.08428909143691776\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)\n",
    "print(np.mean(fed_local_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08041695084371796, 0.08744908469946008, 0.09325712222656975, 0.08830002137804277, 0.07983406686721599, 0.08646285887297293, 0.09836889615869276]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    local_pred,local_loss,local_model=clients[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07551540075865103, 0.09200877592937179, 0.09616044051434895, 0.08540064513632288, 0.08885041996836662, 0.0840047773050323, 0.08917854520913264]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    central_pred,central_loss,central_model=server.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08041695084371796, 0.08744908469946008, 0.09325712222656975, 0.08830002137804277, 0.07983406686721599, 0.08646285887297293, 0.09836889615869276]\n",
      "[0.07551540075865103, 0.09200877592937179, 0.09616044051434895, 0.08540064513632288, 0.08885041996836662, 0.0840047773050323, 0.08917854520913264]\n",
      "[0.07414915443283238, 0.08909740545212814, 0.09162906327680366, 0.08211914724223826, 0.08759168286693014, 0.07979674568425303, 0.08564044110323876]\n",
      "[0.07415353316711644, 0.08799371513703914, 0.08981677175384678, 0.08206316067084465, 0.08298212394424498, 0.08085841480132243, 0.08618789921476416]\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_losses': local_fine_tune_losses\n",
    "})\n",
    "df.T.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the server object\n",
    "with open('../result/24/server_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "# Save the clients object\n",
    "with open('../result/24/clients_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save server and clients\n",
    "with open('server.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "with open('clients.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)\n",
    "\n",
    "# Load server and clients\n",
    "with open('server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "with open('clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
