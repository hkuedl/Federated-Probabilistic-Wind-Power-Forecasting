{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "from utils import get_data, plot_prob_result,seed_everything\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# Load the server object\n",
    "scale=12\n",
    "\n",
    "with open('../result/'+str(scale)+'/server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07019277994380627, 0.06390050535205088, 0.07960541857636139, 0.07376481334946744, 0.07278922048384605, 0.06719664158299565, 0.08431435201623261]\n"
     ]
    }
   ],
   "source": [
    "fed_local_proposed_losses=[]\n",
    "fed_local_proposed_preds=[]\n",
    "fed_local_proposed_models=[]\n",
    "for i in range(7):\n",
    "    fed_local_proposed_pred,fed_local_proposed_loss,fed_local_proposed_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_proposed_losses.append(fed_local_proposed_loss[server.index_set[i]])\n",
    "    fed_local_proposed_preds.append(fed_local_proposed_pred[server.index_set[i]])\n",
    "    fed_local_proposed_models.append(fed_local_proposed_model[server.index_set[i]])\n",
    "print(fed_local_proposed_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0454 Val Loss: 0.0454\n",
      "Epoch [2/20] Train Loss: 0.0451 Val Loss: 0.0451\n",
      "Epoch [3/20] Train Loss: 0.0449 Val Loss: 0.0450\n",
      "Epoch [4/20] Train Loss: 0.0448 Val Loss: 0.0450\n",
      "Epoch [5/20] Train Loss: 0.0448 Val Loss: 0.0453\n",
      "Epoch [6/20] Train Loss: 0.0447 Val Loss: 0.0449\n",
      "Epoch [7/20] Train Loss: 0.0446 Val Loss: 0.0448\n",
      "Epoch [8/20] Train Loss: 0.0446 Val Loss: 0.0449\n",
      "Epoch [9/20] Train Loss: 0.0446 Val Loss: 0.0448\n",
      "Epoch [10/20] Train Loss: 0.0446 Val Loss: 0.0447\n",
      "Epoch [11/20] Train Loss: 0.0445 Val Loss: 0.0447\n",
      "Epoch [12/20] Train Loss: 0.0445 Val Loss: 0.0446\n",
      "Epoch [13/20] Train Loss: 0.0445 Val Loss: 0.0447\n",
      "Epoch [14/20] Train Loss: 0.0445 Val Loss: 0.0448\n",
      "Epoch [15/20] Train Loss: 0.0445 Val Loss: 0.0446\n",
      "Epoch [16/20] Train Loss: 0.0445 Val Loss: 0.0446\n",
      "Epoch [17/20] Train Loss: 0.0444 Val Loss: 0.0445\n",
      "Epoch [18/20] Train Loss: 0.0444 Val Loss: 0.0446\n",
      "Epoch [19/20] Train Loss: 0.0444 Val Loss: 0.0446\n",
      "Epoch [20/20] Train Loss: 0.0444 Val Loss: 0.0447\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0469 Val Loss: 0.0463\n",
      "Epoch [2/20] Train Loss: 0.0463 Val Loss: 0.0462\n",
      "Epoch [3/20] Train Loss: 0.0460 Val Loss: 0.0458\n",
      "Epoch [4/20] Train Loss: 0.0458 Val Loss: 0.0456\n",
      "Epoch [5/20] Train Loss: 0.0458 Val Loss: 0.0455\n",
      "Epoch [6/20] Train Loss: 0.0457 Val Loss: 0.0460\n",
      "Epoch [7/20] Train Loss: 0.0456 Val Loss: 0.0455\n",
      "Epoch [8/20] Train Loss: 0.0455 Val Loss: 0.0454\n",
      "Epoch [9/20] Train Loss: 0.0455 Val Loss: 0.0454\n",
      "Epoch [10/20] Train Loss: 0.0454 Val Loss: 0.0453\n",
      "Epoch [11/20] Train Loss: 0.0454 Val Loss: 0.0454\n",
      "Epoch [12/20] Train Loss: 0.0454 Val Loss: 0.0452\n",
      "Epoch [13/20] Train Loss: 0.0453 Val Loss: 0.0454\n",
      "Epoch [14/20] Train Loss: 0.0453 Val Loss: 0.0452\n",
      "Epoch [15/20] Train Loss: 0.0453 Val Loss: 0.0453\n",
      "Epoch [16/20] Train Loss: 0.0453 Val Loss: 0.0452\n",
      "Epoch [17/20] Train Loss: 0.0453 Val Loss: 0.0451\n",
      "Epoch [18/20] Train Loss: 0.0453 Val Loss: 0.0450\n",
      "Epoch [19/20] Train Loss: 0.0453 Val Loss: 0.0452\n",
      "Epoch [20/20] Train Loss: 0.0452 Val Loss: 0.0453\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0538 Val Loss: 0.0531\n",
      "Epoch [2/20] Train Loss: 0.0531 Val Loss: 0.0528\n",
      "Epoch [3/20] Train Loss: 0.0529 Val Loss: 0.0527\n",
      "Epoch [4/20] Train Loss: 0.0528 Val Loss: 0.0526\n",
      "Epoch [5/20] Train Loss: 0.0526 Val Loss: 0.0524\n",
      "Epoch [6/20] Train Loss: 0.0526 Val Loss: 0.0523\n",
      "Epoch [7/20] Train Loss: 0.0526 Val Loss: 0.0524\n",
      "Epoch [8/20] Train Loss: 0.0525 Val Loss: 0.0525\n",
      "Epoch [9/20] Train Loss: 0.0525 Val Loss: 0.0522\n",
      "Epoch [10/20] Train Loss: 0.0524 Val Loss: 0.0521\n",
      "Epoch [11/20] Train Loss: 0.0524 Val Loss: 0.0521\n",
      "Epoch [12/20] Train Loss: 0.0524 Val Loss: 0.0522\n",
      "Epoch [13/20] Train Loss: 0.0524 Val Loss: 0.0521\n",
      "Epoch [14/20] Train Loss: 0.0524 Val Loss: 0.0520\n",
      "Epoch [15/20] Train Loss: 0.0523 Val Loss: 0.0521\n",
      "Epoch [16/20] Train Loss: 0.0523 Val Loss: 0.0520\n",
      "Epoch [17/20] Train Loss: 0.0523 Val Loss: 0.0520\n",
      "Epoch [18/20] Train Loss: 0.0522 Val Loss: 0.0519\n",
      "Epoch [19/20] Train Loss: 0.0523 Val Loss: 0.0523\n",
      "Epoch [20/20] Train Loss: 0.0523 Val Loss: 0.0520\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0452 Val Loss: 0.0456\n",
      "Epoch [2/20] Train Loss: 0.0439 Val Loss: 0.0450\n",
      "Epoch [3/20] Train Loss: 0.0437 Val Loss: 0.0450\n",
      "Epoch [4/20] Train Loss: 0.0436 Val Loss: 0.0450\n",
      "Epoch [5/20] Train Loss: 0.0436 Val Loss: 0.0457\n",
      "Epoch [6/20] Train Loss: 0.0435 Val Loss: 0.0449\n",
      "Epoch [7/20] Train Loss: 0.0435 Val Loss: 0.0448\n",
      "Epoch [8/20] Train Loss: 0.0434 Val Loss: 0.0448\n",
      "Epoch [9/20] Train Loss: 0.0434 Val Loss: 0.0450\n",
      "Epoch [10/20] Train Loss: 0.0434 Val Loss: 0.0446\n",
      "Epoch [11/20] Train Loss: 0.0433 Val Loss: 0.0450\n",
      "Epoch [12/20] Train Loss: 0.0435 Val Loss: 0.0448\n",
      "Epoch [13/20] Train Loss: 0.0433 Val Loss: 0.0446\n",
      "Epoch [14/20] Train Loss: 0.0434 Val Loss: 0.0446\n",
      "Epoch [15/20] Train Loss: 0.0433 Val Loss: 0.0449\n",
      "Epoch [16/20] Train Loss: 0.0433 Val Loss: 0.0449\n",
      "Epoch [17/20] Train Loss: 0.0434 Val Loss: 0.0446\n",
      "Epoch [18/20] Train Loss: 0.0432 Val Loss: 0.0447\n",
      "Epoch [19/20] Train Loss: 0.0434 Val Loss: 0.0445\n",
      "Epoch [20/20] Train Loss: 0.0433 Val Loss: 0.0447\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0500 Val Loss: 0.0494\n",
      "Epoch [2/20] Train Loss: 0.0493 Val Loss: 0.0488\n",
      "Epoch [3/20] Train Loss: 0.0490 Val Loss: 0.0486\n",
      "Epoch [4/20] Train Loss: 0.0489 Val Loss: 0.0486\n",
      "Epoch [5/20] Train Loss: 0.0488 Val Loss: 0.0485\n",
      "Epoch [6/20] Train Loss: 0.0488 Val Loss: 0.0486\n",
      "Epoch [7/20] Train Loss: 0.0487 Val Loss: 0.0485\n",
      "Epoch [8/20] Train Loss: 0.0487 Val Loss: 0.0483\n",
      "Epoch [9/20] Train Loss: 0.0486 Val Loss: 0.0482\n",
      "Epoch [10/20] Train Loss: 0.0486 Val Loss: 0.0483\n",
      "Epoch [11/20] Train Loss: 0.0486 Val Loss: 0.0483\n",
      "Epoch [12/20] Train Loss: 0.0486 Val Loss: 0.0480\n",
      "Epoch [13/20] Train Loss: 0.0486 Val Loss: 0.0485\n",
      "Epoch [14/20] Train Loss: 0.0486 Val Loss: 0.0481\n",
      "Epoch [15/20] Train Loss: 0.0485 Val Loss: 0.0483\n",
      "Epoch [16/20] Train Loss: 0.0486 Val Loss: 0.0481\n",
      "Epoch [17/20] Train Loss: 0.0486 Val Loss: 0.0480\n",
      "Epoch [18/20] Train Loss: 0.0485 Val Loss: 0.0483\n",
      "Epoch [19/20] Train Loss: 0.0485 Val Loss: 0.0481\n",
      "Epoch [20/20] Train Loss: 0.0485 Val Loss: 0.0479\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0388 Val Loss: 0.0409\n",
      "Epoch [2/20] Train Loss: 0.0387 Val Loss: 0.0411\n",
      "Epoch [3/20] Train Loss: 0.0386 Val Loss: 0.0409\n",
      "Epoch [4/20] Train Loss: 0.0385 Val Loss: 0.0408\n",
      "Epoch [5/20] Train Loss: 0.0385 Val Loss: 0.0406\n",
      "Epoch [6/20] Train Loss: 0.0385 Val Loss: 0.0407\n",
      "Epoch [7/20] Train Loss: 0.0385 Val Loss: 0.0407\n",
      "Epoch [8/20] Train Loss: 0.0383 Val Loss: 0.0406\n",
      "Epoch [9/20] Train Loss: 0.0384 Val Loss: 0.0407\n",
      "Epoch [10/20] Train Loss: 0.0383 Val Loss: 0.0405\n",
      "Epoch [11/20] Train Loss: 0.0383 Val Loss: 0.0405\n",
      "Epoch [12/20] Train Loss: 0.0383 Val Loss: 0.0405\n",
      "Epoch [13/20] Train Loss: 0.0383 Val Loss: 0.0404\n",
      "Epoch [14/20] Train Loss: 0.0383 Val Loss: 0.0405\n",
      "Epoch [15/20] Train Loss: 0.0383 Val Loss: 0.0404\n",
      "Epoch [16/20] Train Loss: 0.0383 Val Loss: 0.0404\n",
      "Epoch [17/20] Train Loss: 0.0382 Val Loss: 0.0404\n",
      "Epoch [18/20] Train Loss: 0.0382 Val Loss: 0.0404\n",
      "Epoch [19/20] Train Loss: 0.0382 Val Loss: 0.0405\n",
      "Epoch [20/20] Train Loss: 0.0382 Val Loss: 0.0404\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0434 Val Loss: 0.0449\n",
      "Epoch [2/20] Train Loss: 0.0428 Val Loss: 0.0445\n",
      "Epoch [3/20] Train Loss: 0.0426 Val Loss: 0.0446\n",
      "Epoch [4/20] Train Loss: 0.0425 Val Loss: 0.0445\n",
      "Epoch [5/20] Train Loss: 0.0425 Val Loss: 0.0442\n",
      "Epoch [6/20] Train Loss: 0.0425 Val Loss: 0.0442\n",
      "Epoch [7/20] Train Loss: 0.0424 Val Loss: 0.0442\n",
      "Epoch [8/20] Train Loss: 0.0424 Val Loss: 0.0443\n",
      "Epoch [9/20] Train Loss: 0.0424 Val Loss: 0.0443\n",
      "Epoch [10/20] Train Loss: 0.0423 Val Loss: 0.0442\n",
      "Epoch [11/20] Train Loss: 0.0423 Val Loss: 0.0443\n",
      "Epoch [12/20] Train Loss: 0.0424 Val Loss: 0.0444\n",
      "Epoch [13/20] Train Loss: 0.0423 Val Loss: 0.0446\n",
      "Early stopping\n",
      "[0.06422739151916275, 0.06387627024637306, 0.07292083787974225, 0.07075107832477517, 0.06966070825718854, 0.06795490283697639, 0.07290313319519978]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_ewc_losses=[]\n",
    "local_fine_tune_ewc_preds=[]\n",
    "local_fine_tune_ewc_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_ewc_pred,local_fine_tune_ewc_loss,local_fine_tune_ewc_model=clients[i].local_fine_tune(server.index_set[i],ewc_flag=True,importance=0.02,fine_tune_epochs=20)\n",
    "    local_fine_tune_ewc_losses.append(local_fine_tune_ewc_loss)\n",
    "    local_fine_tune_ewc_preds.append(local_fine_tune_ewc_pred)\n",
    "    local_fine_tune_ewc_models.append(local_fine_tune_ewc_model)\n",
    "print(local_fine_tune_ewc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0453 Val Loss: 0.0455\n",
      "Epoch [2/20] Train Loss: 0.0448 Val Loss: 0.0452\n",
      "Epoch [3/20] Train Loss: 0.0444 Val Loss: 0.0449\n",
      "Epoch [4/20] Train Loss: 0.0442 Val Loss: 0.0449\n",
      "Epoch [5/20] Train Loss: 0.0440 Val Loss: 0.0447\n",
      "Epoch [6/20] Train Loss: 0.0439 Val Loss: 0.0445\n",
      "Epoch [7/20] Train Loss: 0.0437 Val Loss: 0.0444\n",
      "Epoch [8/20] Train Loss: 0.0436 Val Loss: 0.0445\n",
      "Epoch [9/20] Train Loss: 0.0435 Val Loss: 0.0443\n",
      "Epoch [10/20] Train Loss: 0.0433 Val Loss: 0.0442\n",
      "Epoch [11/20] Train Loss: 0.0432 Val Loss: 0.0441\n",
      "Epoch [12/20] Train Loss: 0.0432 Val Loss: 0.0442\n",
      "Epoch [13/20] Train Loss: 0.0431 Val Loss: 0.0443\n",
      "Epoch [14/20] Train Loss: 0.0430 Val Loss: 0.0440\n",
      "Epoch [15/20] Train Loss: 0.0429 Val Loss: 0.0440\n",
      "Epoch [16/20] Train Loss: 0.0428 Val Loss: 0.0438\n",
      "Epoch [17/20] Train Loss: 0.0428 Val Loss: 0.0437\n",
      "Epoch [18/20] Train Loss: 0.0426 Val Loss: 0.0439\n",
      "Epoch [19/20] Train Loss: 0.0427 Val Loss: 0.0437\n",
      "Epoch [20/20] Train Loss: 0.0425 Val Loss: 0.0436\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0469 Val Loss: 0.0463\n",
      "Epoch [2/20] Train Loss: 0.0461 Val Loss: 0.0463\n",
      "Epoch [3/20] Train Loss: 0.0456 Val Loss: 0.0457\n",
      "Epoch [4/20] Train Loss: 0.0454 Val Loss: 0.0455\n",
      "Epoch [5/20] Train Loss: 0.0452 Val Loss: 0.0454\n",
      "Epoch [6/20] Train Loss: 0.0449 Val Loss: 0.0453\n",
      "Epoch [7/20] Train Loss: 0.0448 Val Loss: 0.0456\n",
      "Epoch [8/20] Train Loss: 0.0447 Val Loss: 0.0454\n",
      "Epoch [9/20] Train Loss: 0.0445 Val Loss: 0.0451\n",
      "Epoch [10/20] Train Loss: 0.0444 Val Loss: 0.0452\n",
      "Epoch [11/20] Train Loss: 0.0443 Val Loss: 0.0448\n",
      "Epoch [12/20] Train Loss: 0.0441 Val Loss: 0.0447\n",
      "Epoch [13/20] Train Loss: 0.0440 Val Loss: 0.0448\n",
      "Epoch [14/20] Train Loss: 0.0440 Val Loss: 0.0447\n",
      "Epoch [15/20] Train Loss: 0.0438 Val Loss: 0.0445\n",
      "Epoch [16/20] Train Loss: 0.0438 Val Loss: 0.0445\n",
      "Epoch [17/20] Train Loss: 0.0436 Val Loss: 0.0444\n",
      "Epoch [18/20] Train Loss: 0.0436 Val Loss: 0.0444\n",
      "Epoch [19/20] Train Loss: 0.0435 Val Loss: 0.0443\n",
      "Epoch [20/20] Train Loss: 0.0434 Val Loss: 0.0444\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0535 Val Loss: 0.0531\n",
      "Epoch [2/20] Train Loss: 0.0525 Val Loss: 0.0524\n",
      "Epoch [3/20] Train Loss: 0.0520 Val Loss: 0.0524\n",
      "Epoch [4/20] Train Loss: 0.0517 Val Loss: 0.0522\n",
      "Epoch [5/20] Train Loss: 0.0514 Val Loss: 0.0521\n",
      "Epoch [6/20] Train Loss: 0.0512 Val Loss: 0.0518\n",
      "Epoch [7/20] Train Loss: 0.0510 Val Loss: 0.0516\n",
      "Epoch [8/20] Train Loss: 0.0509 Val Loss: 0.0515\n",
      "Epoch [9/20] Train Loss: 0.0507 Val Loss: 0.0517\n",
      "Epoch [10/20] Train Loss: 0.0506 Val Loss: 0.0513\n",
      "Epoch [11/20] Train Loss: 0.0504 Val Loss: 0.0512\n",
      "Epoch [12/20] Train Loss: 0.0502 Val Loss: 0.0511\n",
      "Epoch [13/20] Train Loss: 0.0501 Val Loss: 0.0512\n",
      "Epoch [14/20] Train Loss: 0.0500 Val Loss: 0.0510\n",
      "Epoch [15/20] Train Loss: 0.0499 Val Loss: 0.0508\n",
      "Epoch [16/20] Train Loss: 0.0498 Val Loss: 0.0508\n",
      "Epoch [17/20] Train Loss: 0.0497 Val Loss: 0.0506\n",
      "Epoch [18/20] Train Loss: 0.0496 Val Loss: 0.0506\n",
      "Epoch [19/20] Train Loss: 0.0494 Val Loss: 0.0505\n",
      "Epoch [20/20] Train Loss: 0.0494 Val Loss: 0.0505\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0445 Val Loss: 0.0451\n",
      "Epoch [2/20] Train Loss: 0.0430 Val Loss: 0.0450\n",
      "Epoch [3/20] Train Loss: 0.0427 Val Loss: 0.0448\n",
      "Epoch [4/20] Train Loss: 0.0424 Val Loss: 0.0446\n",
      "Epoch [5/20] Train Loss: 0.0423 Val Loss: 0.0446\n",
      "Epoch [6/20] Train Loss: 0.0421 Val Loss: 0.0444\n",
      "Epoch [7/20] Train Loss: 0.0420 Val Loss: 0.0444\n",
      "Epoch [8/20] Train Loss: 0.0419 Val Loss: 0.0443\n",
      "Epoch [9/20] Train Loss: 0.0418 Val Loss: 0.0443\n",
      "Epoch [10/20] Train Loss: 0.0417 Val Loss: 0.0444\n",
      "Epoch [11/20] Train Loss: 0.0417 Val Loss: 0.0440\n",
      "Epoch [12/20] Train Loss: 0.0416 Val Loss: 0.0441\n",
      "Epoch [13/20] Train Loss: 0.0414 Val Loss: 0.0440\n",
      "Epoch [14/20] Train Loss: 0.0414 Val Loss: 0.0439\n",
      "Epoch [15/20] Train Loss: 0.0413 Val Loss: 0.0438\n",
      "Epoch [16/20] Train Loss: 0.0413 Val Loss: 0.0439\n",
      "Epoch [17/20] Train Loss: 0.0412 Val Loss: 0.0438\n",
      "Epoch [18/20] Train Loss: 0.0411 Val Loss: 0.0437\n",
      "Epoch [19/20] Train Loss: 0.0411 Val Loss: 0.0439\n",
      "Epoch [20/20] Train Loss: 0.0410 Val Loss: 0.0438\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0496 Val Loss: 0.0489\n",
      "Epoch [2/20] Train Loss: 0.0484 Val Loss: 0.0484\n",
      "Epoch [3/20] Train Loss: 0.0479 Val Loss: 0.0480\n",
      "Epoch [4/20] Train Loss: 0.0475 Val Loss: 0.0480\n",
      "Epoch [5/20] Train Loss: 0.0472 Val Loss: 0.0476\n",
      "Epoch [6/20] Train Loss: 0.0470 Val Loss: 0.0474\n",
      "Epoch [7/20] Train Loss: 0.0468 Val Loss: 0.0473\n",
      "Epoch [8/20] Train Loss: 0.0466 Val Loss: 0.0473\n",
      "Epoch [9/20] Train Loss: 0.0465 Val Loss: 0.0475\n",
      "Epoch [10/20] Train Loss: 0.0464 Val Loss: 0.0471\n",
      "Epoch [11/20] Train Loss: 0.0463 Val Loss: 0.0471\n",
      "Epoch [12/20] Train Loss: 0.0462 Val Loss: 0.0470\n",
      "Epoch [13/20] Train Loss: 0.0460 Val Loss: 0.0468\n",
      "Epoch [14/20] Train Loss: 0.0459 Val Loss: 0.0467\n",
      "Epoch [15/20] Train Loss: 0.0458 Val Loss: 0.0466\n",
      "Epoch [16/20] Train Loss: 0.0457 Val Loss: 0.0466\n",
      "Epoch [17/20] Train Loss: 0.0457 Val Loss: 0.0469\n",
      "Epoch [18/20] Train Loss: 0.0456 Val Loss: 0.0465\n",
      "Epoch [19/20] Train Loss: 0.0455 Val Loss: 0.0463\n",
      "Epoch [20/20] Train Loss: 0.0453 Val Loss: 0.0463\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0388 Val Loss: 0.0410\n",
      "Epoch [2/20] Train Loss: 0.0385 Val Loss: 0.0411\n",
      "Epoch [3/20] Train Loss: 0.0384 Val Loss: 0.0407\n",
      "Epoch [4/20] Train Loss: 0.0383 Val Loss: 0.0408\n",
      "Epoch [5/20] Train Loss: 0.0382 Val Loss: 0.0407\n",
      "Epoch [6/20] Train Loss: 0.0381 Val Loss: 0.0407\n",
      "Epoch [7/20] Train Loss: 0.0380 Val Loss: 0.0404\n",
      "Epoch [8/20] Train Loss: 0.0379 Val Loss: 0.0405\n",
      "Epoch [9/20] Train Loss: 0.0379 Val Loss: 0.0405\n",
      "Epoch [10/20] Train Loss: 0.0377 Val Loss: 0.0403\n",
      "Epoch [11/20] Train Loss: 0.0377 Val Loss: 0.0405\n",
      "Epoch [12/20] Train Loss: 0.0376 Val Loss: 0.0402\n",
      "Epoch [13/20] Train Loss: 0.0376 Val Loss: 0.0401\n",
      "Epoch [14/20] Train Loss: 0.0375 Val Loss: 0.0402\n",
      "Epoch [15/20] Train Loss: 0.0374 Val Loss: 0.0401\n",
      "Epoch [16/20] Train Loss: 0.0374 Val Loss: 0.0400\n",
      "Epoch [17/20] Train Loss: 0.0373 Val Loss: 0.0399\n",
      "Epoch [18/20] Train Loss: 0.0373 Val Loss: 0.0400\n",
      "Epoch [19/20] Train Loss: 0.0372 Val Loss: 0.0400\n",
      "Epoch [20/20] Train Loss: 0.0372 Val Loss: 0.0400\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0429 Val Loss: 0.0447\n",
      "Epoch [2/20] Train Loss: 0.0419 Val Loss: 0.0442\n",
      "Epoch [3/20] Train Loss: 0.0415 Val Loss: 0.0441\n",
      "Epoch [4/20] Train Loss: 0.0413 Val Loss: 0.0442\n",
      "Epoch [5/20] Train Loss: 0.0411 Val Loss: 0.0440\n",
      "Epoch [6/20] Train Loss: 0.0410 Val Loss: 0.0440\n",
      "Epoch [7/20] Train Loss: 0.0409 Val Loss: 0.0438\n",
      "Epoch [8/20] Train Loss: 0.0407 Val Loss: 0.0438\n",
      "Epoch [9/20] Train Loss: 0.0406 Val Loss: 0.0436\n",
      "Epoch [10/20] Train Loss: 0.0405 Val Loss: 0.0436\n",
      "Epoch [11/20] Train Loss: 0.0404 Val Loss: 0.0434\n",
      "Epoch [12/20] Train Loss: 0.0403 Val Loss: 0.0434\n",
      "Epoch [13/20] Train Loss: 0.0402 Val Loss: 0.0434\n",
      "Epoch [14/20] Train Loss: 0.0401 Val Loss: 0.0433\n",
      "Epoch [15/20] Train Loss: 0.0401 Val Loss: 0.0439\n",
      "Epoch [16/20] Train Loss: 0.0400 Val Loss: 0.0434\n",
      "Epoch [17/20] Train Loss: 0.0399 Val Loss: 0.0432\n",
      "Epoch [18/20] Train Loss: 0.0398 Val Loss: 0.0432\n",
      "Epoch [19/20] Train Loss: 0.0398 Val Loss: 0.0429\n",
      "Epoch [20/20] Train Loss: 0.0397 Val Loss: 0.0430\n",
      "[0.06474406839259071, 0.06437504491832567, 0.07228052033085937, 0.07123737315302842, 0.0696164061289842, 0.06810541941516407, 0.07295988341283104]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_noewc_losses=[]\n",
    "local_fine_tune_noewc_preds=[]\n",
    "local_fine_tune_noewc_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_noewc_pred,local_fine_tune_noewc_loss,local_fine_tune_noewc_model=clients[i].local_fine_tune(server.index_set[i],ewc_flag=False,importance=0.1,fine_tune_epochs=20)\n",
    "    local_fine_tune_noewc_losses.append(local_fine_tune_noewc_loss)\n",
    "    local_fine_tune_noewc_preds.append(local_fine_tune_noewc_pred)\n",
    "    local_fine_tune_noewc_models.append(local_fine_tune_noewc_model)\n",
    "print(local_fine_tune_noewc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the server object\n",
    "with open('../result/'+str(scale)+'/server_benchmark.pkl', 'rb') as f:\n",
    "    server_benchmark = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients_benchmark.pkl', 'rb') as f:\n",
    "    clients_benchmark = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06865391036980364, 0.08005914130337434, 0.08350040195892526, 0.0766893360215201, 0.07898468616074078, 0.07257786304498576, 0.07864485521583933]\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(7):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients_benchmark[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06877125395232275, 0.08701450000070546, 0.08665346308317903, 0.07787583673959725, 0.08269334102228079, 0.0758516240788445, 0.08388754275104363]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(7):\n",
    "    central_pred,central_loss,central_model=server_benchmark.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08187799558859982, 0.08105895224295251, 0.08582551417591637, 0.07698124492770597, 0.07616442277364127, 0.07639797528159536, 0.09301163217894835]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(7):\n",
    "    local_pred,local_loss,local_model=clients_benchmark[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0579 Val Loss: 0.0575\n",
      "Epoch [2/20] Train Loss: 0.0577 Val Loss: 0.0572\n",
      "Epoch [3/20] Train Loss: 0.0575 Val Loss: 0.0571\n",
      "Epoch [4/20] Train Loss: 0.0573 Val Loss: 0.0570\n",
      "Epoch [5/20] Train Loss: 0.0573 Val Loss: 0.0570\n",
      "Epoch [6/20] Train Loss: 0.0573 Val Loss: 0.0569\n",
      "Epoch [7/20] Train Loss: 0.0571 Val Loss: 0.0568\n",
      "Epoch [8/20] Train Loss: 0.0571 Val Loss: 0.0568\n",
      "Epoch [9/20] Train Loss: 0.0570 Val Loss: 0.0567\n",
      "Epoch [10/20] Train Loss: 0.0570 Val Loss: 0.0567\n",
      "Epoch [11/20] Train Loss: 0.0570 Val Loss: 0.0568\n",
      "Epoch [12/20] Train Loss: 0.0570 Val Loss: 0.0567\n",
      "Epoch [13/20] Train Loss: 0.0569 Val Loss: 0.0566\n",
      "Epoch [14/20] Train Loss: 0.0569 Val Loss: 0.0566\n",
      "Epoch [15/20] Train Loss: 0.0569 Val Loss: 0.0566\n",
      "Epoch [16/20] Train Loss: 0.0569 Val Loss: 0.0566\n",
      "Epoch [17/20] Train Loss: 0.0568 Val Loss: 0.0565\n",
      "Epoch [18/20] Train Loss: 0.0568 Val Loss: 0.0568\n",
      "Epoch [19/20] Train Loss: 0.0569 Val Loss: 0.0565\n",
      "Epoch [20/20] Train Loss: 0.0568 Val Loss: 0.0565\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0675 Val Loss: 0.0666\n",
      "Epoch [2/20] Train Loss: 0.0671 Val Loss: 0.0665\n",
      "Epoch [3/20] Train Loss: 0.0668 Val Loss: 0.0661\n",
      "Epoch [4/20] Train Loss: 0.0666 Val Loss: 0.0657\n",
      "Epoch [5/20] Train Loss: 0.0664 Val Loss: 0.0656\n",
      "Epoch [6/20] Train Loss: 0.0663 Val Loss: 0.0658\n",
      "Epoch [7/20] Train Loss: 0.0662 Val Loss: 0.0654\n",
      "Epoch [8/20] Train Loss: 0.0662 Val Loss: 0.0655\n",
      "Epoch [9/20] Train Loss: 0.0661 Val Loss: 0.0657\n",
      "Epoch [10/20] Train Loss: 0.0661 Val Loss: 0.0652\n",
      "Epoch [11/20] Train Loss: 0.0660 Val Loss: 0.0651\n",
      "Epoch [12/20] Train Loss: 0.0659 Val Loss: 0.0651\n",
      "Epoch [13/20] Train Loss: 0.0659 Val Loss: 0.0651\n",
      "Epoch [14/20] Train Loss: 0.0659 Val Loss: 0.0651\n",
      "Epoch [15/20] Train Loss: 0.0659 Val Loss: 0.0650\n",
      "Epoch [16/20] Train Loss: 0.0658 Val Loss: 0.0649\n",
      "Epoch [17/20] Train Loss: 0.0658 Val Loss: 0.0649\n",
      "Epoch [18/20] Train Loss: 0.0657 Val Loss: 0.0648\n",
      "Epoch [19/20] Train Loss: 0.0657 Val Loss: 0.0648\n",
      "Epoch [20/20] Train Loss: 0.0657 Val Loss: 0.0648\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0742 Val Loss: 0.0715\n",
      "Epoch [2/20] Train Loss: 0.0739 Val Loss: 0.0711\n",
      "Epoch [3/20] Train Loss: 0.0736 Val Loss: 0.0709\n",
      "Epoch [4/20] Train Loss: 0.0734 Val Loss: 0.0709\n",
      "Epoch [5/20] Train Loss: 0.0733 Val Loss: 0.0706\n",
      "Epoch [6/20] Train Loss: 0.0732 Val Loss: 0.0706\n",
      "Epoch [7/20] Train Loss: 0.0732 Val Loss: 0.0705\n",
      "Epoch [8/20] Train Loss: 0.0731 Val Loss: 0.0705\n",
      "Epoch [9/20] Train Loss: 0.0730 Val Loss: 0.0705\n",
      "Epoch [10/20] Train Loss: 0.0730 Val Loss: 0.0703\n",
      "Epoch [11/20] Train Loss: 0.0729 Val Loss: 0.0703\n",
      "Epoch [12/20] Train Loss: 0.0729 Val Loss: 0.0706\n",
      "Epoch [13/20] Train Loss: 0.0729 Val Loss: 0.0702\n",
      "Epoch [14/20] Train Loss: 0.0729 Val Loss: 0.0701\n",
      "Epoch [15/20] Train Loss: 0.0728 Val Loss: 0.0701\n",
      "Epoch [16/20] Train Loss: 0.0728 Val Loss: 0.0702\n",
      "Epoch [17/20] Train Loss: 0.0728 Val Loss: 0.0701\n",
      "Epoch [18/20] Train Loss: 0.0727 Val Loss: 0.0700\n",
      "Epoch [19/20] Train Loss: 0.0727 Val Loss: 0.0701\n",
      "Epoch [20/20] Train Loss: 0.0727 Val Loss: 0.0702\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0639 Val Loss: 0.0634\n",
      "Epoch [2/20] Train Loss: 0.0631 Val Loss: 0.0632\n",
      "Epoch [3/20] Train Loss: 0.0629 Val Loss: 0.0631\n",
      "Epoch [4/20] Train Loss: 0.0628 Val Loss: 0.0633\n",
      "Epoch [5/20] Train Loss: 0.0628 Val Loss: 0.0631\n",
      "Epoch [6/20] Train Loss: 0.0626 Val Loss: 0.0629\n",
      "Epoch [7/20] Train Loss: 0.0626 Val Loss: 0.0627\n",
      "Epoch [8/20] Train Loss: 0.0625 Val Loss: 0.0627\n",
      "Epoch [9/20] Train Loss: 0.0625 Val Loss: 0.0627\n",
      "Epoch [10/20] Train Loss: 0.0625 Val Loss: 0.0627\n",
      "Epoch [11/20] Train Loss: 0.0624 Val Loss: 0.0629\n",
      "Epoch [12/20] Train Loss: 0.0624 Val Loss: 0.0626\n",
      "Epoch [13/20] Train Loss: 0.0623 Val Loss: 0.0627\n",
      "Epoch [14/20] Train Loss: 0.0623 Val Loss: 0.0626\n",
      "Epoch [15/20] Train Loss: 0.0623 Val Loss: 0.0627\n",
      "Epoch [16/20] Train Loss: 0.0623 Val Loss: 0.0625\n",
      "Epoch [17/20] Train Loss: 0.0622 Val Loss: 0.0625\n",
      "Epoch [18/20] Train Loss: 0.0622 Val Loss: 0.0628\n",
      "Epoch [19/20] Train Loss: 0.0623 Val Loss: 0.0624\n",
      "Epoch [20/20] Train Loss: 0.0622 Val Loss: 0.0624\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0680 Val Loss: 0.0676\n",
      "Epoch [2/20] Train Loss: 0.0676 Val Loss: 0.0672\n",
      "Epoch [3/20] Train Loss: 0.0674 Val Loss: 0.0670\n",
      "Epoch [4/20] Train Loss: 0.0672 Val Loss: 0.0669\n",
      "Epoch [5/20] Train Loss: 0.0671 Val Loss: 0.0668\n",
      "Epoch [6/20] Train Loss: 0.0670 Val Loss: 0.0666\n",
      "Epoch [7/20] Train Loss: 0.0669 Val Loss: 0.0665\n",
      "Epoch [8/20] Train Loss: 0.0668 Val Loss: 0.0667\n",
      "Epoch [9/20] Train Loss: 0.0668 Val Loss: 0.0664\n",
      "Epoch [10/20] Train Loss: 0.0667 Val Loss: 0.0663\n",
      "Epoch [11/20] Train Loss: 0.0667 Val Loss: 0.0663\n",
      "Epoch [12/20] Train Loss: 0.0667 Val Loss: 0.0662\n",
      "Epoch [13/20] Train Loss: 0.0666 Val Loss: 0.0664\n",
      "Epoch [14/20] Train Loss: 0.0666 Val Loss: 0.0661\n",
      "Epoch [15/20] Train Loss: 0.0665 Val Loss: 0.0661\n",
      "Epoch [16/20] Train Loss: 0.0665 Val Loss: 0.0660\n",
      "Epoch [17/20] Train Loss: 0.0665 Val Loss: 0.0660\n",
      "Epoch [18/20] Train Loss: 0.0665 Val Loss: 0.0662\n",
      "Epoch [19/20] Train Loss: 0.0664 Val Loss: 0.0659\n",
      "Epoch [20/20] Train Loss: 0.0664 Val Loss: 0.0659\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0588 Val Loss: 0.0596\n",
      "Epoch [2/20] Train Loss: 0.0587 Val Loss: 0.0595\n",
      "Epoch [3/20] Train Loss: 0.0586 Val Loss: 0.0594\n",
      "Epoch [4/20] Train Loss: 0.0584 Val Loss: 0.0593\n",
      "Epoch [5/20] Train Loss: 0.0583 Val Loss: 0.0593\n",
      "Epoch [6/20] Train Loss: 0.0583 Val Loss: 0.0592\n",
      "Epoch [7/20] Train Loss: 0.0582 Val Loss: 0.0593\n",
      "Epoch [8/20] Train Loss: 0.0582 Val Loss: 0.0592\n",
      "Epoch [9/20] Train Loss: 0.0582 Val Loss: 0.0591\n",
      "Epoch [10/20] Train Loss: 0.0581 Val Loss: 0.0590\n",
      "Epoch [11/20] Train Loss: 0.0581 Val Loss: 0.0590\n",
      "Epoch [12/20] Train Loss: 0.0581 Val Loss: 0.0590\n",
      "Epoch [13/20] Train Loss: 0.0580 Val Loss: 0.0589\n",
      "Epoch [14/20] Train Loss: 0.0580 Val Loss: 0.0589\n",
      "Epoch [15/20] Train Loss: 0.0580 Val Loss: 0.0588\n",
      "Epoch [16/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [17/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [18/20] Train Loss: 0.0579 Val Loss: 0.0589\n",
      "Epoch [19/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "Epoch [20/20] Train Loss: 0.0579 Val Loss: 0.0588\n",
      "1e-05 0\n",
      "self.importance 0.02\n",
      "Epoch [1/20] Train Loss: 0.0652 Val Loss: 0.0660\n",
      "Epoch [2/20] Train Loss: 0.0651 Val Loss: 0.0658\n",
      "Epoch [3/20] Train Loss: 0.0649 Val Loss: 0.0657\n",
      "Epoch [4/20] Train Loss: 0.0648 Val Loss: 0.0656\n",
      "Epoch [5/20] Train Loss: 0.0647 Val Loss: 0.0657\n",
      "Epoch [6/20] Train Loss: 0.0646 Val Loss: 0.0655\n",
      "Epoch [7/20] Train Loss: 0.0646 Val Loss: 0.0654\n",
      "Epoch [8/20] Train Loss: 0.0645 Val Loss: 0.0654\n",
      "Epoch [9/20] Train Loss: 0.0645 Val Loss: 0.0654\n",
      "Epoch [10/20] Train Loss: 0.0644 Val Loss: 0.0654\n",
      "Epoch [11/20] Train Loss: 0.0644 Val Loss: 0.0654\n",
      "Epoch [12/20] Train Loss: 0.0644 Val Loss: 0.0653\n",
      "Epoch [13/20] Train Loss: 0.0643 Val Loss: 0.0657\n",
      "Epoch [14/20] Train Loss: 0.0643 Val Loss: 0.0652\n",
      "Epoch [15/20] Train Loss: 0.0643 Val Loss: 0.0653\n",
      "Epoch [16/20] Train Loss: 0.0643 Val Loss: 0.0652\n",
      "Epoch [17/20] Train Loss: 0.0643 Val Loss: 0.0651\n",
      "Epoch [18/20] Train Loss: 0.0642 Val Loss: 0.0654\n",
      "Epoch [19/20] Train Loss: 0.0642 Val Loss: 0.0651\n",
      "Epoch [20/20] Train Loss: 0.0642 Val Loss: 0.0651\n",
      "[0.06748886449798329, 0.07927469522628473, 0.08302414955647841, 0.07458296313575685, 0.07597812874982618, 0.07320099175996976, 0.0789871802425956]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_ewc_benchmark_losses=[]\n",
    "local_fine_tune_ewc_benchmark_preds=[]\n",
    "local_fine_tune_ewc_benchmark_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_ewc_benchmark_pred,local_fine_tune_ewc_benchmark_loss,local_fine_tune_ewc_benchmark_model=clients_benchmark[i].local_fine_tune(ewc_flag=True,importance=0.02,fine_tune_epochs=20)\n",
    "    local_fine_tune_ewc_benchmark_losses.append(local_fine_tune_ewc_benchmark_loss)\n",
    "    local_fine_tune_ewc_benchmark_preds.append(local_fine_tune_ewc_benchmark_pred)\n",
    "    local_fine_tune_ewc_benchmark_models.append(local_fine_tune_ewc_benchmark_model)\n",
    "print(local_fine_tune_ewc_benchmark_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.0579 Val Loss: 0.0578\n",
      "Epoch [2/20] Train Loss: 0.0576 Val Loss: 0.0572\n",
      "Epoch [3/20] Train Loss: 0.0573 Val Loss: 0.0570\n",
      "Epoch [4/20] Train Loss: 0.0571 Val Loss: 0.0570\n",
      "Epoch [5/20] Train Loss: 0.0569 Val Loss: 0.0569\n",
      "Epoch [6/20] Train Loss: 0.0568 Val Loss: 0.0568\n",
      "Epoch [7/20] Train Loss: 0.0567 Val Loss: 0.0567\n",
      "Epoch [8/20] Train Loss: 0.0566 Val Loss: 0.0566\n",
      "Epoch [9/20] Train Loss: 0.0565 Val Loss: 0.0566\n",
      "Epoch [10/20] Train Loss: 0.0564 Val Loss: 0.0566\n",
      "Epoch [11/20] Train Loss: 0.0563 Val Loss: 0.0564\n",
      "Epoch [12/20] Train Loss: 0.0563 Val Loss: 0.0564\n",
      "Epoch [13/20] Train Loss: 0.0562 Val Loss: 0.0565\n",
      "Epoch [14/20] Train Loss: 0.0561 Val Loss: 0.0562\n",
      "Epoch [15/20] Train Loss: 0.0560 Val Loss: 0.0563\n",
      "Epoch [16/20] Train Loss: 0.0559 Val Loss: 0.0561\n",
      "Epoch [17/20] Train Loss: 0.0559 Val Loss: 0.0560\n",
      "Epoch [18/20] Train Loss: 0.0558 Val Loss: 0.0562\n",
      "Epoch [19/20] Train Loss: 0.0558 Val Loss: 0.0559\n",
      "Epoch [20/20] Train Loss: 0.0557 Val Loss: 0.0559\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0675 Val Loss: 0.0666\n",
      "Epoch [2/20] Train Loss: 0.0669 Val Loss: 0.0662\n",
      "Epoch [3/20] Train Loss: 0.0665 Val Loss: 0.0658\n",
      "Epoch [4/20] Train Loss: 0.0662 Val Loss: 0.0657\n",
      "Epoch [5/20] Train Loss: 0.0659 Val Loss: 0.0653\n",
      "Epoch [6/20] Train Loss: 0.0657 Val Loss: 0.0653\n",
      "Epoch [7/20] Train Loss: 0.0656 Val Loss: 0.0651\n",
      "Epoch [8/20] Train Loss: 0.0654 Val Loss: 0.0650\n",
      "Epoch [9/20] Train Loss: 0.0652 Val Loss: 0.0648\n",
      "Epoch [10/20] Train Loss: 0.0650 Val Loss: 0.0646\n",
      "Epoch [11/20] Train Loss: 0.0649 Val Loss: 0.0647\n",
      "Epoch [12/20] Train Loss: 0.0647 Val Loss: 0.0643\n",
      "Epoch [13/20] Train Loss: 0.0646 Val Loss: 0.0644\n",
      "Epoch [14/20] Train Loss: 0.0644 Val Loss: 0.0642\n",
      "Epoch [15/20] Train Loss: 0.0643 Val Loss: 0.0641\n",
      "Epoch [16/20] Train Loss: 0.0642 Val Loss: 0.0641\n",
      "Epoch [17/20] Train Loss: 0.0640 Val Loss: 0.0638\n",
      "Epoch [18/20] Train Loss: 0.0639 Val Loss: 0.0637\n",
      "Epoch [19/20] Train Loss: 0.0638 Val Loss: 0.0636\n",
      "Epoch [20/20] Train Loss: 0.0636 Val Loss: 0.0636\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0741 Val Loss: 0.0715\n",
      "Epoch [2/20] Train Loss: 0.0737 Val Loss: 0.0710\n",
      "Epoch [3/20] Train Loss: 0.0734 Val Loss: 0.0708\n",
      "Epoch [4/20] Train Loss: 0.0731 Val Loss: 0.0707\n",
      "Epoch [5/20] Train Loss: 0.0729 Val Loss: 0.0706\n",
      "Epoch [6/20] Train Loss: 0.0727 Val Loss: 0.0704\n",
      "Epoch [7/20] Train Loss: 0.0725 Val Loss: 0.0703\n",
      "Epoch [8/20] Train Loss: 0.0724 Val Loss: 0.0702\n",
      "Epoch [9/20] Train Loss: 0.0722 Val Loss: 0.0701\n",
      "Epoch [10/20] Train Loss: 0.0721 Val Loss: 0.0700\n",
      "Epoch [11/20] Train Loss: 0.0720 Val Loss: 0.0699\n",
      "Epoch [12/20] Train Loss: 0.0719 Val Loss: 0.0698\n",
      "Epoch [13/20] Train Loss: 0.0717 Val Loss: 0.0700\n",
      "Epoch [14/20] Train Loss: 0.0716 Val Loss: 0.0697\n",
      "Epoch [15/20] Train Loss: 0.0716 Val Loss: 0.0695\n",
      "Epoch [16/20] Train Loss: 0.0714 Val Loss: 0.0694\n",
      "Epoch [17/20] Train Loss: 0.0713 Val Loss: 0.0694\n",
      "Epoch [18/20] Train Loss: 0.0712 Val Loss: 0.0693\n",
      "Epoch [19/20] Train Loss: 0.0711 Val Loss: 0.0692\n",
      "Epoch [20/20] Train Loss: 0.0710 Val Loss: 0.0693\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0638 Val Loss: 0.0632\n",
      "Epoch [2/20] Train Loss: 0.0628 Val Loss: 0.0630\n",
      "Epoch [3/20] Train Loss: 0.0625 Val Loss: 0.0630\n",
      "Epoch [4/20] Train Loss: 0.0623 Val Loss: 0.0630\n",
      "Epoch [5/20] Train Loss: 0.0623 Val Loss: 0.0628\n",
      "Epoch [6/20] Train Loss: 0.0622 Val Loss: 0.0628\n",
      "Epoch [7/20] Train Loss: 0.0620 Val Loss: 0.0626\n",
      "Epoch [8/20] Train Loss: 0.0619 Val Loss: 0.0626\n",
      "Epoch [9/20] Train Loss: 0.0618 Val Loss: 0.0625\n",
      "Epoch [10/20] Train Loss: 0.0617 Val Loss: 0.0624\n",
      "Epoch [11/20] Train Loss: 0.0617 Val Loss: 0.0623\n",
      "Epoch [12/20] Train Loss: 0.0615 Val Loss: 0.0624\n",
      "Epoch [13/20] Train Loss: 0.0615 Val Loss: 0.0622\n",
      "Epoch [14/20] Train Loss: 0.0614 Val Loss: 0.0621\n",
      "Epoch [15/20] Train Loss: 0.0613 Val Loss: 0.0621\n",
      "Epoch [16/20] Train Loss: 0.0612 Val Loss: 0.0620\n",
      "Epoch [17/20] Train Loss: 0.0611 Val Loss: 0.0621\n",
      "Epoch [18/20] Train Loss: 0.0611 Val Loss: 0.0619\n",
      "Epoch [19/20] Train Loss: 0.0610 Val Loss: 0.0618\n",
      "Epoch [20/20] Train Loss: 0.0609 Val Loss: 0.0618\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0680 Val Loss: 0.0677\n",
      "Epoch [2/20] Train Loss: 0.0674 Val Loss: 0.0671\n",
      "Epoch [3/20] Train Loss: 0.0670 Val Loss: 0.0668\n",
      "Epoch [4/20] Train Loss: 0.0668 Val Loss: 0.0666\n",
      "Epoch [5/20] Train Loss: 0.0666 Val Loss: 0.0665\n",
      "Epoch [6/20] Train Loss: 0.0664 Val Loss: 0.0663\n",
      "Epoch [7/20] Train Loss: 0.0662 Val Loss: 0.0662\n",
      "Epoch [8/20] Train Loss: 0.0661 Val Loss: 0.0661\n",
      "Epoch [9/20] Train Loss: 0.0659 Val Loss: 0.0660\n",
      "Epoch [10/20] Train Loss: 0.0658 Val Loss: 0.0658\n",
      "Epoch [11/20] Train Loss: 0.0657 Val Loss: 0.0657\n",
      "Epoch [12/20] Train Loss: 0.0655 Val Loss: 0.0657\n",
      "Epoch [13/20] Train Loss: 0.0654 Val Loss: 0.0656\n",
      "Epoch [14/20] Train Loss: 0.0653 Val Loss: 0.0655\n",
      "Epoch [15/20] Train Loss: 0.0652 Val Loss: 0.0656\n",
      "Epoch [16/20] Train Loss: 0.0651 Val Loss: 0.0655\n",
      "Epoch [17/20] Train Loss: 0.0650 Val Loss: 0.0654\n",
      "Epoch [18/20] Train Loss: 0.0649 Val Loss: 0.0652\n",
      "Epoch [19/20] Train Loss: 0.0648 Val Loss: 0.0651\n",
      "Epoch [20/20] Train Loss: 0.0646 Val Loss: 0.0651\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0588 Val Loss: 0.0596\n",
      "Epoch [2/20] Train Loss: 0.0586 Val Loss: 0.0595\n",
      "Epoch [3/20] Train Loss: 0.0583 Val Loss: 0.0593\n",
      "Epoch [4/20] Train Loss: 0.0582 Val Loss: 0.0593\n",
      "Epoch [5/20] Train Loss: 0.0581 Val Loss: 0.0592\n",
      "Epoch [6/20] Train Loss: 0.0579 Val Loss: 0.0590\n",
      "Epoch [7/20] Train Loss: 0.0578 Val Loss: 0.0590\n",
      "Epoch [8/20] Train Loss: 0.0577 Val Loss: 0.0589\n",
      "Epoch [9/20] Train Loss: 0.0577 Val Loss: 0.0590\n",
      "Epoch [10/20] Train Loss: 0.0575 Val Loss: 0.0588\n",
      "Epoch [11/20] Train Loss: 0.0575 Val Loss: 0.0588\n",
      "Epoch [12/20] Train Loss: 0.0574 Val Loss: 0.0586\n",
      "Epoch [13/20] Train Loss: 0.0573 Val Loss: 0.0586\n",
      "Epoch [14/20] Train Loss: 0.0572 Val Loss: 0.0586\n",
      "Epoch [15/20] Train Loss: 0.0571 Val Loss: 0.0585\n",
      "Epoch [16/20] Train Loss: 0.0570 Val Loss: 0.0585\n",
      "Epoch [17/20] Train Loss: 0.0570 Val Loss: 0.0584\n",
      "Epoch [18/20] Train Loss: 0.0569 Val Loss: 0.0583\n",
      "Epoch [19/20] Train Loss: 0.0568 Val Loss: 0.0583\n",
      "Epoch [20/20] Train Loss: 0.0567 Val Loss: 0.0583\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0651 Val Loss: 0.0662\n",
      "Epoch [2/20] Train Loss: 0.0650 Val Loss: 0.0659\n",
      "Epoch [3/20] Train Loss: 0.0647 Val Loss: 0.0657\n",
      "Epoch [4/20] Train Loss: 0.0646 Val Loss: 0.0657\n",
      "Epoch [5/20] Train Loss: 0.0645 Val Loss: 0.0654\n",
      "Epoch [6/20] Train Loss: 0.0643 Val Loss: 0.0653\n",
      "Epoch [7/20] Train Loss: 0.0642 Val Loss: 0.0652\n",
      "Epoch [8/20] Train Loss: 0.0641 Val Loss: 0.0651\n",
      "Epoch [9/20] Train Loss: 0.0640 Val Loss: 0.0653\n",
      "Epoch [10/20] Train Loss: 0.0638 Val Loss: 0.0653\n",
      "Epoch [11/20] Train Loss: 0.0637 Val Loss: 0.0650\n",
      "Epoch [12/20] Train Loss: 0.0637 Val Loss: 0.0650\n",
      "Epoch [13/20] Train Loss: 0.0635 Val Loss: 0.0648\n",
      "Epoch [14/20] Train Loss: 0.0634 Val Loss: 0.0647\n",
      "Epoch [15/20] Train Loss: 0.0634 Val Loss: 0.0651\n",
      "Epoch [16/20] Train Loss: 0.0632 Val Loss: 0.0648\n",
      "Epoch [17/20] Train Loss: 0.0631 Val Loss: 0.0646\n",
      "Epoch [18/20] Train Loss: 0.0630 Val Loss: 0.0646\n",
      "Epoch [19/20] Train Loss: 0.0630 Val Loss: 0.0645\n",
      "Epoch [20/20] Train Loss: 0.0628 Val Loss: 0.0648\n",
      "[0.0674807617747008, 0.07917149850384217, 0.08372500471209418, 0.07453125617658235, 0.07562352281880297, 0.07363936380317358, 0.0791351490347863]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_benchmark_losses=[]\n",
    "local_fine_tune_benchmark_preds=[]\n",
    "local_fine_tune_benchmark_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(7):\n",
    "    local_fine_tune_benchmark_pred,local_fine_tune_benchmark_loss,local_fine_tune_benchmark_model=clients_benchmark[i].local_fine_tune()\n",
    "    local_fine_tune_benchmark_losses.append(local_fine_tune_benchmark_loss)\n",
    "    local_fine_tune_benchmark_preds.append(local_fine_tune_benchmark_pred)\n",
    "    local_fine_tune_benchmark_models.append(local_fine_tune_benchmark_model)\n",
    "print(local_fine_tune_benchmark_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08187799558859982, 0.08105895224295251, 0.08582551417591637, 0.07698124492770597, 0.07616442277364127, 0.07639797528159536, 0.09301163217894835]\n",
      "[0.06877125395232275, 0.08701450000070546, 0.08665346308317903, 0.07787583673959725, 0.08269334102228079, 0.0758516240788445, 0.08388754275104363]\n",
      "[0.06865391036980364, 0.08005914130337434, 0.08350040195892526, 0.0766893360215201, 0.07898468616074078, 0.07257786304498576, 0.07864485521583933]\n",
      "[0.0674807617747008, 0.07917149850384217, 0.08372500471209418, 0.07453125617658235, 0.07562352281880297, 0.07363936380317358, 0.0791351490347863]\n",
      "[0.06748886449798329, 0.07927469522628473, 0.08302414955647841, 0.07458296313575685, 0.07597812874982618, 0.07320099175996976, 0.0789871802425956]\n",
      "[0.07019277994380627, 0.06390050535205088, 0.07960541857636139, 0.07376481334946744, 0.07278922048384605, 0.06719664158299565, 0.08431435201623261]\n",
      "[0.06474406839259071, 0.06437504491832567, 0.07228052033085937, 0.07123737315302842, 0.0696164061289842, 0.06810541941516407, 0.07295988341283104]\n",
      "[0.06422739151916275, 0.06387627024637306, 0.07292083787974225, 0.07075107832477517, 0.06966070825718854, 0.06795490283697639, 0.07290313319519978]\n",
      "0.07607671045269926\n",
      "0.07618665097485462\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_benchmark_losses)\n",
    "print(local_fine_tune_ewc_benchmark_losses)\n",
    "print(fed_local_proposed_losses)\n",
    "print(local_fine_tune_noewc_losses)\n",
    "print(local_fine_tune_ewc_losses)\n",
    "\n",
    "print(np.mean(local_fine_tune_ewc_benchmark_losses))\n",
    "print(np.mean(local_fine_tune_benchmark_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=scale)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "parser_train.add_argument('--k', type=int, default=2)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "\n",
    "parser_train.add_argument('--warm_up_epochs', type=int, default=15)\n",
    "parser_train.add_argument('--selection_epochs', type=int, default=4)\n",
    "parser_train.add_argument('--importance', type=float, default=0)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model2/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "parser_train.add_argument('--decay', type=float, default=0.75)\n",
    "args_train = parser_train.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "class RMSELoss:\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        self.reduction = reduction\n",
    "        self.mse_loss = MSELoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        output=output.cpu()[:,4]#.reshape(-1,1)\n",
    "        mse = self.mse_loss(output, target)\n",
    "        rmse = torch.sqrt(mse)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            rmse = rmse.sum()\n",
    "        if self.reduction == 'mean':\n",
    "            rmse = rmse.mean()\n",
    "\n",
    "        return rmse\n",
    "    \n",
    "\n",
    "import torch\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "class MAELoss:\n",
    "    def __init__(self, reduction: str = 'mean'):\n",
    "        self.reduction = reduction\n",
    "        self.mae_loss = L1Loss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, output: torch.Tensor, target: torch.Tensor):\n",
    "        output = output.cpu()[:, 4]\n",
    "        mae = self.mae_loss(output, target)\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            mae = mae.sum()\n",
    "        if self.reduction == 'mean':\n",
    "            mae = mae.mean()\n",
    "\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y_lst=[]\n",
    "\n",
    "for i in range(7):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths='wf'+str(i+1)\n",
    "    test_data, test_loader = get_data(args_temp,flag='test')\n",
    "    actual_y=[]\n",
    "    for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "        actual_y.append(seq_y)\n",
    "    actual_y = torch.cat([torch.flatten(t) for t in actual_y])\n",
    "    actual_y_lst.append(actual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rmse = []\n",
    "central_rmse = []\n",
    "fed_local_rmse = []\n",
    "local_fine_tune_rmse = []\n",
    "fed_local_proposed_rmse = []\n",
    "local_fine_tune_noewc_rmse = []\n",
    "local_fine_tune_ewc_rmse = []\n",
    "local_fine_tune_benchmark_ewc_rmse = []\n",
    "rmse_loss = RMSELoss('mean')\n",
    "\n",
    "for i in range(7):\n",
    "    fed_local_rmse.append(rmse_loss(fed_local_preds[i], actual_y_lst[i]))\n",
    "    local_rmse.append(rmse_loss(local_preds[i], actual_y_lst[i]))\n",
    "    central_rmse.append(rmse_loss(central_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_rmse.append(rmse_loss(local_fine_tune_benchmark_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_benchmark_ewc_rmse.append(rmse_loss(local_fine_tune_ewc_benchmark_preds[i], actual_y_lst[i]))\n",
    "    fed_local_proposed_rmse.append(rmse_loss(fed_local_proposed_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_noewc_rmse.append(rmse_loss(local_fine_tune_noewc_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_ewc_rmse.append(rmse_loss(local_fine_tune_ewc_preds[i], actual_y_lst[i]))\n",
    "\n",
    "df_RMSE = pd.DataFrame({\n",
    "    'local_rmse': np.array(local_rmse),\n",
    "    'central_rmse': np.array(central_rmse),\n",
    "    'fed_local_rmse': np.array(fed_local_rmse),\n",
    "    'local_fine_tune_rmse': np.array(local_fine_tune_rmse),\n",
    "    'local_fine_tune_benchmark_ewc_rmse': np.array(local_fine_tune_benchmark_ewc_rmse),\n",
    "    'fed_local_proposed_rmse': np.array(fed_local_proposed_rmse),\n",
    "    'local_fine_tune_noewc_rmse': np.array(local_fine_tune_noewc_rmse),\n",
    "    'local_fine_tune_ewc_rmse': np.array(local_fine_tune_ewc_rmse),\n",
    "}).T\n",
    "df_RMSE['mean_rmse'] = df_RMSE.mean(axis=1)\n",
    "df_RMSE.to_csv('rmse.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mae = []\n",
    "central_mae = []\n",
    "fed_local_mae = []\n",
    "local_fine_tune_mae = []\n",
    "fed_local_proposed_mae = []\n",
    "local_fine_tune_noewc_mae = []\n",
    "local_fine_tune_ewc_mae = []\n",
    "local_fine_tune_benchmark_ewc_mae = []\n",
    "mae_loss = MAELoss('mean')\n",
    "\n",
    "for i in range(7):\n",
    "    fed_local_mae.append(mae_loss(fed_local_preds[i], actual_y_lst[i]))\n",
    "    local_mae.append(mae_loss(local_preds[i], actual_y_lst[i]))\n",
    "    central_mae.append(mae_loss(central_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_mae.append(mae_loss(local_fine_tune_benchmark_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_benchmark_ewc_mae.append(mae_loss(local_fine_tune_ewc_benchmark_preds[i], actual_y_lst[i]))\n",
    "    fed_local_proposed_mae.append(mae_loss(fed_local_proposed_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_noewc_mae.append(mae_loss(local_fine_tune_noewc_preds[i], actual_y_lst[i]))\n",
    "    local_fine_tune_ewc_mae.append(mae_loss(local_fine_tune_ewc_preds[i], actual_y_lst[i]))\n",
    "\n",
    "df_MAE = pd.DataFrame({\n",
    "    'local_mae': np.array(local_mae),\n",
    "    'central_mae': np.array(central_mae),\n",
    "    'fed_local_mae': np.array(fed_local_mae),\n",
    "    'local_fine_tune_mae': np.array(local_fine_tune_mae),\n",
    "    'local_fine_tune_benchmark_ewc_mae': np.array(local_fine_tune_benchmark_ewc_mae),\n",
    "    'fed_local_proposed_mae': np.array(fed_local_proposed_mae),\n",
    "    'local_fine_tune_noewc_mae': np.array(local_fine_tune_noewc_mae),\n",
    "    'local_fine_tune_ewc_mae': np.array(local_fine_tune_ewc_mae),\n",
    "}).T\n",
    "df_MAE['mean_mae'] = df_MAE.mean(axis=1)\n",
    "df_MAE.to_csv('mae.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_benchmark_losses': local_fine_tune_benchmark_losses,\n",
    "    'local_fine_tune_ewc_benchmark_losses': local_fine_tune_ewc_benchmark_losses,\n",
    "    'fed_local_proposed_losses': fed_local_proposed_losses,\n",
    "    'local_fine_tune_noewc_losses': local_fine_tune_noewc_losses,\n",
    "    'local_fine_tune_ewc_losses': local_fine_tune_ewc_losses\n",
    "}).T\n",
    "\n",
    "df.to_csv('losses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
