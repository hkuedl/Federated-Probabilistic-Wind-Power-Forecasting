{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Data_loader import Dataset_Custom\n",
    "import argparse\n",
    "import warnings\n",
    "from tools import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import get_data\n",
    "from Model import ANN\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Server import  Server\n",
    "from Clients import Client\n",
    "from Train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_train = argparse.ArgumentParser(description='FL')\n",
    "parser_train.add_argument('--root_path', type=str, default='../Data/GFC12/')\n",
    "parser_train.add_argument('--dataset_paths', type=list, default=[\"wf1\", \"wf2\", \"wf3\", \"wf4\", \"wf5\", \"wf6\", \"wf7\"])\n",
    "parser_train.add_argument('--number_clients', type=int, default=7)\n",
    "parser_train.add_argument('--seq_len', type=int, default=24*4)\n",
    "parser_train.add_argument('--pred_len', type=int, default=12)\n",
    "parser_train.add_argument('--label_len', type=int, default=0)\n",
    "parser_train.add_argument('--train_length', type=int, default=16800)\n",
    "parser_train.add_argument('--target', type=str, default='target')\n",
    "parser_train.add_argument('--scale', type=bool, default=True)\n",
    "parser_train.add_argument('--inverse', type=bool, default=True)\n",
    "\n",
    "parser_train.add_argument('--lr', type=float, default=1e-4)\n",
    "parser_train.add_argument('--global_epochs', type=int, default=200)\n",
    "parser_train.add_argument('--local_epochs', type=int, default=1)\n",
    "parser_train.add_argument('--fine_tune_epochs', type=int, default=20)\n",
    "parser_train.add_argument('--patience', type=int, default=3)\n",
    "parser_train.add_argument('--fed_patience', type=int, default=3)\n",
    "parser_train.add_argument('--hidden_layers', type=list, default=[64,64,64])\n",
    "parser_train.add_argument('--input_size', type=int, default=293)\n",
    "parser_train.add_argument('--output_size', type=int, default=9)\n",
    "\n",
    "parser_train.add_argument('--fine_tune_lr', type=float, default=1e-5)\n",
    "parser_train.add_argument('--ensemble_flag', type=bool, default=True)\n",
    "parser_train.add_argument('--batch_size', type=int, default=64)\n",
    "parser_train.add_argument('--device', type=str, default='cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "parser_train.add_argument('--forecasting_mode', type=str, default='prob')\n",
    "parser_train.add_argument('--model_type', type=str, default='NN')\n",
    "parser_train.add_argument('--model_save_path', type=str, default='../Model12/', help='location of model checkpoints')\n",
    "parser_train.add_argument('--quantiles', type=list, default=[0.1,0.2,0.3,0.4, 0.5,0.6,0.7,0.8, 0.9])\n",
    "args_train = parser_train.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "clients=[]\n",
    "for path in tqdm(args_train.dataset_paths):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths=path\n",
    "    clients.append(Client(args_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server(args_train,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\nscale=12\\n# Load the server object\\nwith open('../result/'+str(scale)+'/server_benchmark.pkl', 'rb') as f:\\n    server_benchmark = pickle.load(f)\\n\\n# Load the clients object\\nwith open('../result/'+str(scale)+'/clients_benchmark.pkl', 'rb') as f:\\n    clients_benchmark = pickle.load(f)\\n\\n\\nfor i in range(7):\\n    clients[i].copy_client(clients_benchmark[i])\\n\\nserver = Server(args_train,clients)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pickle\n",
    "scale=12\n",
    "# Load the server object\n",
    "with open('../result/'+str(scale)+'/server_benchmark.pkl', 'rb') as f:\n",
    "    server_benchmark = pickle.load(f)\n",
    "\n",
    "# Load the clients object\n",
    "with open('../result/'+str(scale)+'/clients_benchmark.pkl', 'rb') as f:\n",
    "    clients_benchmark = pickle.load(f)\n",
    "\n",
    "\n",
    "for i in range(7):\n",
    "    clients[i].copy_client(clients_benchmark[i])\n",
    "\n",
    "server = Server(args_train,clients)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performance: [0.1677448276226243, 0.16753428755965952, 0.19564487713657014, 0.17997160227331396, 0.17244937605135244, 0.18444072286764238, 0.18148799113010708]\n",
      "Epoch: 0 | Loss: 0.0867\n",
      "Epoch: 0 | Loss: 0.0716\n",
      "Epoch: 0 | Loss: 0.1119\n",
      "Epoch: 0 | Loss: 0.0911\n",
      "Epoch: 0 | Loss: 0.0706\n",
      "Epoch: 0 | Loss: 0.0866\n",
      "Epoch: 0 | Loss: 0.0930\n",
      "Federated training Epoch [1/200] Val Loss: 0.0754\n",
      "test performance: [0.07842033547795799, 0.08361714192959543, 0.09718022554194274, 0.08820951684084657, 0.09092676126691576, 0.08746098989799414, 0.09202577464588702]\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0923\n",
      "Epoch: 0 | Loss: 0.0844\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0920\n",
      "Federated training Epoch [2/200] Val Loss: 0.0736\n",
      "test performance: [0.07694024429337619, 0.08226170071898257, 0.0935047646244503, 0.0861237039917136, 0.08941671346658714, 0.08499742643780088, 0.08936039526780991]\n",
      "Epoch: 0 | Loss: 0.0620\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0780\n",
      "Epoch: 0 | Loss: 0.0895\n",
      "Epoch: 0 | Loss: 0.0665\n",
      "Epoch: 0 | Loss: 0.0718\n",
      "Epoch: 0 | Loss: 0.0988\n",
      "Federated training Epoch [3/200] Val Loss: 0.0702\n",
      "test performance: [0.07434325888497781, 0.08100251621273283, 0.08967611395230848, 0.08282957241347391, 0.08672890240607196, 0.08120380089401383, 0.086029241193239]\n",
      "Epoch: 0 | Loss: 0.0618\n",
      "Epoch: 0 | Loss: 0.0673\n",
      "Epoch: 0 | Loss: 0.0843\n",
      "Epoch: 0 | Loss: 0.0770\n",
      "Epoch: 0 | Loss: 0.0712\n",
      "Epoch: 0 | Loss: 0.0675\n",
      "Epoch: 0 | Loss: 0.0804\n",
      "Federated training Epoch [4/200] Val Loss: 0.0687\n",
      "test performance: [0.07296229742008122, 0.0808737350704327, 0.08774111703140279, 0.08125925787456639, 0.0851394957161113, 0.07925359934109123, 0.0844719269733927]\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0874\n",
      "Epoch: 0 | Loss: 0.0832\n",
      "Epoch: 0 | Loss: 0.0709\n",
      "Epoch: 0 | Loss: 0.0753\n",
      "Epoch: 0 | Loss: 0.0760\n",
      "Federated training Epoch [5/200] Val Loss: 0.0682\n",
      "test performance: [0.07237007923118055, 0.08064398459120564, 0.08674717768516442, 0.08067873861538628, 0.08378207497298717, 0.07836548984050751, 0.08360196925597647]\n",
      "Epoch: 0 | Loss: 0.0726\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Epoch: 0 | Loss: 0.0841\n",
      "Epoch: 0 | Loss: 0.0881\n",
      "Epoch: 0 | Loss: 0.0903\n",
      "Epoch: 0 | Loss: 0.0810\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Federated training Epoch [6/200] Val Loss: 0.0678\n",
      "test performance: [0.07184934725806322, 0.08057813050403986, 0.08620497941562574, 0.08017359879378179, 0.08302992505774107, 0.07773430966963507, 0.08319517938190535]\n",
      "Epoch: 0 | Loss: 0.0773\n",
      "Epoch: 0 | Loss: 0.0686\n",
      "Epoch: 0 | Loss: 0.0869\n",
      "Epoch: 0 | Loss: 0.0846\n",
      "Epoch: 0 | Loss: 0.0764\n",
      "Epoch: 0 | Loss: 0.0723\n",
      "Epoch: 0 | Loss: 0.0728\n",
      "Federated training Epoch [7/200] Val Loss: 0.0674\n",
      "test performance: [0.07138825497551732, 0.08091244337544458, 0.08607173912635405, 0.07958522909767415, 0.08283873759720424, 0.07717672734176867, 0.0830402444052982]\n",
      "Epoch: 0 | Loss: 0.0624\n",
      "Epoch: 0 | Loss: 0.0868\n",
      "Epoch: 0 | Loss: 0.0902\n",
      "Epoch: 0 | Loss: 0.0617\n",
      "Epoch: 0 | Loss: 0.0630\n",
      "Epoch: 0 | Loss: 0.0616\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Federated training Epoch [8/200] Val Loss: 0.0673\n",
      "test performance: [0.07130178320540549, 0.08041005214191463, 0.08560318049451668, 0.07964620651192453, 0.08189130828345884, 0.07696552219966503, 0.08245319902437599]\n",
      "Epoch: 0 | Loss: 0.0615\n",
      "Epoch: 0 | Loss: 0.0786\n",
      "Epoch: 0 | Loss: 0.0883\n",
      "Epoch: 0 | Loss: 0.0692\n",
      "Epoch: 0 | Loss: 0.0837\n",
      "Epoch: 0 | Loss: 0.0798\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Federated training Epoch [9/200] Val Loss: 0.0667\n",
      "test performance: [0.07069102180994129, 0.08105685477693604, 0.08573057438718946, 0.07871910758724768, 0.08237131386485001, 0.0763647087326605, 0.08264173863277044]\n",
      "Epoch: 0 | Loss: 0.0671\n",
      "Epoch: 0 | Loss: 0.0775\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Epoch: 0 | Loss: 0.0672\n",
      "Epoch: 0 | Loss: 0.0796\n",
      "Epoch: 0 | Loss: 0.0623\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Federated training Epoch [10/200] Val Loss: 0.0666\n",
      "test performance: [0.07060301025742538, 0.08048640222173847, 0.08519346869155152, 0.07866089723442923, 0.08140338989204332, 0.07609338606771542, 0.08185521090939028]\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Epoch: 0 | Loss: 0.0764\n",
      "Epoch: 0 | Loss: 0.0969\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0810\n",
      "Epoch: 0 | Loss: 0.0711\n",
      "Epoch: 0 | Loss: 0.0798\n",
      "Federated training Epoch [11/200] Val Loss: 0.0665\n",
      "test performance: [0.07060228542054761, 0.08038756067622198, 0.08503177217271639, 0.07883118243556317, 0.08100506390304599, 0.07595458173843687, 0.08159752632810237]\n",
      "Epoch: 0 | Loss: 0.0653\n",
      "Epoch: 0 | Loss: 0.0662\n",
      "Epoch: 0 | Loss: 0.0941\n",
      "Epoch: 0 | Loss: 0.0757\n",
      "Epoch: 0 | Loss: 0.0670\n",
      "Epoch: 0 | Loss: 0.0837\n",
      "Epoch: 0 | Loss: 0.0794\n",
      "Federated training Epoch [12/200] Val Loss: 0.0665\n",
      "test performance: [0.07064898509838402, 0.0799847125037484, 0.08481316603975345, 0.07890244621834526, 0.08047276509491956, 0.07600671313192746, 0.08126911588574517]\n",
      "Epoch: 0 | Loss: 0.0689\n",
      "Epoch: 0 | Loss: 0.0680\n",
      "Epoch: 0 | Loss: 0.0921\n",
      "Epoch: 0 | Loss: 0.0802\n",
      "Epoch: 0 | Loss: 0.0874\n",
      "Epoch: 0 | Loss: 0.0725\n",
      "Epoch: 0 | Loss: 0.0737\n",
      "Federated training Epoch [13/200] Val Loss: 0.0659\n",
      "test performance: [0.07012179751612552, 0.08052765227190845, 0.08493582080182148, 0.07819803047619045, 0.08077508657660386, 0.07543420235384, 0.08128251521911932]\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Epoch: 0 | Loss: 0.0772\n",
      "Epoch: 0 | Loss: 0.0724\n",
      "Epoch: 0 | Loss: 0.0775\n",
      "Epoch: 0 | Loss: 0.0568\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0841\n",
      "Federated training Epoch [14/200] Val Loss: 0.0657\n",
      "test performance: [0.06992203878774626, 0.08032871717714692, 0.08473078710065313, 0.0779404533414604, 0.08044058226099977, 0.07516437382373499, 0.08099041293592077]\n",
      "Epoch: 0 | Loss: 0.0677\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0819\n",
      "Epoch: 0 | Loss: 0.0911\n",
      "Epoch: 0 | Loss: 0.0913\n",
      "Epoch: 0 | Loss: 0.0743\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Federated training Epoch [15/200] Val Loss: 0.0658\n",
      "test performance: [0.07005234533436086, 0.0800371877849102, 0.08453311907663329, 0.07816518378788478, 0.07996521250995463, 0.07526301772474017, 0.08070120041909283]\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.0791\n",
      "Epoch: 0 | Loss: 0.0870\n",
      "Epoch: 0 | Loss: 0.0837\n",
      "Epoch: 0 | Loss: 0.0838\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Epoch: 0 | Loss: 0.0771\n",
      "Federated training Epoch [16/200] Val Loss: 0.0651\n",
      "test performance: [0.06950498922179414, 0.08071128859452598, 0.08481225505318135, 0.07727070304577889, 0.08053328736034567, 0.07469398181324136, 0.08090363908559084]\n",
      "Epoch: 0 | Loss: 0.0676\n",
      "Epoch: 0 | Loss: 0.0709\n",
      "Epoch: 0 | Loss: 0.0869\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0845\n",
      "Federated training Epoch [17/200] Val Loss: 0.0650\n",
      "test performance: [0.06943777596501455, 0.08035916558224453, 0.08467160740688648, 0.07749850948481526, 0.07998568756021049, 0.07476153268083317, 0.08070818878302019]\n",
      "Epoch: 0 | Loss: 0.0668\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.0844\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0704\n",
      "Epoch: 0 | Loss: 0.0687\n",
      "Epoch: 0 | Loss: 0.0926\n",
      "Federated training Epoch [18/200] Val Loss: 0.0647\n",
      "test performance: [0.06933339257813888, 0.08092043992795356, 0.08480046874499075, 0.07716466453283617, 0.08039071925035486, 0.07445883047958352, 0.08078098508899342]\n",
      "Epoch: 0 | Loss: 0.0522\n",
      "Epoch: 0 | Loss: 0.0797\n",
      "Epoch: 0 | Loss: 0.0792\n",
      "Epoch: 0 | Loss: 0.0615\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Epoch: 0 | Loss: 0.0642\n",
      "Epoch: 0 | Loss: 0.0747\n",
      "Federated training Epoch [19/200] Val Loss: 0.0646\n",
      "test performance: [0.06933396933472728, 0.08078947260159336, 0.08472074384558691, 0.0772850578905989, 0.0801157981718648, 0.07443251340866905, 0.08051070200968279]\n",
      "Epoch: 0 | Loss: 0.0669\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0737\n",
      "Epoch: 0 | Loss: 0.0740\n",
      "Epoch: 0 | Loss: 0.0692\n",
      "Epoch: 0 | Loss: 0.0473\n",
      "Epoch: 0 | Loss: 0.0857\n",
      "Federated training Epoch [20/200] Val Loss: 0.0652\n",
      "test performance: [0.06983067785479026, 0.08000017502281355, 0.08430660985512277, 0.07807272848711438, 0.0791833566961019, 0.07491346559653135, 0.0800674687943434]\n",
      "Epoch: 0 | Loss: 0.0688\n",
      "Epoch: 0 | Loss: 0.0746\n",
      "Epoch: 0 | Loss: 0.0926\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0736\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0848\n",
      "Federated training Epoch [21/200] Val Loss: 0.0642\n",
      "test performance: [0.06911658632173522, 0.08076984199299796, 0.0846149575169364, 0.0768855666894823, 0.0798886271244655, 0.07425594430620948, 0.0803997608998867]\n",
      "Epoch: 0 | Loss: 0.0639\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0784\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0783\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.1003\n",
      "Federated training Epoch [22/200] Val Loss: 0.0643\n",
      "test performance: [0.06922459405885167, 0.08037262046326922, 0.08441562146866975, 0.07721264225315966, 0.07940174117429208, 0.07425969430845078, 0.08014712434211006]\n",
      "Epoch: 0 | Loss: 0.0612\n",
      "Epoch: 0 | Loss: 0.0734\n",
      "Epoch: 0 | Loss: 0.0823\n",
      "Epoch: 0 | Loss: 0.0712\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0700\n",
      "Epoch: 0 | Loss: 0.0695\n",
      "Federated training Epoch [23/200] Val Loss: 0.0650\n",
      "test performance: [0.06989733665569188, 0.07989129501917999, 0.08425688929855824, 0.0783920016599028, 0.07877743801688902, 0.0748823274768991, 0.07984520679926627]\n",
      "Epoch: 0 | Loss: 0.0730\n",
      "Epoch: 0 | Loss: 0.0850\n",
      "Epoch: 0 | Loss: 0.0850\n",
      "Epoch: 0 | Loss: 0.0715\n",
      "Epoch: 0 | Loss: 0.0844\n",
      "Epoch: 0 | Loss: 0.0723\n",
      "Epoch: 0 | Loss: 0.0735\n",
      "Federated training Epoch [24/200] Val Loss: 0.0640\n",
      "test performance: [0.06912092405231032, 0.08062654099593015, 0.08441878135686051, 0.07716026714658492, 0.07937742027211679, 0.07419289788869146, 0.08004029636105446]\n",
      "Epoch: 0 | Loss: 0.0617\n",
      "Epoch: 0 | Loss: 0.0647\n",
      "Epoch: 0 | Loss: 0.0836\n",
      "Epoch: 0 | Loss: 0.0734\n",
      "Epoch: 0 | Loss: 0.0902\n",
      "Epoch: 0 | Loss: 0.0697\n",
      "Epoch: 0 | Loss: 0.0854\n",
      "Federated training Epoch [25/200] Val Loss: 0.0635\n",
      "test performance: [0.06880619366691537, 0.08102748296797684, 0.08455360956387976, 0.07653379597229092, 0.079863014005839, 0.0738594925847568, 0.08028849898135826]\n",
      "Epoch: 0 | Loss: 0.0817\n",
      "Epoch: 0 | Loss: 0.0717\n",
      "Epoch: 0 | Loss: 0.0902\n",
      "Epoch: 0 | Loss: 0.0775\n",
      "Epoch: 0 | Loss: 0.0698\n",
      "Epoch: 0 | Loss: 0.0630\n",
      "Epoch: 0 | Loss: 0.0710\n",
      "Federated training Epoch [26/200] Val Loss: 0.0635\n",
      "test performance: [0.06894535492238117, 0.08087108988467961, 0.08452638845941791, 0.07688942068090586, 0.07953487635169128, 0.07406444566911213, 0.08011405852508463]\n",
      "Epoch: 0 | Loss: 0.0562\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Epoch: 0 | Loss: 0.0752\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0614\n",
      "Epoch: 0 | Loss: 0.0642\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Federated training Epoch [27/200] Val Loss: 0.0633\n",
      "test performance: [0.06883319038642596, 0.08075771138888516, 0.08432571216141932, 0.07664389126257945, 0.07943291100992324, 0.07383254934612611, 0.07991713105560574]\n",
      "Epoch: 0 | Loss: 0.0651\n",
      "Epoch: 0 | Loss: 0.0681\n",
      "Epoch: 0 | Loss: 0.0726\n",
      "Epoch: 0 | Loss: 0.0790\n",
      "Epoch: 0 | Loss: 0.0574\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0505\n",
      "Federated training Epoch [28/200] Val Loss: 0.0631\n",
      "test performance: [0.06875059627353737, 0.08079892931480522, 0.08437784238118831, 0.07666810258763702, 0.07932353556819566, 0.07391703478379609, 0.080003207613242]\n",
      "Epoch: 0 | Loss: 0.0576\n",
      "Epoch: 0 | Loss: 0.0786\n",
      "Epoch: 0 | Loss: 0.0919\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.1045\n",
      "Epoch: 0 | Loss: 0.0737\n",
      "Epoch: 0 | Loss: 0.0710\n",
      "Federated training Epoch [29/200] Val Loss: 0.0627\n",
      "test performance: [0.06852311208486965, 0.08139548050112104, 0.08449953720483878, 0.0759620213472884, 0.08001968810615474, 0.07358970593865195, 0.08031469210982323]\n",
      "Epoch: 0 | Loss: 0.0515\n",
      "Epoch: 0 | Loss: 0.0624\n",
      "Epoch: 0 | Loss: 0.0879\n",
      "Epoch: 0 | Loss: 0.0867\n",
      "Epoch: 0 | Loss: 0.0648\n",
      "Epoch: 0 | Loss: 0.0714\n",
      "Epoch: 0 | Loss: 0.0782\n",
      "Federated training Epoch [30/200] Val Loss: 0.0627\n",
      "test performance: [0.06858090951732576, 0.08108378839615273, 0.0842377054441261, 0.07619585013910107, 0.07952642581132177, 0.07366370743982596, 0.07997524462742349]\n",
      "Epoch: 0 | Loss: 0.0596\n",
      "Epoch: 0 | Loss: 0.0756\n",
      "Epoch: 0 | Loss: 0.0891\n",
      "Epoch: 0 | Loss: 0.0606\n",
      "Epoch: 0 | Loss: 0.0850\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Federated training Epoch [31/200] Val Loss: 0.0624\n",
      "test performance: [0.06845516976836609, 0.08136981752484221, 0.08434882913142035, 0.07592963586778265, 0.07987683372291392, 0.0735297693603047, 0.0802264531359893]\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0781\n",
      "Epoch: 0 | Loss: 0.0855\n",
      "Epoch: 0 | Loss: 0.0780\n",
      "Epoch: 0 | Loss: 0.0703\n",
      "Epoch: 0 | Loss: 0.0694\n",
      "Epoch: 0 | Loss: 0.0727\n",
      "Federated training Epoch [32/200] Val Loss: 0.0622\n",
      "test performance: [0.06847475397668473, 0.08162364040896909, 0.08449210536551394, 0.07588175145832643, 0.08004388737505021, 0.07356492234776689, 0.08039568797111103]\n",
      "Epoch: 0 | Loss: 0.0611\n",
      "Epoch: 0 | Loss: 0.0593\n",
      "Epoch: 0 | Loss: 0.0718\n",
      "Epoch: 0 | Loss: 0.0704\n",
      "Epoch: 0 | Loss: 0.0764\n",
      "Epoch: 0 | Loss: 0.0519\n",
      "Epoch: 0 | Loss: 0.0766\n",
      "Federated training Epoch [33/200] Val Loss: 0.0623\n",
      "test performance: [0.06867783672291122, 0.0811911125680151, 0.08438846222699097, 0.07654273855716806, 0.07950781036984839, 0.0737929145537623, 0.08007137675144493]\n",
      "Epoch: 0 | Loss: 0.0644\n",
      "Epoch: 0 | Loss: 0.0928\n",
      "Epoch: 0 | Loss: 0.0700\n",
      "Epoch: 0 | Loss: 0.0598\n",
      "Epoch: 0 | Loss: 0.0738\n",
      "Epoch: 0 | Loss: 0.0793\n",
      "Epoch: 0 | Loss: 0.0672\n",
      "Federated training Epoch [34/200] Val Loss: 0.0623\n",
      "test performance: [0.06865940125633592, 0.08092889424464474, 0.0841058403355618, 0.07639746882072458, 0.07931258358469565, 0.07371643570902413, 0.0798441392229232]\n",
      "Epoch: 0 | Loss: 0.0634\n",
      "Epoch: 0 | Loss: 0.0732\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0694\n",
      "Epoch: 0 | Loss: 0.0704\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Epoch: 0 | Loss: 0.0751\n",
      "Federated training Epoch [35/200] Val Loss: 0.0616\n",
      "test performance: [0.06837696555287462, 0.08199405619134642, 0.08463415779071311, 0.07577877987992682, 0.08031596215314245, 0.07361997789001629, 0.0807099128652313]\n",
      "Epoch: 0 | Loss: 0.0467\n",
      "Epoch: 0 | Loss: 0.0680\n",
      "Epoch: 0 | Loss: 0.0835\n",
      "Epoch: 0 | Loss: 0.0642\n",
      "Epoch: 0 | Loss: 0.0646\n",
      "Epoch: 0 | Loss: 0.0658\n",
      "Epoch: 0 | Loss: 0.0716\n",
      "Federated training Epoch [36/200] Val Loss: 0.0624\n",
      "test performance: [0.06903570459211526, 0.08053403050472883, 0.08425765521569203, 0.07709052929118888, 0.07876957196436107, 0.07399883320274418, 0.0795993146116603]\n",
      "Epoch: 0 | Loss: 0.0499\n",
      "Epoch: 0 | Loss: 0.0671\n",
      "Epoch: 0 | Loss: 0.0841\n",
      "Epoch: 0 | Loss: 0.0673\n",
      "Epoch: 0 | Loss: 0.0587\n",
      "Epoch: 0 | Loss: 0.0689\n",
      "Epoch: 0 | Loss: 0.0673\n",
      "Federated training Epoch [37/200] Val Loss: 0.0619\n",
      "test performance: [0.06871292298685198, 0.08089688336093949, 0.08421671182580598, 0.07652591182914091, 0.079025212841185, 0.07376924602391377, 0.07984437686327385]\n",
      "Epoch: 0 | Loss: 0.0652\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0755\n",
      "Epoch: 0 | Loss: 0.0658\n",
      "Epoch: 0 | Loss: 0.0697\n",
      "Epoch: 0 | Loss: 0.0629\n",
      "Epoch: 0 | Loss: 0.0761\n",
      "Federated training Epoch [38/200] Val Loss: 0.0616\n",
      "test performance: [0.06853375481824352, 0.08113405979812553, 0.08408807212971661, 0.07605803436408304, 0.07935324981043192, 0.0735142783147015, 0.07998357466996124]\n",
      "Epoch: 0 | Loss: 0.0632\n",
      "Epoch: 0 | Loss: 0.0738\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0745\n",
      "Epoch: 0 | Loss: 0.0589\n",
      "Epoch: 0 | Loss: 0.0799\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Federated training Epoch [39/200] Val Loss: 0.0614\n",
      "test performance: [0.06831670926296957, 0.08086523158501273, 0.08401633768457256, 0.07590275116213789, 0.07921873919716844, 0.07347082083866205, 0.07990168278705176]\n",
      "Epoch: 0 | Loss: 0.0572\n",
      "Epoch: 0 | Loss: 0.0709\n",
      "Epoch: 0 | Loss: 0.0762\n",
      "Epoch: 0 | Loss: 0.0733\n",
      "Epoch: 0 | Loss: 0.0675\n",
      "Epoch: 0 | Loss: 0.0645\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Federated training Epoch [40/200] Val Loss: 0.0611\n",
      "test performance: [0.0684068810500919, 0.08113182378192879, 0.08432386747931372, 0.07591890442912301, 0.0793816903861214, 0.07360738162461618, 0.080039546148826]\n",
      "Epoch: 0 | Loss: 0.0539\n",
      "Epoch: 0 | Loss: 0.0668\n",
      "Epoch: 0 | Loss: 0.0764\n",
      "Epoch: 0 | Loss: 0.0645\n",
      "Epoch: 0 | Loss: 0.0776\n",
      "Epoch: 0 | Loss: 0.0674\n",
      "Epoch: 0 | Loss: 0.0892\n",
      "Federated training Epoch [41/200] Val Loss: 0.0611\n",
      "test performance: [0.06837185095893601, 0.08081918573389722, 0.08409349391976856, 0.07598080380169088, 0.07907619309445767, 0.07350194304926347, 0.07976866485423421]\n",
      "Epoch: 0 | Loss: 0.0608\n",
      "Epoch: 0 | Loss: 0.0690\n",
      "Epoch: 0 | Loss: 0.0813\n",
      "Epoch: 0 | Loss: 0.0572\n",
      "Epoch: 0 | Loss: 0.0906\n",
      "Epoch: 0 | Loss: 0.0655\n",
      "Epoch: 0 | Loss: 0.0557\n",
      "Federated training Epoch [42/200] Val Loss: 0.0618\n",
      "test performance: [0.06906546145830661, 0.08057248482659254, 0.08441198152834423, 0.07729880708231501, 0.07867357420594726, 0.07411880018061971, 0.07958041019227406]\n",
      "Epoch: 0 | Loss: 0.0571\n",
      "Epoch: 0 | Loss: 0.0660\n",
      "Epoch: 0 | Loss: 0.0819\n",
      "Epoch: 0 | Loss: 0.0756\n",
      "Epoch: 0 | Loss: 0.0633\n",
      "Epoch: 0 | Loss: 0.0693\n",
      "Epoch: 0 | Loss: 0.0796\n",
      "Federated training Epoch [43/200] Val Loss: 0.0611\n",
      "test performance: [0.06851080128897542, 0.08085298316221531, 0.08402112105937853, 0.07625336651626514, 0.07904409604427749, 0.07347527003451569, 0.07968336463688988]\n",
      "Epoch: 0 | Loss: 0.0643\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0857\n",
      "Epoch: 0 | Loss: 0.0618\n",
      "Epoch: 0 | Loss: 0.0867\n",
      "Epoch: 0 | Loss: 0.0529\n",
      "Epoch: 0 | Loss: 0.0777\n",
      "Federated training Epoch [44/200] Val Loss: 0.0614\n",
      "test performance: [0.0689699219743887, 0.0806605092390147, 0.08430966344496159, 0.07702426572827852, 0.07886034792467747, 0.07389794348751845, 0.07968396582474856]\n",
      "Epoch: 0 | Loss: 0.0766\n",
      "Epoch: 0 | Loss: 0.0869\n",
      "Epoch: 0 | Loss: 0.0785\n",
      "Epoch: 0 | Loss: 0.0690\n",
      "Epoch: 0 | Loss: 0.0721\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Epoch: 0 | Loss: 0.0638\n",
      "Federated training Epoch [45/200] Val Loss: 0.0611\n",
      "test performance: [0.06870397746767083, 0.08077686565787826, 0.08394921624599254, 0.07651306645408885, 0.07908989587313917, 0.07350003742293952, 0.07959863000026304]\n",
      "Epoch: 0 | Loss: 0.0597\n",
      "Epoch: 0 | Loss: 0.0684\n",
      "Epoch: 0 | Loss: 0.0795\n",
      "Epoch: 0 | Loss: 0.0723\n",
      "Epoch: 0 | Loss: 0.0702\n",
      "Epoch: 0 | Loss: 0.0706\n",
      "Epoch: 0 | Loss: 0.0643\n",
      "Federated training Epoch [46/200] Val Loss: 0.0606\n",
      "test performance: [0.06826009792722251, 0.08064979304597802, 0.08388235996643158, 0.07579932236814335, 0.07905434721391903, 0.07327257210312232, 0.07968430080744501]\n",
      "Epoch: 0 | Loss: 0.0571\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0801\n",
      "Epoch: 0 | Loss: 0.0638\n",
      "Epoch: 0 | Loss: 0.0808\n",
      "Epoch: 0 | Loss: 0.0650\n",
      "Epoch: 0 | Loss: 0.0733\n",
      "Federated training Epoch [47/200] Val Loss: 0.0604\n",
      "test performance: [0.06812603529287528, 0.08081777099111717, 0.08388961961314287, 0.07573853716356298, 0.07927503762128826, 0.07313485474210896, 0.07986458531287435]\n",
      "Epoch: 0 | Loss: 0.0636\n",
      "Epoch: 0 | Loss: 0.0726\n",
      "Epoch: 0 | Loss: 0.0971\n",
      "Epoch: 0 | Loss: 0.0624\n",
      "Epoch: 0 | Loss: 0.0765\n",
      "Epoch: 0 | Loss: 0.0639\n",
      "Epoch: 0 | Loss: 0.0760\n",
      "Federated training Epoch [48/200] Val Loss: 0.0607\n",
      "test performance: [0.06860915614827855, 0.08070642874920614, 0.08413171779670536, 0.07640821230278848, 0.07891580144785447, 0.07353172416811528, 0.07952195404684298]\n",
      "Epoch: 0 | Loss: 0.0644\n",
      "Epoch: 0 | Loss: 0.0692\n",
      "Epoch: 0 | Loss: 0.0769\n",
      "Epoch: 0 | Loss: 0.0670\n",
      "Epoch: 0 | Loss: 0.0811\n",
      "Epoch: 0 | Loss: 0.0634\n",
      "Epoch: 0 | Loss: 0.0736\n",
      "Federated training Epoch [49/200] Val Loss: 0.0610\n",
      "test performance: [0.06891155152339233, 0.08064420778967746, 0.08397206148668511, 0.0769617095819279, 0.07891245329216735, 0.07356679002588538, 0.07948885512321371]\n",
      "Epoch: 0 | Loss: 0.0648\n",
      "Epoch: 0 | Loss: 0.0688\n",
      "Epoch: 0 | Loss: 0.0837\n",
      "Epoch: 0 | Loss: 0.0704\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.0606\n",
      "Epoch: 0 | Loss: 0.0692\n",
      "Federated training Epoch [50/200] Val Loss: 0.0603\n",
      "test performance: [0.06851804783338145, 0.08058178233467553, 0.08421401488148186, 0.07635116607767262, 0.07893082614324681, 0.07343522893035248, 0.07955289013326576]\n",
      "Epoch: 0 | Loss: 0.0546\n",
      "Epoch: 0 | Loss: 0.0675\n",
      "Epoch: 0 | Loss: 0.0682\n",
      "Epoch: 0 | Loss: 0.0749\n",
      "Epoch: 0 | Loss: 0.0706\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0637\n",
      "Federated training Epoch [51/200] Val Loss: 0.0601\n",
      "test performance: [0.068298143640875, 0.08108294184944809, 0.0839538546916965, 0.07586900522447612, 0.0794620516340006, 0.07303107458434693, 0.07984725100128617]\n",
      "Epoch: 0 | Loss: 0.0554\n",
      "Epoch: 0 | Loss: 0.0628\n",
      "Epoch: 0 | Loss: 0.0789\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0784\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.0660\n",
      "Federated training Epoch [52/200] Val Loss: 0.0603\n",
      "test performance: [0.06857747691747261, 0.08068095614546783, 0.08404892593724271, 0.07658142574794896, 0.07900413046654774, 0.07330061130429784, 0.07955223258125456]\n",
      "Epoch: 0 | Loss: 0.0670\n",
      "Epoch: 0 | Loss: 0.0793\n",
      "Epoch: 0 | Loss: 0.0813\n",
      "Epoch: 0 | Loss: 0.0797\n",
      "Epoch: 0 | Loss: 0.0830\n",
      "Epoch: 0 | Loss: 0.0659\n",
      "Epoch: 0 | Loss: 0.0717\n",
      "Federated training Epoch [53/200] Val Loss: 0.0597\n",
      "test performance: [0.06796777646427285, 0.08082262500610253, 0.08352347145699067, 0.07567443560859928, 0.07952636790347017, 0.07286257063332077, 0.0796397326590672]\n",
      "Epoch: 0 | Loss: 0.0532\n",
      "Epoch: 0 | Loss: 0.0648\n",
      "Epoch: 0 | Loss: 0.0914\n",
      "Epoch: 0 | Loss: 0.0669\n",
      "Epoch: 0 | Loss: 0.0645\n",
      "Epoch: 0 | Loss: 0.0644\n",
      "Epoch: 0 | Loss: 0.0728\n",
      "Federated training Epoch [54/200] Val Loss: 0.0599\n",
      "test performance: [0.06833678839906845, 0.08080196010637773, 0.08382080820682522, 0.0760268921109095, 0.07924741276935356, 0.0729436221507008, 0.07957068232385671]\n",
      "Epoch: 0 | Loss: 0.0679\n",
      "Epoch: 0 | Loss: 0.0654\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0625\n",
      "Epoch: 0 | Loss: 0.0731\n",
      "Epoch: 0 | Loss: 0.0569\n",
      "Epoch: 0 | Loss: 0.0689\n",
      "Federated training Epoch [55/200] Val Loss: 0.0597\n",
      "test performance: [0.06826111643689953, 0.08101714968885461, 0.08373324172443723, 0.07605453246362405, 0.07951009767257596, 0.0729072009761856, 0.07973209315630263]\n",
      "Epoch: 0 | Loss: 0.0567\n",
      "Epoch: 0 | Loss: 0.0729\n",
      "Epoch: 0 | Loss: 0.0865\n",
      "Epoch: 0 | Loss: 0.0581\n",
      "Epoch: 0 | Loss: 0.0728\n",
      "Epoch: 0 | Loss: 0.0522\n",
      "Epoch: 0 | Loss: 0.0694\n",
      "Federated training Epoch [56/200] Val Loss: 0.0599\n",
      "test performance: [0.06853176076730637, 0.08031145675898824, 0.08407809800940426, 0.07638922306925874, 0.07869224844832126, 0.07316301817916436, 0.07914117534887301]\n",
      "Epoch: 0 | Loss: 0.0550\n",
      "Epoch: 0 | Loss: 0.0806\n",
      "Epoch: 0 | Loss: 0.0790\n",
      "Epoch: 0 | Loss: 0.0539\n",
      "Epoch: 0 | Loss: 0.0719\n",
      "Epoch: 0 | Loss: 0.0576\n",
      "Epoch: 0 | Loss: 0.0717\n",
      "Federated training Epoch [57/200] Val Loss: 0.0595\n",
      "test performance: [0.06815191622415226, 0.08041772461968334, 0.08367762588321755, 0.07599460388444466, 0.0791139813037972, 0.07281925788863677, 0.07924189810220102]\n",
      "Epoch: 0 | Loss: 0.0588\n",
      "Epoch: 0 | Loss: 0.0704\n",
      "Epoch: 0 | Loss: 0.0696\n",
      "Epoch: 0 | Loss: 0.0707\n",
      "Epoch: 0 | Loss: 0.0715\n",
      "Epoch: 0 | Loss: 0.0559\n",
      "Epoch: 0 | Loss: 0.0609\n",
      "Federated training Epoch [58/200] Val Loss: 0.0597\n",
      "test performance: [0.06848742618952712, 0.08042379369168248, 0.08376365768633885, 0.07647770135472082, 0.0791545327727313, 0.07303156845322618, 0.07916174197492942]\n",
      "Epoch: 0 | Loss: 0.0617\n",
      "Epoch: 0 | Loss: 0.0713\n",
      "Epoch: 0 | Loss: 0.0678\n",
      "Epoch: 0 | Loss: 0.0680\n",
      "Epoch: 0 | Loss: 0.0744\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Epoch: 0 | Loss: 0.0646\n",
      "Federated training Epoch [59/200] Val Loss: 0.0594\n",
      "test performance: [0.06837398182498673, 0.08104457558222013, 0.08386846190343981, 0.07606125993603101, 0.07961059666608702, 0.07275381538225975, 0.0794892175735472]\n",
      "Epoch: 0 | Loss: 0.0558\n",
      "Epoch: 0 | Loss: 0.0567\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Epoch: 0 | Loss: 0.0489\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Epoch: 0 | Loss: 0.0758\n",
      "Epoch: 0 | Loss: 0.0664\n",
      "Federated training Epoch [60/200] Val Loss: 0.0590\n",
      "test performance: [0.06806700785121281, 0.08067830003900071, 0.0837197758589093, 0.07570394659287309, 0.07964176914259179, 0.07245494113367511, 0.07955778480749832]\n",
      "Epoch: 0 | Loss: 0.0509\n",
      "Epoch: 0 | Loss: 0.0863\n",
      "Epoch: 0 | Loss: 0.0719\n",
      "Epoch: 0 | Loss: 0.0760\n",
      "Epoch: 0 | Loss: 0.0591\n",
      "Epoch: 0 | Loss: 0.0592\n",
      "Epoch: 0 | Loss: 0.0725\n",
      "Federated training Epoch [61/200] Val Loss: 0.0591\n",
      "test performance: [0.06812124307688376, 0.08056092045384727, 0.08365760464221239, 0.07600703149115386, 0.07966756767095769, 0.07259751324962875, 0.07949218884977984]\n",
      "Epoch: 0 | Loss: 0.0575\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0748\n",
      "Epoch: 0 | Loss: 0.0649\n",
      "Epoch: 0 | Loss: 0.0854\n",
      "Epoch: 0 | Loss: 0.0624\n",
      "Epoch: 0 | Loss: 0.0627\n",
      "Federated training Epoch [62/200] Val Loss: 0.0591\n",
      "test performance: [0.06820161359971516, 0.08009375880587183, 0.08380410392857986, 0.07603655414885446, 0.07902521467831446, 0.07244930878817422, 0.07900003804378722]\n",
      "Epoch: 0 | Loss: 0.0608\n",
      "Epoch: 0 | Loss: 0.0754\n",
      "Epoch: 0 | Loss: 0.0689\n",
      "Epoch: 0 | Loss: 0.0627\n",
      "Epoch: 0 | Loss: 0.0600\n",
      "Epoch: 0 | Loss: 0.0515\n",
      "Epoch: 0 | Loss: 0.0705\n",
      "Federated training Epoch [63/200] Val Loss: 0.0595\n",
      "test performance: [0.0685242324914426, 0.07964595964765303, 0.08366349968446853, 0.07632291587452365, 0.07848308398707272, 0.07255934123332573, 0.07861896701259156]\n",
      "Epoch: 0 | Loss: 0.0680\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0695\n",
      "Epoch: 0 | Loss: 0.0720\n",
      "Epoch: 0 | Loss: 0.0649\n",
      "Epoch: 0 | Loss: 0.0582\n",
      "Epoch: 0 | Loss: 0.0535\n",
      "Federated training Epoch [64/200] Val Loss: 0.0594\n",
      "test performance: [0.06852350696564129, 0.07971251371941745, 0.08362025054401323, 0.07641644663598439, 0.07863820443720851, 0.07247768086700203, 0.07836309097681755]\n",
      "Epoch: 0 | Loss: 0.0549\n",
      "Epoch: 0 | Loss: 0.0641\n",
      "Epoch: 0 | Loss: 0.0816\n",
      "Epoch: 0 | Loss: 0.0601\n",
      "Epoch: 0 | Loss: 0.0708\n",
      "Epoch: 0 | Loss: 0.0538\n",
      "Epoch: 0 | Loss: 0.0541\n",
      "Federated training Epoch [65/200] Val Loss: 0.0593\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.fed_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Train Loss: 0.0941 Val Loss: 0.0752\n",
      "Epoch [2/200] Train Loss: 0.0763 Val Loss: 0.0731\n",
      "Epoch [3/200] Train Loss: 0.0730 Val Loss: 0.0701\n",
      "Epoch [4/200] Train Loss: 0.0701 Val Loss: 0.0685\n",
      "Epoch [5/200] Train Loss: 0.0689 Val Loss: 0.0680\n",
      "Epoch [6/200] Train Loss: 0.0682 Val Loss: 0.0678\n",
      "Epoch [7/200] Train Loss: 0.0676 Val Loss: 0.0669\n",
      "Epoch [8/200] Train Loss: 0.0670 Val Loss: 0.0661\n",
      "Epoch [9/200] Train Loss: 0.0665 Val Loss: 0.0657\n",
      "Epoch [10/200] Train Loss: 0.0663 Val Loss: 0.0653\n",
      "Epoch [11/200] Train Loss: 0.0658 Val Loss: 0.0650\n",
      "Epoch [12/200] Train Loss: 0.0654 Val Loss: 0.0649\n",
      "Epoch [13/200] Train Loss: 0.0650 Val Loss: 0.0646\n",
      "Epoch [14/200] Train Loss: 0.0645 Val Loss: 0.0641\n",
      "Epoch [15/200] Train Loss: 0.0643 Val Loss: 0.0635\n",
      "Epoch [16/200] Train Loss: 0.0638 Val Loss: 0.0633\n",
      "Epoch [17/200] Train Loss: 0.0635 Val Loss: 0.0629\n",
      "Epoch [18/200] Train Loss: 0.0632 Val Loss: 0.0631\n",
      "Epoch [19/200] Train Loss: 0.0628 Val Loss: 0.0628\n",
      "Epoch [20/200] Train Loss: 0.0625 Val Loss: 0.0626\n",
      "Epoch [21/200] Train Loss: 0.0623 Val Loss: 0.0617\n",
      "Epoch [22/200] Train Loss: 0.0618 Val Loss: 0.0619\n",
      "Epoch [23/200] Train Loss: 0.0615 Val Loss: 0.0612\n",
      "Epoch [24/200] Train Loss: 0.0614 Val Loss: 0.0610\n",
      "Epoch [25/200] Train Loss: 0.0613 Val Loss: 0.0609\n",
      "Epoch [26/200] Train Loss: 0.0608 Val Loss: 0.0615\n",
      "Epoch [27/200] Train Loss: 0.0605 Val Loss: 0.0616\n",
      "Epoch [28/200] Train Loss: 0.0602 Val Loss: 0.0621\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=293, out_features=64, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.central_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch Local Training!\n",
      "Epoch [1/200] Train Loss: 0.0877 Val Loss: 0.0745\n",
      "Epoch [2/200] Train Loss: 0.0747 Val Loss: 0.0717\n",
      "Epoch [3/200] Train Loss: 0.0713 Val Loss: 0.0692\n",
      "Epoch [4/200] Train Loss: 0.0695 Val Loss: 0.0683\n",
      "Epoch [5/200] Train Loss: 0.0685 Val Loss: 0.0680\n",
      "Epoch [6/200] Train Loss: 0.0680 Val Loss: 0.0675\n",
      "Epoch [7/200] Train Loss: 0.0672 Val Loss: 0.0667\n",
      "Epoch [8/200] Train Loss: 0.0667 Val Loss: 0.0658\n",
      "Epoch [9/200] Train Loss: 0.0661 Val Loss: 0.0653\n",
      "Epoch [10/200] Train Loss: 0.0656 Val Loss: 0.0657\n",
      "Epoch [11/200] Train Loss: 0.0651 Val Loss: 0.0654\n",
      "Epoch [12/200] Train Loss: 0.0646 Val Loss: 0.0638\n",
      "Epoch [13/200] Train Loss: 0.0642 Val Loss: 0.0640\n",
      "Epoch [14/200] Train Loss: 0.0636 Val Loss: 0.0630\n",
      "Epoch [15/200] Train Loss: 0.0633 Val Loss: 0.0627\n",
      "Epoch [16/200] Train Loss: 0.0627 Val Loss: 0.0628\n",
      "Epoch [17/200] Train Loss: 0.0622 Val Loss: 0.0619\n",
      "Epoch [18/200] Train Loss: 0.0618 Val Loss: 0.0621\n",
      "Epoch [19/200] Train Loss: 0.0615 Val Loss: 0.0611\n",
      "Epoch [20/200] Train Loss: 0.0614 Val Loss: 0.0616\n",
      "Epoch [21/200] Train Loss: 0.0609 Val Loss: 0.0606\n",
      "Epoch [22/200] Train Loss: 0.0604 Val Loss: 0.0605\n",
      "Epoch [23/200] Train Loss: 0.0602 Val Loss: 0.0618\n",
      "Epoch [24/200] Train Loss: 0.0598 Val Loss: 0.0601\n",
      "Epoch [25/200] Train Loss: 0.0595 Val Loss: 0.0600\n",
      "Epoch [26/200] Train Loss: 0.0594 Val Loss: 0.0594\n",
      "Epoch [27/200] Train Loss: 0.0590 Val Loss: 0.0595\n",
      "Epoch [28/200] Train Loss: 0.0586 Val Loss: 0.0588\n",
      "Epoch [29/200] Train Loss: 0.0585 Val Loss: 0.0587\n",
      "Epoch [30/200] Train Loss: 0.0581 Val Loss: 0.0587\n",
      "Epoch [31/200] Train Loss: 0.0580 Val Loss: 0.0589\n",
      "Epoch [32/200] Train Loss: 0.0576 Val Loss: 0.0602\n",
      "Epoch [33/200] Train Loss: 0.0578 Val Loss: 0.0581\n",
      "Epoch [34/200] Train Loss: 0.0572 Val Loss: 0.0577\n",
      "Epoch [35/200] Train Loss: 0.0572 Val Loss: 0.0575\n",
      "Epoch [36/200] Train Loss: 0.0568 Val Loss: 0.0590\n",
      "Epoch [37/200] Train Loss: 0.0566 Val Loss: 0.0589\n",
      "Epoch [38/200] Train Loss: 0.0562 Val Loss: 0.0572\n",
      "Epoch [39/200] Train Loss: 0.0562 Val Loss: 0.0574\n",
      "Epoch [40/200] Train Loss: 0.0559 Val Loss: 0.0571\n",
      "Epoch [41/200] Train Loss: 0.0559 Val Loss: 0.0572\n",
      "Epoch [42/200] Train Loss: 0.0555 Val Loss: 0.0569\n",
      "Epoch [43/200] Train Loss: 0.0556 Val Loss: 0.0604\n",
      "Epoch [44/200] Train Loss: 0.0551 Val Loss: 0.0561\n",
      "Epoch [45/200] Train Loss: 0.0549 Val Loss: 0.0569\n",
      "Epoch [46/200] Train Loss: 0.0546 Val Loss: 0.0560\n",
      "Epoch [47/200] Train Loss: 0.0543 Val Loss: 0.0558\n",
      "Epoch [48/200] Train Loss: 0.0544 Val Loss: 0.0575\n",
      "Epoch [49/200] Train Loss: 0.0541 Val Loss: 0.0559\n",
      "Epoch [50/200] Train Loss: 0.0538 Val Loss: 0.0559\n",
      "Epoch [51/200] Train Loss: 0.0537 Val Loss: 0.0553\n",
      "Epoch [52/200] Train Loss: 0.0533 Val Loss: 0.0556\n",
      "Epoch [53/200] Train Loss: 0.0530 Val Loss: 0.0552\n",
      "Epoch [54/200] Train Loss: 0.0535 Val Loss: 0.0552\n",
      "Epoch [55/200] Train Loss: 0.0528 Val Loss: 0.0551\n",
      "Epoch [56/200] Train Loss: 0.0526 Val Loss: 0.0545\n",
      "Epoch [57/200] Train Loss: 0.0523 Val Loss: 0.0545\n",
      "Epoch [58/200] Train Loss: 0.0521 Val Loss: 0.0544\n",
      "Epoch [59/200] Train Loss: 0.0519 Val Loss: 0.0541\n",
      "Epoch [60/200] Train Loss: 0.0519 Val Loss: 0.0543\n",
      "Epoch [61/200] Train Loss: 0.0516 Val Loss: 0.0550\n",
      "Epoch [62/200] Train Loss: 0.0514 Val Loss: 0.0535\n",
      "Epoch [63/200] Train Loss: 0.0508 Val Loss: 0.0536\n",
      "Epoch [64/200] Train Loss: 0.0508 Val Loss: 0.0550\n",
      "Epoch [65/200] Train Loss: 0.0507 Val Loss: 0.0532\n",
      "Epoch [66/200] Train Loss: 0.0503 Val Loss: 0.0535\n",
      "Epoch [67/200] Train Loss: 0.0501 Val Loss: 0.0529\n",
      "Epoch [68/200] Train Loss: 0.0501 Val Loss: 0.0532\n",
      "Epoch [69/200] Train Loss: 0.0498 Val Loss: 0.0535\n",
      "Epoch [70/200] Train Loss: 0.0495 Val Loss: 0.0527\n",
      "Epoch [71/200] Train Loss: 0.0491 Val Loss: 0.0527\n",
      "Epoch [72/200] Train Loss: 0.0492 Val Loss: 0.0517\n",
      "Epoch [73/200] Train Loss: 0.0487 Val Loss: 0.0525\n",
      "Epoch [74/200] Train Loss: 0.0485 Val Loss: 0.0517\n",
      "Epoch [75/200] Train Loss: 0.0485 Val Loss: 0.0520\n",
      "Epoch [76/200] Train Loss: 0.0480 Val Loss: 0.0515\n",
      "Epoch [77/200] Train Loss: 0.0479 Val Loss: 0.0522\n",
      "Epoch [78/200] Train Loss: 0.0478 Val Loss: 0.0532\n",
      "Epoch [79/200] Train Loss: 0.0475 Val Loss: 0.0507\n",
      "Epoch [80/200] Train Loss: 0.0469 Val Loss: 0.0506\n",
      "Epoch [81/200] Train Loss: 0.0470 Val Loss: 0.0504\n",
      "Epoch [82/200] Train Loss: 0.0467 Val Loss: 0.0510\n",
      "Epoch [83/200] Train Loss: 0.0465 Val Loss: 0.0511\n",
      "Epoch [84/200] Train Loss: 0.0461 Val Loss: 0.0503\n",
      "Epoch [85/200] Train Loss: 0.0458 Val Loss: 0.0503\n",
      "Epoch [86/200] Train Loss: 0.0461 Val Loss: 0.0502\n",
      "Epoch [87/200] Train Loss: 0.0458 Val Loss: 0.0496\n",
      "Epoch [88/200] Train Loss: 0.0452 Val Loss: 0.0495\n",
      "Epoch [89/200] Train Loss: 0.0453 Val Loss: 0.0497\n",
      "Epoch [90/200] Train Loss: 0.0447 Val Loss: 0.0515\n",
      "Epoch [91/200] Train Loss: 0.0446 Val Loss: 0.0490\n",
      "Epoch [92/200] Train Loss: 0.0445 Val Loss: 0.0498\n",
      "Epoch [93/200] Train Loss: 0.0445 Val Loss: 0.0491\n",
      "Epoch [94/200] Train Loss: 0.0438 Val Loss: 0.0480\n",
      "Epoch [95/200] Train Loss: 0.0439 Val Loss: 0.0479\n",
      "Epoch [96/200] Train Loss: 0.0435 Val Loss: 0.0488\n",
      "Epoch [97/200] Train Loss: 0.0434 Val Loss: 0.0476\n",
      "Epoch [98/200] Train Loss: 0.0431 Val Loss: 0.0477\n",
      "Epoch [99/200] Train Loss: 0.0430 Val Loss: 0.0475\n",
      "Epoch [100/200] Train Loss: 0.0427 Val Loss: 0.0476\n",
      "Epoch [101/200] Train Loss: 0.0425 Val Loss: 0.0473\n",
      "Epoch [102/200] Train Loss: 0.0423 Val Loss: 0.0477\n",
      "Epoch [103/200] Train Loss: 0.0421 Val Loss: 0.0472\n",
      "Epoch [104/200] Train Loss: 0.0423 Val Loss: 0.0497\n",
      "Epoch [105/200] Train Loss: 0.0415 Val Loss: 0.0466\n",
      "Epoch [106/200] Train Loss: 0.0415 Val Loss: 0.0473\n",
      "Epoch [107/200] Train Loss: 0.0414 Val Loss: 0.0463\n",
      "Epoch [108/200] Train Loss: 0.0410 Val Loss: 0.0463\n",
      "Epoch [109/200] Train Loss: 0.0410 Val Loss: 0.0466\n",
      "Epoch [110/200] Train Loss: 0.0409 Val Loss: 0.0470\n",
      "Epoch [111/200] Train Loss: 0.0406 Val Loss: 0.0457\n",
      "Epoch [112/200] Train Loss: 0.0404 Val Loss: 0.0472\n",
      "Epoch [113/200] Train Loss: 0.0405 Val Loss: 0.0456\n",
      "Epoch [114/200] Train Loss: 0.0405 Val Loss: 0.0458\n",
      "Epoch [115/200] Train Loss: 0.0401 Val Loss: 0.0453\n",
      "Epoch [116/200] Train Loss: 0.0400 Val Loss: 0.0456\n",
      "Epoch [117/200] Train Loss: 0.0393 Val Loss: 0.0453\n",
      "Epoch [118/200] Train Loss: 0.0392 Val Loss: 0.0464\n",
      "Epoch [119/200] Train Loss: 0.0393 Val Loss: 0.0453\n",
      "Epoch [120/200] Train Loss: 0.0391 Val Loss: 0.0449\n",
      "Epoch [121/200] Train Loss: 0.0391 Val Loss: 0.0448\n",
      "Epoch [122/200] Train Loss: 0.0389 Val Loss: 0.0471\n",
      "Epoch [123/200] Train Loss: 0.0386 Val Loss: 0.0455\n",
      "Epoch [124/200] Train Loss: 0.0386 Val Loss: 0.0445\n",
      "Epoch [125/200] Train Loss: 0.0385 Val Loss: 0.0446\n",
      "Epoch [126/200] Train Loss: 0.0386 Val Loss: 0.0440\n",
      "Epoch [127/200] Train Loss: 0.0381 Val Loss: 0.0438\n",
      "Epoch [128/200] Train Loss: 0.0380 Val Loss: 0.0444\n",
      "Epoch [129/200] Train Loss: 0.0380 Val Loss: 0.0463\n",
      "Epoch [130/200] Train Loss: 0.0376 Val Loss: 0.0438\n",
      "Epoch [131/200] Train Loss: 0.0376 Val Loss: 0.0441\n",
      "Epoch [132/200] Train Loss: 0.0378 Val Loss: 0.0445\n",
      "Epoch [133/200] Train Loss: 0.0374 Val Loss: 0.0439\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0917 Val Loss: 0.0786\n",
      "Epoch [2/200] Train Loss: 0.0791 Val Loss: 0.0749\n",
      "Epoch [3/200] Train Loss: 0.0760 Val Loss: 0.0732\n",
      "Epoch [4/200] Train Loss: 0.0747 Val Loss: 0.0728\n",
      "Epoch [5/200] Train Loss: 0.0742 Val Loss: 0.0723\n",
      "Epoch [6/200] Train Loss: 0.0737 Val Loss: 0.0714\n",
      "Epoch [7/200] Train Loss: 0.0735 Val Loss: 0.0710\n",
      "Epoch [8/200] Train Loss: 0.0730 Val Loss: 0.0707\n",
      "Epoch [9/200] Train Loss: 0.0726 Val Loss: 0.0703\n",
      "Epoch [10/200] Train Loss: 0.0723 Val Loss: 0.0699\n",
      "Epoch [11/200] Train Loss: 0.0720 Val Loss: 0.0699\n",
      "Epoch [12/200] Train Loss: 0.0715 Val Loss: 0.0692\n",
      "Epoch [13/200] Train Loss: 0.0711 Val Loss: 0.0691\n",
      "Epoch [14/200] Train Loss: 0.0706 Val Loss: 0.0688\n",
      "Epoch [15/200] Train Loss: 0.0704 Val Loss: 0.0693\n",
      "Epoch [16/200] Train Loss: 0.0699 Val Loss: 0.0678\n",
      "Epoch [17/200] Train Loss: 0.0694 Val Loss: 0.0673\n",
      "Epoch [18/200] Train Loss: 0.0688 Val Loss: 0.0687\n",
      "Epoch [19/200] Train Loss: 0.0685 Val Loss: 0.0667\n",
      "Epoch [20/200] Train Loss: 0.0679 Val Loss: 0.0663\n",
      "Epoch [21/200] Train Loss: 0.0675 Val Loss: 0.0657\n",
      "Epoch [22/200] Train Loss: 0.0671 Val Loss: 0.0652\n",
      "Epoch [23/200] Train Loss: 0.0666 Val Loss: 0.0649\n",
      "Epoch [24/200] Train Loss: 0.0661 Val Loss: 0.0655\n",
      "Epoch [25/200] Train Loss: 0.0657 Val Loss: 0.0647\n",
      "Epoch [26/200] Train Loss: 0.0654 Val Loss: 0.0637\n",
      "Epoch [27/200] Train Loss: 0.0649 Val Loss: 0.0633\n",
      "Epoch [28/200] Train Loss: 0.0647 Val Loss: 0.0639\n",
      "Epoch [29/200] Train Loss: 0.0641 Val Loss: 0.0627\n",
      "Epoch [30/200] Train Loss: 0.0638 Val Loss: 0.0631\n",
      "Epoch [31/200] Train Loss: 0.0633 Val Loss: 0.0627\n",
      "Epoch [32/200] Train Loss: 0.0633 Val Loss: 0.0622\n",
      "Epoch [33/200] Train Loss: 0.0627 Val Loss: 0.0617\n",
      "Epoch [34/200] Train Loss: 0.0623 Val Loss: 0.0616\n",
      "Epoch [35/200] Train Loss: 0.0620 Val Loss: 0.0612\n",
      "Epoch [36/200] Train Loss: 0.0615 Val Loss: 0.0615\n",
      "Epoch [37/200] Train Loss: 0.0612 Val Loss: 0.0614\n",
      "Epoch [38/200] Train Loss: 0.0608 Val Loss: 0.0605\n",
      "Epoch [39/200] Train Loss: 0.0608 Val Loss: 0.0599\n",
      "Epoch [40/200] Train Loss: 0.0600 Val Loss: 0.0605\n",
      "Epoch [41/200] Train Loss: 0.0598 Val Loss: 0.0599\n",
      "Epoch [42/200] Train Loss: 0.0593 Val Loss: 0.0599\n",
      "Epoch [43/200] Train Loss: 0.0589 Val Loss: 0.0593\n",
      "Epoch [44/200] Train Loss: 0.0585 Val Loss: 0.0611\n",
      "Epoch [45/200] Train Loss: 0.0580 Val Loss: 0.0584\n",
      "Epoch [46/200] Train Loss: 0.0580 Val Loss: 0.0592\n",
      "Epoch [47/200] Train Loss: 0.0573 Val Loss: 0.0584\n",
      "Epoch [48/200] Train Loss: 0.0566 Val Loss: 0.0579\n",
      "Epoch [49/200] Train Loss: 0.0566 Val Loss: 0.0575\n",
      "Epoch [50/200] Train Loss: 0.0560 Val Loss: 0.0570\n",
      "Epoch [51/200] Train Loss: 0.0554 Val Loss: 0.0575\n",
      "Epoch [52/200] Train Loss: 0.0551 Val Loss: 0.0568\n",
      "Epoch [53/200] Train Loss: 0.0547 Val Loss: 0.0560\n",
      "Epoch [54/200] Train Loss: 0.0541 Val Loss: 0.0559\n",
      "Epoch [55/200] Train Loss: 0.0539 Val Loss: 0.0555\n",
      "Epoch [56/200] Train Loss: 0.0533 Val Loss: 0.0553\n",
      "Epoch [57/200] Train Loss: 0.0526 Val Loss: 0.0548\n",
      "Epoch [58/200] Train Loss: 0.0525 Val Loss: 0.0546\n",
      "Epoch [59/200] Train Loss: 0.0519 Val Loss: 0.0557\n",
      "Epoch [60/200] Train Loss: 0.0516 Val Loss: 0.0542\n",
      "Epoch [61/200] Train Loss: 0.0513 Val Loss: 0.0532\n",
      "Epoch [62/200] Train Loss: 0.0508 Val Loss: 0.0530\n",
      "Epoch [63/200] Train Loss: 0.0503 Val Loss: 0.0527\n",
      "Epoch [64/200] Train Loss: 0.0499 Val Loss: 0.0521\n",
      "Epoch [65/200] Train Loss: 0.0497 Val Loss: 0.0533\n",
      "Epoch [66/200] Train Loss: 0.0490 Val Loss: 0.0525\n",
      "Epoch [67/200] Train Loss: 0.0490 Val Loss: 0.0517\n",
      "Epoch [68/200] Train Loss: 0.0484 Val Loss: 0.0538\n",
      "Epoch [69/200] Train Loss: 0.0480 Val Loss: 0.0511\n",
      "Epoch [70/200] Train Loss: 0.0473 Val Loss: 0.0511\n",
      "Epoch [71/200] Train Loss: 0.0471 Val Loss: 0.0523\n",
      "Epoch [72/200] Train Loss: 0.0470 Val Loss: 0.0499\n",
      "Epoch [73/200] Train Loss: 0.0464 Val Loss: 0.0500\n",
      "Epoch [74/200] Train Loss: 0.0459 Val Loss: 0.0495\n",
      "Epoch [75/200] Train Loss: 0.0458 Val Loss: 0.0500\n",
      "Epoch [76/200] Train Loss: 0.0454 Val Loss: 0.0515\n",
      "Epoch [77/200] Train Loss: 0.0453 Val Loss: 0.0481\n",
      "Epoch [78/200] Train Loss: 0.0446 Val Loss: 0.0506\n",
      "Epoch [79/200] Train Loss: 0.0444 Val Loss: 0.0479\n",
      "Epoch [80/200] Train Loss: 0.0445 Val Loss: 0.0475\n",
      "Epoch [81/200] Train Loss: 0.0437 Val Loss: 0.0482\n",
      "Epoch [82/200] Train Loss: 0.0436 Val Loss: 0.0473\n",
      "Epoch [83/200] Train Loss: 0.0432 Val Loss: 0.0475\n",
      "Epoch [84/200] Train Loss: 0.0427 Val Loss: 0.0468\n",
      "Epoch [85/200] Train Loss: 0.0428 Val Loss: 0.0465\n",
      "Epoch [86/200] Train Loss: 0.0423 Val Loss: 0.0475\n",
      "Epoch [87/200] Train Loss: 0.0421 Val Loss: 0.0459\n",
      "Epoch [88/200] Train Loss: 0.0418 Val Loss: 0.0459\n",
      "Epoch [89/200] Train Loss: 0.0420 Val Loss: 0.0478\n",
      "Epoch [90/200] Train Loss: 0.0414 Val Loss: 0.0468\n",
      "Epoch [91/200] Train Loss: 0.0411 Val Loss: 0.0465\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1169 Val Loss: 0.0955\n",
      "Epoch [2/200] Train Loss: 0.0964 Val Loss: 0.0924\n",
      "Epoch [3/200] Train Loss: 0.0925 Val Loss: 0.0872\n",
      "Epoch [4/200] Train Loss: 0.0885 Val Loss: 0.0844\n",
      "Epoch [5/200] Train Loss: 0.0869 Val Loss: 0.0827\n",
      "Epoch [6/200] Train Loss: 0.0860 Val Loss: 0.0819\n",
      "Epoch [7/200] Train Loss: 0.0854 Val Loss: 0.0812\n",
      "Epoch [8/200] Train Loss: 0.0846 Val Loss: 0.0809\n",
      "Epoch [9/200] Train Loss: 0.0841 Val Loss: 0.0802\n",
      "Epoch [10/200] Train Loss: 0.0838 Val Loss: 0.0800\n",
      "Epoch [11/200] Train Loss: 0.0831 Val Loss: 0.0793\n",
      "Epoch [12/200] Train Loss: 0.0826 Val Loss: 0.0789\n",
      "Epoch [13/200] Train Loss: 0.0823 Val Loss: 0.0797\n",
      "Epoch [14/200] Train Loss: 0.0818 Val Loss: 0.0784\n",
      "Epoch [15/200] Train Loss: 0.0812 Val Loss: 0.0778\n",
      "Epoch [16/200] Train Loss: 0.0806 Val Loss: 0.0775\n",
      "Epoch [17/200] Train Loss: 0.0807 Val Loss: 0.0776\n",
      "Epoch [18/200] Train Loss: 0.0801 Val Loss: 0.0773\n",
      "Epoch [19/200] Train Loss: 0.0797 Val Loss: 0.0770\n",
      "Epoch [20/200] Train Loss: 0.0794 Val Loss: 0.0765\n",
      "Epoch [21/200] Train Loss: 0.0790 Val Loss: 0.0779\n",
      "Epoch [22/200] Train Loss: 0.0787 Val Loss: 0.0788\n",
      "Epoch [23/200] Train Loss: 0.0785 Val Loss: 0.0760\n",
      "Epoch [24/200] Train Loss: 0.0782 Val Loss: 0.0758\n",
      "Epoch [25/200] Train Loss: 0.0777 Val Loss: 0.0757\n",
      "Epoch [26/200] Train Loss: 0.0774 Val Loss: 0.0759\n",
      "Epoch [27/200] Train Loss: 0.0770 Val Loss: 0.0755\n",
      "Epoch [28/200] Train Loss: 0.0767 Val Loss: 0.0764\n",
      "Epoch [29/200] Train Loss: 0.0765 Val Loss: 0.0748\n",
      "Epoch [30/200] Train Loss: 0.0761 Val Loss: 0.0748\n",
      "Epoch [31/200] Train Loss: 0.0760 Val Loss: 0.0742\n",
      "Epoch [32/200] Train Loss: 0.0756 Val Loss: 0.0741\n",
      "Epoch [33/200] Train Loss: 0.0756 Val Loss: 0.0756\n",
      "Epoch [34/200] Train Loss: 0.0751 Val Loss: 0.0743\n",
      "Epoch [35/200] Train Loss: 0.0750 Val Loss: 0.0746\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0990 Val Loss: 0.0847\n",
      "Epoch [2/200] Train Loss: 0.0861 Val Loss: 0.0810\n",
      "Epoch [3/200] Train Loss: 0.0819 Val Loss: 0.0780\n",
      "Epoch [4/200] Train Loss: 0.0799 Val Loss: 0.0767\n",
      "Epoch [5/200] Train Loss: 0.0789 Val Loss: 0.0759\n",
      "Epoch [6/200] Train Loss: 0.0783 Val Loss: 0.0755\n",
      "Epoch [7/200] Train Loss: 0.0777 Val Loss: 0.0752\n",
      "Epoch [8/200] Train Loss: 0.0774 Val Loss: 0.0748\n",
      "Epoch [9/200] Train Loss: 0.0770 Val Loss: 0.0745\n",
      "Epoch [10/200] Train Loss: 0.0765 Val Loss: 0.0743\n",
      "Epoch [11/200] Train Loss: 0.0763 Val Loss: 0.0741\n",
      "Epoch [12/200] Train Loss: 0.0761 Val Loss: 0.0739\n",
      "Epoch [13/200] Train Loss: 0.0756 Val Loss: 0.0737\n",
      "Epoch [14/200] Train Loss: 0.0754 Val Loss: 0.0734\n",
      "Epoch [15/200] Train Loss: 0.0752 Val Loss: 0.0734\n",
      "Epoch [16/200] Train Loss: 0.0751 Val Loss: 0.0732\n",
      "Epoch [17/200] Train Loss: 0.0746 Val Loss: 0.0729\n",
      "Epoch [18/200] Train Loss: 0.0745 Val Loss: 0.0732\n",
      "Epoch [19/200] Train Loss: 0.0741 Val Loss: 0.0727\n",
      "Epoch [20/200] Train Loss: 0.0740 Val Loss: 0.0726\n",
      "Epoch [21/200] Train Loss: 0.0736 Val Loss: 0.0726\n",
      "Epoch [22/200] Train Loss: 0.0734 Val Loss: 0.0723\n",
      "Epoch [23/200] Train Loss: 0.0732 Val Loss: 0.0734\n",
      "Epoch [24/200] Train Loss: 0.0732 Val Loss: 0.0732\n",
      "Epoch [25/200] Train Loss: 0.0728 Val Loss: 0.0721\n",
      "Epoch [26/200] Train Loss: 0.0728 Val Loss: 0.0733\n",
      "Epoch [27/200] Train Loss: 0.0725 Val Loss: 0.0716\n",
      "Epoch [28/200] Train Loss: 0.0721 Val Loss: 0.0722\n",
      "Epoch [29/200] Train Loss: 0.0720 Val Loss: 0.0718\n",
      "Epoch [30/200] Train Loss: 0.0716 Val Loss: 0.0719\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1031 Val Loss: 0.0861\n",
      "Epoch [2/200] Train Loss: 0.0867 Val Loss: 0.0832\n",
      "Epoch [3/200] Train Loss: 0.0833 Val Loss: 0.0800\n",
      "Epoch [4/200] Train Loss: 0.0806 Val Loss: 0.0784\n",
      "Epoch [5/200] Train Loss: 0.0791 Val Loss: 0.0771\n",
      "Epoch [6/200] Train Loss: 0.0781 Val Loss: 0.0764\n",
      "Epoch [7/200] Train Loss: 0.0774 Val Loss: 0.0760\n",
      "Epoch [8/200] Train Loss: 0.0767 Val Loss: 0.0753\n",
      "Epoch [9/200] Train Loss: 0.0762 Val Loss: 0.0751\n",
      "Epoch [10/200] Train Loss: 0.0758 Val Loss: 0.0743\n",
      "Epoch [11/200] Train Loss: 0.0751 Val Loss: 0.0742\n",
      "Epoch [12/200] Train Loss: 0.0748 Val Loss: 0.0735\n",
      "Epoch [13/200] Train Loss: 0.0742 Val Loss: 0.0731\n",
      "Epoch [14/200] Train Loss: 0.0738 Val Loss: 0.0727\n",
      "Epoch [15/200] Train Loss: 0.0735 Val Loss: 0.0726\n",
      "Epoch [16/200] Train Loss: 0.0730 Val Loss: 0.0720\n",
      "Epoch [17/200] Train Loss: 0.0727 Val Loss: 0.0723\n",
      "Epoch [18/200] Train Loss: 0.0721 Val Loss: 0.0716\n",
      "Epoch [19/200] Train Loss: 0.0718 Val Loss: 0.0713\n",
      "Epoch [20/200] Train Loss: 0.0714 Val Loss: 0.0711\n",
      "Epoch [21/200] Train Loss: 0.0710 Val Loss: 0.0707\n",
      "Epoch [22/200] Train Loss: 0.0706 Val Loss: 0.0708\n",
      "Epoch [23/200] Train Loss: 0.0701 Val Loss: 0.0702\n",
      "Epoch [24/200] Train Loss: 0.0698 Val Loss: 0.0717\n",
      "Epoch [25/200] Train Loss: 0.0695 Val Loss: 0.0713\n",
      "Epoch [26/200] Train Loss: 0.0690 Val Loss: 0.0700\n",
      "Epoch [27/200] Train Loss: 0.0689 Val Loss: 0.0694\n",
      "Epoch [28/200] Train Loss: 0.0684 Val Loss: 0.0692\n",
      "Epoch [29/200] Train Loss: 0.0680 Val Loss: 0.0687\n",
      "Epoch [30/200] Train Loss: 0.0678 Val Loss: 0.0685\n",
      "Epoch [31/200] Train Loss: 0.0673 Val Loss: 0.0684\n",
      "Epoch [32/200] Train Loss: 0.0670 Val Loss: 0.0680\n",
      "Epoch [33/200] Train Loss: 0.0667 Val Loss: 0.0686\n",
      "Epoch [34/200] Train Loss: 0.0663 Val Loss: 0.0683\n",
      "Epoch [35/200] Train Loss: 0.0658 Val Loss: 0.0672\n",
      "Epoch [36/200] Train Loss: 0.0659 Val Loss: 0.0670\n",
      "Epoch [37/200] Train Loss: 0.0652 Val Loss: 0.0667\n",
      "Epoch [38/200] Train Loss: 0.0648 Val Loss: 0.0665\n",
      "Epoch [39/200] Train Loss: 0.0645 Val Loss: 0.0661\n",
      "Epoch [40/200] Train Loss: 0.0638 Val Loss: 0.0656\n",
      "Epoch [41/200] Train Loss: 0.0633 Val Loss: 0.0658\n",
      "Epoch [42/200] Train Loss: 0.0634 Val Loss: 0.0650\n",
      "Epoch [43/200] Train Loss: 0.0629 Val Loss: 0.0648\n",
      "Epoch [44/200] Train Loss: 0.0621 Val Loss: 0.0647\n",
      "Epoch [45/200] Train Loss: 0.0616 Val Loss: 0.0649\n",
      "Epoch [46/200] Train Loss: 0.0612 Val Loss: 0.0631\n",
      "Epoch [47/200] Train Loss: 0.0609 Val Loss: 0.0625\n",
      "Epoch [48/200] Train Loss: 0.0602 Val Loss: 0.0632\n",
      "Epoch [49/200] Train Loss: 0.0595 Val Loss: 0.0618\n",
      "Epoch [50/200] Train Loss: 0.0591 Val Loss: 0.0613\n",
      "Epoch [51/200] Train Loss: 0.0586 Val Loss: 0.0612\n",
      "Epoch [52/200] Train Loss: 0.0583 Val Loss: 0.0607\n",
      "Epoch [53/200] Train Loss: 0.0579 Val Loss: 0.0611\n",
      "Epoch [54/200] Train Loss: 0.0570 Val Loss: 0.0596\n",
      "Epoch [55/200] Train Loss: 0.0569 Val Loss: 0.0603\n",
      "Epoch [56/200] Train Loss: 0.0566 Val Loss: 0.0590\n",
      "Epoch [57/200] Train Loss: 0.0561 Val Loss: 0.0603\n",
      "Epoch [58/200] Train Loss: 0.0555 Val Loss: 0.0590\n",
      "Epoch [59/200] Train Loss: 0.0552 Val Loss: 0.0591\n",
      "Epoch [60/200] Train Loss: 0.0547 Val Loss: 0.0573\n",
      "Epoch [61/200] Train Loss: 0.0545 Val Loss: 0.0569\n",
      "Epoch [62/200] Train Loss: 0.0538 Val Loss: 0.0578\n",
      "Epoch [63/200] Train Loss: 0.0536 Val Loss: 0.0563\n",
      "Epoch [64/200] Train Loss: 0.0530 Val Loss: 0.0561\n",
      "Epoch [65/200] Train Loss: 0.0529 Val Loss: 0.0559\n",
      "Epoch [66/200] Train Loss: 0.0523 Val Loss: 0.0558\n",
      "Epoch [67/200] Train Loss: 0.0523 Val Loss: 0.0560\n",
      "Epoch [68/200] Train Loss: 0.0523 Val Loss: 0.0554\n",
      "Epoch [69/200] Train Loss: 0.0514 Val Loss: 0.0547\n",
      "Epoch [70/200] Train Loss: 0.0516 Val Loss: 0.0553\n",
      "Epoch [71/200] Train Loss: 0.0509 Val Loss: 0.0559\n",
      "Epoch [72/200] Train Loss: 0.0505 Val Loss: 0.0555\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.0970 Val Loss: 0.0823\n",
      "Epoch [2/200] Train Loss: 0.0827 Val Loss: 0.0779\n",
      "Epoch [3/200] Train Loss: 0.0783 Val Loss: 0.0743\n",
      "Epoch [4/200] Train Loss: 0.0758 Val Loss: 0.0727\n",
      "Epoch [5/200] Train Loss: 0.0749 Val Loss: 0.0720\n",
      "Epoch [6/200] Train Loss: 0.0741 Val Loss: 0.0714\n",
      "Epoch [7/200] Train Loss: 0.0736 Val Loss: 0.0710\n",
      "Epoch [8/200] Train Loss: 0.0733 Val Loss: 0.0708\n",
      "Epoch [9/200] Train Loss: 0.0727 Val Loss: 0.0703\n",
      "Epoch [10/200] Train Loss: 0.0723 Val Loss: 0.0702\n",
      "Epoch [11/200] Train Loss: 0.0719 Val Loss: 0.0696\n",
      "Epoch [12/200] Train Loss: 0.0715 Val Loss: 0.0694\n",
      "Epoch [13/200] Train Loss: 0.0710 Val Loss: 0.0692\n",
      "Epoch [14/200] Train Loss: 0.0706 Val Loss: 0.0690\n",
      "Epoch [15/200] Train Loss: 0.0703 Val Loss: 0.0705\n",
      "Epoch [16/200] Train Loss: 0.0697 Val Loss: 0.0683\n",
      "Epoch [17/200] Train Loss: 0.0693 Val Loss: 0.0684\n",
      "Epoch [18/200] Train Loss: 0.0692 Val Loss: 0.0680\n",
      "Epoch [19/200] Train Loss: 0.0686 Val Loss: 0.0674\n",
      "Epoch [20/200] Train Loss: 0.0681 Val Loss: 0.0672\n",
      "Epoch [21/200] Train Loss: 0.0676 Val Loss: 0.0671\n",
      "Epoch [22/200] Train Loss: 0.0675 Val Loss: 0.0670\n",
      "Epoch [23/200] Train Loss: 0.0671 Val Loss: 0.0665\n",
      "Epoch [24/200] Train Loss: 0.0669 Val Loss: 0.0664\n",
      "Epoch [25/200] Train Loss: 0.0665 Val Loss: 0.0665\n",
      "Epoch [26/200] Train Loss: 0.0663 Val Loss: 0.0663\n",
      "Epoch [27/200] Train Loss: 0.0660 Val Loss: 0.0658\n",
      "Epoch [28/200] Train Loss: 0.0658 Val Loss: 0.0656\n",
      "Epoch [29/200] Train Loss: 0.0653 Val Loss: 0.0654\n",
      "Epoch [30/200] Train Loss: 0.0651 Val Loss: 0.0652\n",
      "Epoch [31/200] Train Loss: 0.0648 Val Loss: 0.0651\n",
      "Epoch [32/200] Train Loss: 0.0644 Val Loss: 0.0651\n",
      "Epoch [33/200] Train Loss: 0.0644 Val Loss: 0.0648\n",
      "Epoch [34/200] Train Loss: 0.0640 Val Loss: 0.0652\n",
      "Epoch [35/200] Train Loss: 0.0637 Val Loss: 0.0661\n",
      "Epoch [36/200] Train Loss: 0.0633 Val Loss: 0.0658\n",
      "Early stopping\n",
      "Epoch [1/200] Train Loss: 0.1075 Val Loss: 0.0893\n",
      "Epoch [2/200] Train Loss: 0.0905 Val Loss: 0.0857\n",
      "Epoch [3/200] Train Loss: 0.0868 Val Loss: 0.0824\n",
      "Epoch [4/200] Train Loss: 0.0845 Val Loss: 0.0808\n",
      "Epoch [5/200] Train Loss: 0.0832 Val Loss: 0.0802\n",
      "Epoch [6/200] Train Loss: 0.0824 Val Loss: 0.0793\n",
      "Epoch [7/200] Train Loss: 0.0819 Val Loss: 0.0788\n",
      "Epoch [8/200] Train Loss: 0.0811 Val Loss: 0.0795\n",
      "Epoch [9/200] Train Loss: 0.0805 Val Loss: 0.0778\n",
      "Epoch [10/200] Train Loss: 0.0801 Val Loss: 0.0774\n",
      "Epoch [11/200] Train Loss: 0.0796 Val Loss: 0.0775\n",
      "Epoch [12/200] Train Loss: 0.0789 Val Loss: 0.0766\n",
      "Epoch [13/200] Train Loss: 0.0785 Val Loss: 0.0764\n",
      "Epoch [14/200] Train Loss: 0.0779 Val Loss: 0.0760\n",
      "Epoch [15/200] Train Loss: 0.0773 Val Loss: 0.0756\n",
      "Epoch [16/200] Train Loss: 0.0769 Val Loss: 0.0754\n",
      "Epoch [17/200] Train Loss: 0.0762 Val Loss: 0.0752\n",
      "Epoch [18/200] Train Loss: 0.0757 Val Loss: 0.0754\n",
      "Epoch [19/200] Train Loss: 0.0752 Val Loss: 0.0741\n",
      "Epoch [20/200] Train Loss: 0.0746 Val Loss: 0.0741\n",
      "Epoch [21/200] Train Loss: 0.0741 Val Loss: 0.0747\n",
      "Epoch [22/200] Train Loss: 0.0736 Val Loss: 0.0735\n",
      "Epoch [23/200] Train Loss: 0.0731 Val Loss: 0.0727\n",
      "Epoch [24/200] Train Loss: 0.0725 Val Loss: 0.0725\n",
      "Epoch [25/200] Train Loss: 0.0721 Val Loss: 0.0720\n",
      "Epoch [26/200] Train Loss: 0.0717 Val Loss: 0.0719\n",
      "Epoch [27/200] Train Loss: 0.0713 Val Loss: 0.0715\n",
      "Epoch [28/200] Train Loss: 0.0709 Val Loss: 0.0711\n",
      "Epoch [29/200] Train Loss: 0.0705 Val Loss: 0.0710\n",
      "Epoch [30/200] Train Loss: 0.0705 Val Loss: 0.0708\n",
      "Epoch [31/200] Train Loss: 0.0697 Val Loss: 0.0702\n",
      "Epoch [32/200] Train Loss: 0.0692 Val Loss: 0.0703\n",
      "Epoch [33/200] Train Loss: 0.0689 Val Loss: 0.0696\n",
      "Epoch [34/200] Train Loss: 0.0689 Val Loss: 0.0694\n",
      "Epoch [35/200] Train Loss: 0.0682 Val Loss: 0.0694\n",
      "Epoch [36/200] Train Loss: 0.0677 Val Loss: 0.0692\n",
      "Epoch [37/200] Train Loss: 0.0674 Val Loss: 0.0696\n",
      "Epoch [38/200] Train Loss: 0.0670 Val Loss: 0.0690\n",
      "Epoch [39/200] Train Loss: 0.0668 Val Loss: 0.0686\n",
      "Epoch [40/200] Train Loss: 0.0664 Val Loss: 0.0680\n",
      "Epoch [41/200] Train Loss: 0.0658 Val Loss: 0.0675\n",
      "Epoch [42/200] Train Loss: 0.0653 Val Loss: 0.0670\n",
      "Epoch [43/200] Train Loss: 0.0650 Val Loss: 0.0665\n",
      "Epoch [44/200] Train Loss: 0.0646 Val Loss: 0.0665\n",
      "Epoch [45/200] Train Loss: 0.0642 Val Loss: 0.0659\n",
      "Epoch [46/200] Train Loss: 0.0640 Val Loss: 0.0658\n",
      "Epoch [47/200] Train Loss: 0.0634 Val Loss: 0.0672\n",
      "Epoch [48/200] Train Loss: 0.0631 Val Loss: 0.0666\n",
      "Epoch [49/200] Train Loss: 0.0626 Val Loss: 0.0654\n",
      "Epoch [50/200] Train Loss: 0.0620 Val Loss: 0.0649\n",
      "Epoch [51/200] Train Loss: 0.0619 Val Loss: 0.0643\n",
      "Epoch [52/200] Train Loss: 0.0615 Val Loss: 0.0654\n",
      "Epoch [53/200] Train Loss: 0.0609 Val Loss: 0.0636\n",
      "Epoch [54/200] Train Loss: 0.0606 Val Loss: 0.0638\n",
      "Epoch [55/200] Train Loss: 0.0604 Val Loss: 0.0650\n",
      "Epoch [56/200] Train Loss: 0.0597 Val Loss: 0.0633\n",
      "Epoch [57/200] Train Loss: 0.0590 Val Loss: 0.0625\n",
      "Epoch [58/200] Train Loss: 0.0586 Val Loss: 0.0625\n",
      "Epoch [59/200] Train Loss: 0.0583 Val Loss: 0.0634\n",
      "Epoch [60/200] Train Loss: 0.0580 Val Loss: 0.0617\n",
      "Epoch [61/200] Train Loss: 0.0576 Val Loss: 0.0612\n",
      "Epoch [62/200] Train Loss: 0.0570 Val Loss: 0.0609\n",
      "Epoch [63/200] Train Loss: 0.0568 Val Loss: 0.0603\n",
      "Epoch [64/200] Train Loss: 0.0564 Val Loss: 0.0610\n",
      "Epoch [65/200] Train Loss: 0.0560 Val Loss: 0.0600\n",
      "Epoch [66/200] Train Loss: 0.0553 Val Loss: 0.0596\n",
      "Epoch [67/200] Train Loss: 0.0549 Val Loss: 0.0589\n",
      "Epoch [68/200] Train Loss: 0.0546 Val Loss: 0.0589\n",
      "Epoch [69/200] Train Loss: 0.0541 Val Loss: 0.0585\n",
      "Epoch [70/200] Train Loss: 0.0535 Val Loss: 0.0579\n",
      "Epoch [71/200] Train Loss: 0.0534 Val Loss: 0.0588\n",
      "Epoch [72/200] Train Loss: 0.0528 Val Loss: 0.0574\n",
      "Epoch [73/200] Train Loss: 0.0526 Val Loss: 0.0574\n",
      "Epoch [74/200] Train Loss: 0.0521 Val Loss: 0.0577\n",
      "Epoch [75/200] Train Loss: 0.0521 Val Loss: 0.0567\n",
      "Epoch [76/200] Train Loss: 0.0515 Val Loss: 0.0566\n",
      "Epoch [77/200] Train Loss: 0.0509 Val Loss: 0.0554\n",
      "Epoch [78/200] Train Loss: 0.0507 Val Loss: 0.0582\n",
      "Epoch [79/200] Train Loss: 0.0500 Val Loss: 0.0586\n",
      "Epoch [80/200] Train Loss: 0.0502 Val Loss: 0.0549\n",
      "Epoch [81/200] Train Loss: 0.0494 Val Loss: 0.0552\n",
      "Epoch [82/200] Train Loss: 0.0491 Val Loss: 0.0546\n",
      "Epoch [83/200] Train Loss: 0.0489 Val Loss: 0.0539\n",
      "Epoch [84/200] Train Loss: 0.0483 Val Loss: 0.0537\n",
      "Epoch [85/200] Train Loss: 0.0481 Val Loss: 0.0529\n",
      "Epoch [86/200] Train Loss: 0.0473 Val Loss: 0.0529\n",
      "Epoch [87/200] Train Loss: 0.0469 Val Loss: 0.0525\n",
      "Epoch [88/200] Train Loss: 0.0469 Val Loss: 0.0521\n",
      "Epoch [89/200] Train Loss: 0.0467 Val Loss: 0.0557\n",
      "Epoch [90/200] Train Loss: 0.0465 Val Loss: 0.0524\n",
      "Epoch [91/200] Train Loss: 0.0462 Val Loss: 0.0515\n",
      "Epoch [92/200] Train Loss: 0.0456 Val Loss: 0.0509\n",
      "Epoch [93/200] Train Loss: 0.0452 Val Loss: 0.0516\n",
      "Epoch [94/200] Train Loss: 0.0450 Val Loss: 0.0507\n",
      "Epoch [95/200] Train Loss: 0.0446 Val Loss: 0.0511\n",
      "Epoch [96/200] Train Loss: 0.0442 Val Loss: 0.0509\n",
      "Epoch [97/200] Train Loss: 0.0438 Val Loss: 0.0508\n",
      "Epoch [98/200] Train Loss: 0.0438 Val Loss: 0.0491\n",
      "Epoch [99/200] Train Loss: 0.0435 Val Loss: 0.0499\n",
      "Epoch [100/200] Train Loss: 0.0430 Val Loss: 0.0492\n",
      "Epoch [101/200] Train Loss: 0.0429 Val Loss: 0.0501\n",
      "Epoch [102/200] Train Loss: 0.0424 Val Loss: 0.0488\n",
      "Epoch [103/200] Train Loss: 0.0425 Val Loss: 0.0479\n",
      "Epoch [104/200] Train Loss: 0.0423 Val Loss: 0.0484\n",
      "Epoch [105/200] Train Loss: 0.0422 Val Loss: 0.0479\n",
      "Epoch [106/200] Train Loss: 0.0414 Val Loss: 0.0479\n",
      "Epoch [107/200] Train Loss: 0.0411 Val Loss: 0.0483\n",
      "Epoch [108/200] Train Loss: 0.0410 Val Loss: 0.0476\n",
      "Epoch [109/200] Train Loss: 0.0406 Val Loss: 0.0469\n",
      "Epoch [110/200] Train Loss: 0.0403 Val Loss: 0.0476\n",
      "Epoch [111/200] Train Loss: 0.0398 Val Loss: 0.0463\n",
      "Epoch [112/200] Train Loss: 0.0402 Val Loss: 0.0466\n",
      "Epoch [113/200] Train Loss: 0.0395 Val Loss: 0.0461\n",
      "Epoch [114/200] Train Loss: 0.0394 Val Loss: 0.0461\n",
      "Epoch [115/200] Train Loss: 0.0392 Val Loss: 0.0471\n",
      "Epoch [116/200] Train Loss: 0.0389 Val Loss: 0.0464\n",
      "Epoch [117/200] Train Loss: 0.0388 Val Loss: 0.0461\n",
      "Epoch [118/200] Train Loss: 0.0384 Val Loss: 0.0477\n",
      "Epoch [119/200] Train Loss: 0.0383 Val Loss: 0.0450\n",
      "Epoch [120/200] Train Loss: 0.0381 Val Loss: 0.0452\n",
      "Epoch [121/200] Train Loss: 0.0379 Val Loss: 0.0449\n",
      "Epoch [122/200] Train Loss: 0.0376 Val Loss: 0.0443\n",
      "Epoch [123/200] Train Loss: 0.0375 Val Loss: 0.0443\n",
      "Epoch [124/200] Train Loss: 0.0372 Val Loss: 0.0447\n",
      "Epoch [125/200] Train Loss: 0.0369 Val Loss: 0.0441\n",
      "Epoch [126/200] Train Loss: 0.0369 Val Loss: 0.0471\n",
      "Epoch [127/200] Train Loss: 0.0369 Val Loss: 0.0446\n",
      "Epoch [128/200] Train Loss: 0.0366 Val Loss: 0.0438\n",
      "Epoch [129/200] Train Loss: 0.0363 Val Loss: 0.0436\n",
      "Epoch [130/200] Train Loss: 0.0360 Val Loss: 0.0426\n",
      "Epoch [131/200] Train Loss: 0.0358 Val Loss: 0.0427\n",
      "Epoch [132/200] Train Loss: 0.0357 Val Loss: 0.0441\n",
      "Epoch [133/200] Train Loss: 0.0360 Val Loss: 0.0443\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "server.local_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0579 Val Loss: 0.0578\n",
      "Epoch [2/20] Train Loss: 0.0576 Val Loss: 0.0572\n",
      "Epoch [3/20] Train Loss: 0.0573 Val Loss: 0.0570\n",
      "Epoch [4/20] Train Loss: 0.0571 Val Loss: 0.0570\n",
      "Epoch [5/20] Train Loss: 0.0569 Val Loss: 0.0569\n",
      "Epoch [6/20] Train Loss: 0.0568 Val Loss: 0.0568\n",
      "Epoch [7/20] Train Loss: 0.0567 Val Loss: 0.0567\n",
      "Epoch [8/20] Train Loss: 0.0566 Val Loss: 0.0566\n",
      "Epoch [9/20] Train Loss: 0.0565 Val Loss: 0.0566\n",
      "Epoch [10/20] Train Loss: 0.0564 Val Loss: 0.0566\n",
      "Epoch [11/20] Train Loss: 0.0563 Val Loss: 0.0564\n",
      "Epoch [12/20] Train Loss: 0.0563 Val Loss: 0.0564\n",
      "Epoch [13/20] Train Loss: 0.0562 Val Loss: 0.0565\n",
      "Epoch [14/20] Train Loss: 0.0561 Val Loss: 0.0562\n",
      "Epoch [15/20] Train Loss: 0.0560 Val Loss: 0.0563\n",
      "Epoch [16/20] Train Loss: 0.0559 Val Loss: 0.0561\n",
      "Epoch [17/20] Train Loss: 0.0559 Val Loss: 0.0560\n",
      "Epoch [18/20] Train Loss: 0.0558 Val Loss: 0.0562\n",
      "Epoch [19/20] Train Loss: 0.0558 Val Loss: 0.0559\n",
      "Epoch [20/20] Train Loss: 0.0557 Val Loss: 0.0559\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0675 Val Loss: 0.0666\n",
      "Epoch [2/20] Train Loss: 0.0669 Val Loss: 0.0662\n",
      "Epoch [3/20] Train Loss: 0.0665 Val Loss: 0.0658\n",
      "Epoch [4/20] Train Loss: 0.0662 Val Loss: 0.0657\n",
      "Epoch [5/20] Train Loss: 0.0659 Val Loss: 0.0653\n",
      "Epoch [6/20] Train Loss: 0.0657 Val Loss: 0.0653\n",
      "Epoch [7/20] Train Loss: 0.0656 Val Loss: 0.0651\n",
      "Epoch [8/20] Train Loss: 0.0654 Val Loss: 0.0650\n",
      "Epoch [9/20] Train Loss: 0.0652 Val Loss: 0.0648\n",
      "Epoch [10/20] Train Loss: 0.0650 Val Loss: 0.0646\n",
      "Epoch [11/20] Train Loss: 0.0649 Val Loss: 0.0647\n",
      "Epoch [12/20] Train Loss: 0.0647 Val Loss: 0.0643\n",
      "Epoch [13/20] Train Loss: 0.0646 Val Loss: 0.0644\n",
      "Epoch [14/20] Train Loss: 0.0644 Val Loss: 0.0642\n",
      "Epoch [15/20] Train Loss: 0.0643 Val Loss: 0.0641\n",
      "Epoch [16/20] Train Loss: 0.0642 Val Loss: 0.0641\n",
      "Epoch [17/20] Train Loss: 0.0640 Val Loss: 0.0638\n",
      "Epoch [18/20] Train Loss: 0.0639 Val Loss: 0.0637\n",
      "Epoch [19/20] Train Loss: 0.0638 Val Loss: 0.0636\n",
      "Epoch [20/20] Train Loss: 0.0636 Val Loss: 0.0636\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0741 Val Loss: 0.0715\n",
      "Epoch [2/20] Train Loss: 0.0737 Val Loss: 0.0710\n",
      "Epoch [3/20] Train Loss: 0.0734 Val Loss: 0.0708\n",
      "Epoch [4/20] Train Loss: 0.0731 Val Loss: 0.0707\n",
      "Epoch [5/20] Train Loss: 0.0729 Val Loss: 0.0706\n",
      "Epoch [6/20] Train Loss: 0.0727 Val Loss: 0.0704\n",
      "Epoch [7/20] Train Loss: 0.0725 Val Loss: 0.0703\n",
      "Epoch [8/20] Train Loss: 0.0724 Val Loss: 0.0702\n",
      "Epoch [9/20] Train Loss: 0.0722 Val Loss: 0.0701\n",
      "Epoch [10/20] Train Loss: 0.0721 Val Loss: 0.0700\n",
      "Epoch [11/20] Train Loss: 0.0720 Val Loss: 0.0699\n",
      "Epoch [12/20] Train Loss: 0.0719 Val Loss: 0.0698\n",
      "Epoch [13/20] Train Loss: 0.0717 Val Loss: 0.0700\n",
      "Epoch [14/20] Train Loss: 0.0716 Val Loss: 0.0697\n",
      "Epoch [15/20] Train Loss: 0.0716 Val Loss: 0.0695\n",
      "Epoch [16/20] Train Loss: 0.0714 Val Loss: 0.0694\n",
      "Epoch [17/20] Train Loss: 0.0713 Val Loss: 0.0694\n",
      "Epoch [18/20] Train Loss: 0.0712 Val Loss: 0.0693\n",
      "Epoch [19/20] Train Loss: 0.0711 Val Loss: 0.0692\n",
      "Epoch [20/20] Train Loss: 0.0710 Val Loss: 0.0693\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0638 Val Loss: 0.0632\n",
      "Epoch [2/20] Train Loss: 0.0628 Val Loss: 0.0630\n",
      "Epoch [3/20] Train Loss: 0.0625 Val Loss: 0.0630\n",
      "Epoch [4/20] Train Loss: 0.0623 Val Loss: 0.0630\n",
      "Epoch [5/20] Train Loss: 0.0623 Val Loss: 0.0628\n",
      "Epoch [6/20] Train Loss: 0.0622 Val Loss: 0.0628\n",
      "Epoch [7/20] Train Loss: 0.0620 Val Loss: 0.0626\n",
      "Epoch [8/20] Train Loss: 0.0619 Val Loss: 0.0626\n",
      "Epoch [9/20] Train Loss: 0.0618 Val Loss: 0.0625\n",
      "Epoch [10/20] Train Loss: 0.0617 Val Loss: 0.0624\n",
      "Epoch [11/20] Train Loss: 0.0617 Val Loss: 0.0623\n",
      "Epoch [12/20] Train Loss: 0.0615 Val Loss: 0.0624\n",
      "Epoch [13/20] Train Loss: 0.0615 Val Loss: 0.0622\n",
      "Epoch [14/20] Train Loss: 0.0614 Val Loss: 0.0621\n",
      "Epoch [15/20] Train Loss: 0.0613 Val Loss: 0.0621\n",
      "Epoch [16/20] Train Loss: 0.0612 Val Loss: 0.0620\n",
      "Epoch [17/20] Train Loss: 0.0611 Val Loss: 0.0621\n",
      "Epoch [18/20] Train Loss: 0.0611 Val Loss: 0.0619\n",
      "Epoch [19/20] Train Loss: 0.0610 Val Loss: 0.0618\n",
      "Epoch [20/20] Train Loss: 0.0609 Val Loss: 0.0618\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0680 Val Loss: 0.0677\n",
      "Epoch [2/20] Train Loss: 0.0674 Val Loss: 0.0671\n",
      "Epoch [3/20] Train Loss: 0.0670 Val Loss: 0.0668\n",
      "Epoch [4/20] Train Loss: 0.0668 Val Loss: 0.0666\n",
      "Epoch [5/20] Train Loss: 0.0666 Val Loss: 0.0665\n",
      "Epoch [6/20] Train Loss: 0.0664 Val Loss: 0.0663\n",
      "Epoch [7/20] Train Loss: 0.0662 Val Loss: 0.0662\n",
      "Epoch [8/20] Train Loss: 0.0661 Val Loss: 0.0661\n",
      "Epoch [9/20] Train Loss: 0.0659 Val Loss: 0.0660\n",
      "Epoch [10/20] Train Loss: 0.0658 Val Loss: 0.0658\n",
      "Epoch [11/20] Train Loss: 0.0657 Val Loss: 0.0657\n",
      "Epoch [12/20] Train Loss: 0.0655 Val Loss: 0.0657\n",
      "Epoch [13/20] Train Loss: 0.0654 Val Loss: 0.0656\n",
      "Epoch [14/20] Train Loss: 0.0653 Val Loss: 0.0655\n",
      "Epoch [15/20] Train Loss: 0.0652 Val Loss: 0.0656\n",
      "Epoch [16/20] Train Loss: 0.0651 Val Loss: 0.0655\n",
      "Epoch [17/20] Train Loss: 0.0650 Val Loss: 0.0654\n",
      "Epoch [18/20] Train Loss: 0.0649 Val Loss: 0.0652\n",
      "Epoch [19/20] Train Loss: 0.0648 Val Loss: 0.0651\n",
      "Epoch [20/20] Train Loss: 0.0646 Val Loss: 0.0651\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0588 Val Loss: 0.0596\n",
      "Epoch [2/20] Train Loss: 0.0586 Val Loss: 0.0595\n",
      "Epoch [3/20] Train Loss: 0.0583 Val Loss: 0.0593\n",
      "Epoch [4/20] Train Loss: 0.0582 Val Loss: 0.0593\n",
      "Epoch [5/20] Train Loss: 0.0581 Val Loss: 0.0592\n",
      "Epoch [6/20] Train Loss: 0.0579 Val Loss: 0.0590\n",
      "Epoch [7/20] Train Loss: 0.0578 Val Loss: 0.0590\n",
      "Epoch [8/20] Train Loss: 0.0577 Val Loss: 0.0589\n",
      "Epoch [9/20] Train Loss: 0.0577 Val Loss: 0.0590\n",
      "Epoch [10/20] Train Loss: 0.0575 Val Loss: 0.0588\n",
      "Epoch [11/20] Train Loss: 0.0575 Val Loss: 0.0588\n",
      "Epoch [12/20] Train Loss: 0.0574 Val Loss: 0.0586\n",
      "Epoch [13/20] Train Loss: 0.0573 Val Loss: 0.0586\n",
      "Epoch [14/20] Train Loss: 0.0572 Val Loss: 0.0586\n",
      "Epoch [15/20] Train Loss: 0.0571 Val Loss: 0.0585\n",
      "Epoch [16/20] Train Loss: 0.0570 Val Loss: 0.0585\n",
      "Epoch [17/20] Train Loss: 0.0570 Val Loss: 0.0584\n",
      "Epoch [18/20] Train Loss: 0.0569 Val Loss: 0.0583\n",
      "Epoch [19/20] Train Loss: 0.0568 Val Loss: 0.0583\n",
      "Epoch [20/20] Train Loss: 0.0567 Val Loss: 0.0583\n",
      "1e-05 0\n",
      "Epoch [1/20] Train Loss: 0.0651 Val Loss: 0.0662\n",
      "Epoch [2/20] Train Loss: 0.0650 Val Loss: 0.0659\n",
      "Epoch [3/20] Train Loss: 0.0647 Val Loss: 0.0657\n",
      "Epoch [4/20] Train Loss: 0.0646 Val Loss: 0.0657\n",
      "Epoch [5/20] Train Loss: 0.0645 Val Loss: 0.0654\n",
      "Epoch [6/20] Train Loss: 0.0643 Val Loss: 0.0653\n",
      "Epoch [7/20] Train Loss: 0.0642 Val Loss: 0.0652\n",
      "Epoch [8/20] Train Loss: 0.0641 Val Loss: 0.0651\n",
      "Epoch [9/20] Train Loss: 0.0640 Val Loss: 0.0653\n",
      "Epoch [10/20] Train Loss: 0.0638 Val Loss: 0.0653\n",
      "Epoch [11/20] Train Loss: 0.0637 Val Loss: 0.0650\n",
      "Epoch [12/20] Train Loss: 0.0637 Val Loss: 0.0650\n",
      "Epoch [13/20] Train Loss: 0.0635 Val Loss: 0.0648\n",
      "Epoch [14/20] Train Loss: 0.0634 Val Loss: 0.0647\n",
      "Epoch [15/20] Train Loss: 0.0634 Val Loss: 0.0651\n",
      "Epoch [16/20] Train Loss: 0.0632 Val Loss: 0.0648\n",
      "Epoch [17/20] Train Loss: 0.0631 Val Loss: 0.0646\n",
      "Epoch [18/20] Train Loss: 0.0630 Val Loss: 0.0646\n",
      "Epoch [19/20] Train Loss: 0.0630 Val Loss: 0.0645\n",
      "Epoch [20/20] Train Loss: 0.0628 Val Loss: 0.0648\n",
      "[0.0674807617747008, 0.07917149850384217, 0.08372500471209418, 0.07453125617658235, 0.07562352281880297, 0.07363936380317358, 0.0791351490347863]\n"
     ]
    }
   ],
   "source": [
    "local_fine_tune_losses=[]\n",
    "local_fine_tune_preds=[]\n",
    "local_fine_tune_models=[]\n",
    "seed_everything(0)\n",
    "for i in range(args_train.number_clients):\n",
    "    local_fine_tune_pred,local_fine_tune_loss,local_fine_tune_model=clients[i].local_fine_tune()\n",
    "    local_fine_tune_losses.append(local_fine_tune_loss)\n",
    "    local_fine_tune_preds.append(local_fine_tune_pred)\n",
    "    local_fine_tune_models.append(local_fine_tune_model)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06865391036980364, 0.08005914130337434, 0.08350040195892526, 0.0766893360215201, 0.07898468616074078, 0.07257786304498576, 0.07864485521583933]\n",
      "0.07701574201074132\n"
     ]
    }
   ],
   "source": [
    "fed_local_losses=[]\n",
    "fed_local_preds=[]\n",
    "fed_local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    fed_local_pred,fed_local_loss,fed_local_model=clients[i].fed_local_evaluation()\n",
    "    fed_local_losses.append(fed_local_loss)\n",
    "    fed_local_preds.append(fed_local_pred)\n",
    "    fed_local_models.append(fed_local_model)\n",
    "print(fed_local_losses)\n",
    "print(np.mean(fed_local_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08187799558859982, 0.08105895224295251, 0.08582551417591637, 0.07698124492770597, 0.07616442277364127, 0.07639797528159536, 0.09301163217894835]\n"
     ]
    }
   ],
   "source": [
    "local_losses=[]\n",
    "local_preds=[]\n",
    "local_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    local_pred,local_loss,local_model=clients[i].local_evaluation()\n",
    "    local_losses.append(local_loss)\n",
    "    local_preds.append(local_pred)\n",
    "    local_models.append(local_model)\n",
    "print(local_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06877125395232275, 0.08701450000070546, 0.08665346308317903, 0.07787583673959725, 0.08269334102228079, 0.0758516240788445, 0.08388754275104363]\n"
     ]
    }
   ],
   "source": [
    "central_losses=[]\n",
    "central_preds=[]\n",
    "central_models=[]\n",
    "for i in range(args_train.number_clients):\n",
    "    central_pred,central_loss,central_model=server.central_evaluation(dataset=i)\n",
    "    central_losses.append(central_loss)\n",
    "    central_preds.append(central_pred)\n",
    "    central_models.append(central_model)\n",
    "print(central_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08187799558859982, 0.08105895224295251, 0.08582551417591637, 0.07698124492770597, 0.07616442277364127, 0.07639797528159536, 0.09301163217894835]\n",
      "[0.06877125395232275, 0.08701450000070546, 0.08665346308317903, 0.07787583673959725, 0.08269334102228079, 0.0758516240788445, 0.08388754275104363]\n",
      "[0.06865391036980364, 0.08005914130337434, 0.08350040195892526, 0.0766893360215201, 0.07898468616074078, 0.07257786304498576, 0.07864485521583933]\n",
      "[0.0674807617747008, 0.07917149850384217, 0.08372500471209418, 0.07453125617658235, 0.07562352281880297, 0.07363936380317358, 0.0791351490347863]\n"
     ]
    }
   ],
   "source": [
    "print(local_losses)\n",
    "print(central_losses)\n",
    "print(fed_local_losses)\n",
    "print(local_fine_tune_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'local_losses': local_losses,\n",
    "    'central_losses': central_losses,\n",
    "    'fed_local_losses': fed_local_losses,\n",
    "    'local_fine_tune_losses': local_fine_tune_losses\n",
    "})\n",
    "df.T.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_y_lst=[]\n",
    "\n",
    "for i in range(args_train.number_clients):\n",
    "    args_temp=copy.deepcopy(args_train)\n",
    "    args_temp.dataset_paths='wf'+str(i+1)\n",
    "    test_data, test_loader = get_data(args_temp,flag='test')\n",
    "    actual_y=[]\n",
    "    for idx, (seq_x, seq_x_concat, seq_y) in enumerate(test_loader):\n",
    "        actual_y.append(seq_y)\n",
    "    actual_y = torch.cat([torch.flatten(t) for t in actual_y])\n",
    "    actual_y_lst.append(actual_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the server object\n",
    "with open('../result/12/server_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "# Save the clients object\n",
    "with open('../result/12/clients_benchmark.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save server and clients\n",
    "with open('server.pkl', 'wb') as f:\n",
    "    pickle.dump(server, f)\n",
    "\n",
    "with open('clients.pkl', 'wb') as f:\n",
    "    pickle.dump(clients, f)\n",
    "\n",
    "# Load server and clients\n",
    "with open('server.pkl', 'rb') as f:\n",
    "    server = pickle.load(f)\n",
    "\n",
    "with open('clients.pkl', 'rb') as f:\n",
    "    clients = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_IET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
